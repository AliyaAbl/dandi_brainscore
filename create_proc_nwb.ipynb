{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used after the inventory is created. It is creating config files for each recording and creating nwb files for spike times and psth (if avail). Finally validating the nwb files, it then uploades the files to dandi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from uuid import uuid4\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import os, yaml, glob, json\n",
    "import pandas as pd\n",
    "from nwbwidgets import nwb2widget\n",
    "from pynwb import NWBHDF5IO, NWBFile\n",
    "from pynwb.file import Subject\n",
    "import shutil\n",
    "import logging\n",
    "import h5py\n",
    "import pytz\n",
    "from tqdm import tqdm\n",
    "from utils.nwb_helper import  create_nwb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Create Custom Config Files for Each Recording #################\n",
    "###############################################################################\n",
    "from utils.config_helper import create_yaml\n",
    "import os\n",
    "dir = '/braintree/home/aliya277/inventory/'\n",
    "array_meta_path  = '/braintree/data2/active/users/sgouldin/array-metadata'\n",
    "for files in os.listdir(dir):\n",
    "    num_files = 0\n",
    "    path = os.path.join(dir, files)\n",
    "    try: \n",
    "        num_files = len(os.listdir(os.path.join(path,'SpikeTimes')))\n",
    "    except: pass\n",
    "    \n",
    "    if num_files == 192: \n",
    "        array_metadata = os.path.join(array_meta_path, '021023_pico_mapping_noCIT_adapter_version.json')\n",
    "        adapter_info_avail = True\n",
    "    elif num_files == 288: \n",
    "        array_metadata = os.path.join(array_meta_path,'pico_firstmapping_Lhem_2023.json')\n",
    "        adapter_info_avail = False\n",
    "        \n",
    "    create_yaml(path, array_metadata, adapter_info_avail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.tz import tzlocal\n",
    "from datetime import datetime\n",
    "from uuid import uuid4\n",
    "import yaml\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from ruamel.yaml import YAML\n",
    "from utils.rhd_helper import read_header\n",
    "import configparser\n",
    "\n",
    "\n",
    "def load_array_metadata(array_path):\n",
    "    array          = open(array_path)\n",
    "    data           = json.load(array)\n",
    "    \n",
    "    bank_assignment     = np.unique(np.array(list(data['bank'].values())))              # unique for each group\n",
    "    indices             = []\n",
    "    for bank in bank_assignment:\n",
    "        indices.append(np.where(np.array(list(data['bank'].values()))==bank)[0][0])\n",
    "\n",
    "    num_arrays          = len(bank_assignment)\n",
    "    subregions          = np.array(list(data['subregion'].values()))[indices]         # unique for each group\n",
    "    region              = np.array(list(data['region'].values()))[indices]            # same for all\n",
    "    hemispheres         = np.array(list(data['hemisphere'].values()))[indices]        # same for all\n",
    "    serialnumbers       = np.array(list(data['arr'].values()))[indices]               # same for all\n",
    "\n",
    "    positions           = np.zeros((len(list(data['subregion'].values())), 3))          # row, col, number\n",
    "    positions[:,0]      = np.array(list(data['row'].values()))\n",
    "    positions[:,1]      = np.array(list(data['col'].values()))\n",
    "    positions[:,2]      = np.array(list(data['elec'].values()))\n",
    "\n",
    "    if num_arrays ==6:\n",
    "        adapter_versions    = np.array(list(data['adapter_version'].values()))[indices]\n",
    "        return subregions, hemispheres, region, serialnumbers, bank_assignment, positions, num_arrays, adapter_versions\n",
    "    \n",
    "    return subregions, hemispheres, region, serialnumbers, bank_assignment, positions, num_arrays\n",
    "       \n",
    "def load_rec_info(path):\n",
    "    with open(os.path.join(path,\"RecInfo.yaml\") , \"r\") as f:\n",
    "        return yaml.load(f, Loader = yaml.FullLoader)\n",
    "\n",
    "def load_intan_info(Rec_Info):\n",
    "    intan_info      = Rec_Info['intanproc'].split('/')[:-1]\n",
    "    intan_info[-2]  = 'intanraw'\n",
    "    intan_info_path = os.path.join('/', *intan_info, 'info.rhd')\n",
    "    fid = open(intan_info_path, 'rb')\n",
    "    return read_header(fid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_excel_sarah():\n",
    "    df_sarah = pd.read_excel( '/braintree/home/aliya277/dandi_brainscore/Pipeline Monkey Schedule New.xlsx' ,sheet_name='pico' )\n",
    "    new_header = df_sarah.iloc[0]  \n",
    "    df_sarah = df_sarah[1:]       \n",
    "    df_sarah.columns = new_header  \n",
    "    df_sarah = df_sarah.fillna('empty') \n",
    "    return df_sarah\n",
    "\n",
    "def create_yaml(storage_dir, df, array_metadata_path, adapter_info_avail=False):\n",
    "\n",
    "    date = f\"20{df['date']}\"\n",
    "    if len(str(df['time'])) != 6: time = f\"0{df['time']}\"\n",
    "    else: time = str(df['time'])\n",
    "    \n",
    "    if df['ImageSet'] == 'normalizers':\n",
    "        directory = f'norm_FOSS.sub_pico.{date}_{time}.proc'\n",
    "    elif df['ImageSet'] == 'normalizers-HVM':\n",
    "        directory = f'norm_HVM.sub_pico.{date}_{time}.proc'\n",
    "    else: \n",
    "        directory = f\"exp_{df['ImageSet']}.sub_pico.{date}_{time}.proc\"\n",
    "\n",
    "    imagesetdir = os.path.join(storage_dir, \".\".join(directory.split(\".\")[0:1]))\n",
    "    subjectdir  = os.path.join(storage_dir, imagesetdir, \".\".join(directory.split(\".\")[0:2]))\n",
    "\n",
    "    config_dict = dict()\n",
    "\n",
    "    config_dict['subject'] = {\n",
    "                'subject_id':   'pico',\n",
    "                'date_of_birth': datetime(2014, 6, 22, tzinfo = tzlocal()), \n",
    "                'sex':          'M',\n",
    "                'species':      'Macaca mulatta',\n",
    "                'description':  'monkey'\n",
    "                }\n",
    "    \n",
    "    config_dict['general'] ={\n",
    "                'experiment_info': {\n",
    "                    'experiment_description': f'Task: Rapid serial visual presentation (RSVP).',\n",
    "                    'keywords': ['Vistual Stimuli', 'Object Recognition', 'Inferior temporal cortex (IT)', 'Ventral visual pathway'],\n",
    "                    'surgery' : '3x Utah Array Implant + Headpost',\n",
    "                    },\n",
    "                'lab_info': {\n",
    "                    'university':  'Massachusetts Institute of Technology',\n",
    "                    'institution': 'McGovern Institute for Brain Research',\n",
    "                    'lab': 'DiCarlo',\n",
    "                    'experimenter': 'Goulding, Sarah',\n",
    "                    }\n",
    "                }\n",
    "\n",
    "    config_dict['hardware'] = {\n",
    "                    'electrode_name': 'Electrode',\n",
    "                    'electrode_description': 'Utah CerePort Array with a single electrode array',\n",
    "                    'electrode_manuf': '2020 Blackrock Microsystems, LLC',\n",
    "                    'system_name': 'RecordingSystem',\n",
    "                    'system_description': 'RHD Recording System',\n",
    "                    'system_manuf': '2010-2023 Intan Technologies', \n",
    "                    'adapter_name': 'Utah Array Pedestal Connector',\n",
    "                    'adapter_description': 'Connects Utah Pedestal to Intan RHD Recording System',\n",
    "                    'adapter_manuf': 'Ripple Neuro', \n",
    "                    'photodiode_name': 'DET36A2 Biased Si Detector',\n",
    "                    'photodiode_description': 'Photodiode for detecting image presentation times. Comes with DET2A Power Adapter',\n",
    "                    'photodiode_manuf': 'Thorlabs Inc.', \n",
    "                    'monitor_name': 'LG UltraGear',\n",
    "                    'monitor_description': 'LG 32GP850-B 32 UltraGear QHD (2560 x 1440) Nano IPS Gaming Monitor w/ 1ms (GtG) Response Time & 165Hz Refresh Rate.\\\n",
    "                        Manually color calibrated and set to 120 HZ refresh rate.',\n",
    "                    'monitor_manuf': 'LG', \n",
    "                    }\n",
    "    \n",
    "    config_dict['software'] ={\n",
    "                    'mwclient_version': 'Version 0.11 (2022.02.15)',\n",
    "                    'mwserver_version': 'Version 0.11 (2022.02.15)',\n",
    "                    'OS': 'macOS Monterery on MAC Pro (Late 2013)',\n",
    "                    'intan_version': 'Version 3.1.0'\n",
    "                    }\n",
    "   \n",
    "                \n",
    "    config_dict['metadata'] = {\n",
    "                    'nwb_version' : '2.6.0',\n",
    "                    'file_create_date': datetime.strptime(date, \"%Y%m%d\"),\n",
    "                    'identifier': str(uuid4()), \n",
    "                    'session_start_time':datetime.strptime(date+time, \"%Y%m%d%H%M%S\")\n",
    "                    }\n",
    "    \n",
    "    \n",
    "    config_dict['paths'] = {\n",
    "                    'SpikeTime': df['Path: SpikeTimes'],\n",
    "                    'psth': df['Path: h5'],\n",
    "                    }\n",
    "\n",
    "    df_sarah = load_excel_sarah()\n",
    "    text = 'Recording Information'\n",
    "    try:\n",
    "        rec_info = df_sarah.iloc[int(df['(excel) Index'])-3][2:14]\n",
    "        text += str(rec_info)\n",
    "\n",
    "    except: pass\n",
    "\n",
    "    config_dict[\"session_info\"] = {\n",
    "                'session_id': directory,\n",
    "                'session_description': text,\n",
    "                }\n",
    "    \n",
    "\n",
    "    config_dict['PSTH info'] = {}\n",
    "\n",
    "    psth_info = configparser.ConfigParser()\n",
    "    psth_info.read('/braintree/data2/active/users/sgouldin/spike-tools-chong/spike_tools/spike_config.ini')\n",
    "    for option in psth_info.options('PSTH'):\n",
    "        config_dict['PSTH info'][option] = psth_info.get('PSTH', option)\n",
    "\n",
    "    config_dict['Filtering info'] = {}\n",
    "\n",
    "    for option in psth_info.options('Filtering'):\n",
    "        config_dict['Filtering info'][option] = psth_info.get('Filtering', option)\n",
    "                \n",
    "    \n",
    "    if adapter_info_avail == True:\n",
    "        subregions, hemispheres, region, serialnumbers, bank_assignment, positions, num_arrays, adapter_versions = load_array_metadata(array_metadata_path)\n",
    "    else:\n",
    "        subregions, hemispheres, region, serialnumbers, bank_assignment, positions, num_arrays = load_array_metadata(array_metadata_path)\n",
    "    \n",
    "    config_dict['array_info'] = {'intan_electrode_labeling_[row,col,id]':json.dumps(positions.tolist())}\n",
    "\n",
    "    if num_arrays == 6:\n",
    "        for arr, i in zip(bank_assignment, range(num_arrays)) :\n",
    "                config_dict['array_info']['array_{}'.format(arr)] = {\n",
    "                            'position': [0.0,0.0,0.0],\n",
    "                            'serialnumber':     str(serialnumbers[i]),\n",
    "                            'adapterversion':   str(adapter_versions[i]),\n",
    "                            'hemisphere':       str(hemispheres[i]),\n",
    "                            'region':           str(region[i]),\n",
    "                            'subregion':        str(subregions[i]),\n",
    "                            }  \n",
    "    else:\n",
    "         for arr, i in zip(bank_assignment, range(num_arrays)) :\n",
    "                config_dict['array_info']['array_{}'.format(arr)] = {\n",
    "                            'position': [0.0,0.0,0.0],\n",
    "                            'serialnumber':     str(serialnumbers[i]),\n",
    "                            'hemisphere':       str(hemispheres[i]),\n",
    "                            'region':           str(region[i]),\n",
    "                            'subregion':        str(subregions[i]),\n",
    "                            }  \n",
    "    print(directory)\n",
    "\n",
    "    yaml = YAML()\n",
    "    with open(os.path.join(subjectdir,directory,f\"config_nwb.yaml\"), 'w') as yamlfile:\n",
    "        yaml.dump((config_dict), yamlfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm_FOSS.sub_pico.20220616_111545.proc\n",
      "norm_FOSS.sub_pico.20220615_113442.proc\n",
      "norm_FOSS.sub_pico.20220907_142157.proc\n",
      "norm_FOSS.sub_pico.20220929_170635.proc\n",
      "norm_FOSS.sub_pico.20230216_145353.proc\n",
      "norm_FOSS.sub_pico.20230221_124922.proc\n",
      "norm_FOSS.sub_pico.20230224_142014.proc\n",
      "norm_FOSS.sub_pico.20230303_152601.proc\n",
      "norm_FOSS.sub_pico.20230328_145456.proc\n",
      "norm_FOSS.sub_pico.20230428_111937.proc\n",
      "norm_FOSS.sub_pico.20230508_120630.proc\n",
      "norm_FOSS.sub_pico.20230509_112900.proc\n",
      "norm_FOSS.sub_pico.20230510_125550.proc\n",
      "norm_FOSS.sub_pico.20230511_120927.proc\n",
      "norm_FOSS.sub_pico.20230515_133554.proc\n",
      "norm_FOSS.sub_pico.20230516_120137.proc\n",
      "norm_FOSS.sub_pico.20230605_122450.proc\n",
      "norm_FOSS.sub_pico.20230606_134244.proc\n",
      "norm_FOSS.sub_pico.20230609_133109.proc\n",
      "norm_FOSS.sub_pico.20230613_124343.proc\n",
      "norm_FOSS.sub_pico.20230616_100142.proc\n",
      "norm_FOSS.sub_pico.20230621_140747.proc\n",
      "norm_FOSS.sub_pico.20230630_135832.proc\n",
      "norm_FOSS.sub_pico.20230705_122755.proc\n",
      "norm_FOSS.sub_pico.20230707_140510.proc\n",
      "norm_FOSS.sub_pico.20230711_142158.proc\n",
      "norm_FOSS.sub_pico.20230712_134744.proc\n",
      "norm_FOSS.sub_pico.20230717_155454.proc\n",
      "norm_FOSS.sub_pico.20230721_122001.proc\n",
      "norm_FOSS.sub_pico.20230724_104204.proc\n",
      "norm_FOSS.sub_pico.20230725_113504.proc\n",
      "norm_FOSS.sub_pico.20230728_164343.proc\n",
      "norm_FOSS.sub_pico.20230730_134454.proc\n",
      "norm_FOSS.sub_pico.20230804_152622.proc\n",
      "norm_FOSS.sub_pico.20230803_105856.proc\n",
      "norm_FOSS.sub_pico.20230809_120658.proc\n",
      "norm_FOSS.sub_pico.20230816_101041.proc\n",
      "norm_FOSS.sub_pico.20230818_095451.proc\n",
      "norm_FOSS.sub_pico.20230821_120107.proc\n",
      "norm_FOSS.sub_pico.20230822_153931.proc\n",
      "norm_FOSS.sub_pico.20230825_100059.proc\n",
      "norm_FOSS.sub_pico.20230831_121100.proc\n",
      "norm_FOSS.sub_pico.20230901_115230.proc\n",
      "norm_FOSS.sub_pico.20230906_093230.proc\n",
      "norm_FOSS.sub_pico.20230912_103737.proc\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/braintree/home/aliya277/inventory/norm_FOSS/norm_FOSS.sub_pico/norm_FOSS.sub_pico.20230912_103737.proc/config_nwb.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/braintree/home/aliya277/dandi_brainscore/create_proc_nwb.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbraintree-gpu-1.mit.edu/braintree/home/aliya277/dandi_brainscore/create_proc_nwb.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     array_metadata \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(array_meta_path,\u001b[39m'\u001b[39m\u001b[39mpico_firstmapping_Lhem_2023.json\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbraintree-gpu-1.mit.edu/braintree/home/aliya277/dandi_brainscore/create_proc_nwb.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     adapter_info_avail \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bbraintree-gpu-1.mit.edu/braintree/home/aliya277/dandi_brainscore/create_proc_nwb.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m create_yaml(storage_dir, DataFrame, array_metadata, adapter_info_avail)\n",
      "\u001b[1;32m/braintree/home/aliya277/dandi_brainscore/create_proc_nwb.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbraintree-gpu-1.mit.edu/braintree/home/aliya277/dandi_brainscore/create_proc_nwb.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=141'>142</a>\u001b[0m \u001b[39mprint\u001b[39m(directory)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbraintree-gpu-1.mit.edu/braintree/home/aliya277/dandi_brainscore/create_proc_nwb.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=143'>144</a>\u001b[0m yaml \u001b[39m=\u001b[39m YAML()\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bbraintree-gpu-1.mit.edu/braintree/home/aliya277/dandi_brainscore/create_proc_nwb.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=144'>145</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(subjectdir,directory,\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mconfig_nwb.yaml\u001b[39;49m\u001b[39m\"\u001b[39;49m), \u001b[39m'\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m yamlfile:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbraintree-gpu-1.mit.edu/braintree/home/aliya277/dandi_brainscore/create_proc_nwb.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=145'>146</a>\u001b[0m     yaml\u001b[39m.\u001b[39mdump((config_dict), yamlfile)\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/braintree/home/aliya277/inventory/norm_FOSS/norm_FOSS.sub_pico/norm_FOSS.sub_pico.20230912_103737.proc/config_nwb.yaml'"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel( '/braintree/home/aliya277/dandi_brainscore/pico_inventory.xlsx'  )\n",
    "SubjectName = 'pico'\n",
    "storage_dir = '/braintree/home/aliya277/inventory'\n",
    "array_meta_path  = '/braintree/data2/active/users/sgouldin/array-metadata'\n",
    "\n",
    "# for imageset, excelname, date_, time_, hasspike, hash5, spikepath, h5path in zip(df['ImageSet'], df['(excel) Stimuli'], df['date'], df['time'], df['Has SpikeTime'], \\\n",
    "    # df['Has h5'], df['Path: SpikeTimes'], df['Path: h5']):\n",
    "    # date = f'20{DataFrame['date']}'\n",
    "    # if len(str(DataFrame['time'])) != 6:\n",
    "        # time = f'0{DataFrame['time']}'\n",
    "\n",
    "for index, DataFrame in df.iterrows():\n",
    "        \n",
    "    if DataFrame['Has SpikeTime'] == 1:\n",
    "\n",
    "        num_files = len(os.listdir(DataFrame['Path: SpikeTimes']))\n",
    "        if num_files == 192: \n",
    "            array_metadata = os.path.join(array_meta_path, '021023_pico_mapping_noCIT_adapter_version.json')\n",
    "            adapter_info_avail = True\n",
    "        elif num_files == 288: \n",
    "            array_metadata = os.path.join(array_meta_path,'pico_firstmapping_Lhem_2023.json')\n",
    "            adapter_info_avail = False\n",
    "\n",
    "        create_yaml(storage_dir, DataFrame, array_metadata, adapter_info_avail)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date: empty\n",
    "Stimuli: facesMSFDE\n",
    "Fixation Window: '2'\n",
    "Visual Deg: 8\n",
    "ON/OFF (ms): 100/100\n",
    "Reps: 30\n",
    "Stimuli per Trial: 8\n",
    "Images*/Videos: 360\n",
    "Total images/Videos: 10800\n",
    "Reward (ms): 65-70\n",
    "Total Water (ml): empty\n",
    "aIT Pedestal: empty\n",
    "pIT Pedestal: empty\n",
    "Referencing: empty\n",
    "Notes: empty\n",
    "2021-12-01 00:00:00: empty\n",
    ".nan: empty\n",
    "intanproc: \n",
    "  /braintree/data2/active/users/sgouldin/projects/facesMSFDE/monkeys/pico/intanproc/ko-pico-facesMSFDE_230327_134955/spikeTime\n",
    "SessionDate: '20230327'\n",
    "SessionTime: '134955'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all files: 201\n",
      "Creating nwb for folder exp_Alireza_paradigm1.sub_pico.20230616_103724.proc\n",
      "Saving NWB File..\n",
      "File 0 saved.\n"
     ]
    }
   ],
   "source": [
    "############### Iterate through every File and Create NWB #####################\n",
    "###############################################################################\n",
    "\n",
    "inventory   = '/braintree/home/aliya277/inventory/'\n",
    "all_files = os.listdir(inventory)\n",
    "i = 0\n",
    "print(f'Number of all files: {len(all_files)}')\n",
    "for folder in all_files[0:1]:\n",
    "    print(f'Creating nwb for folder {folder}')\n",
    "    path = os.path.join(inventory, folder)\n",
    "    SessionName = folder\n",
    "    # try:\n",
    "    with open(os.path.join(path,\"config_nwb.yaml\") , \"r\") as f:\n",
    "        config = yaml.load(f, Loader = yaml.FullLoader)\n",
    "\n",
    "    nwbfile = create_nwb(config, path)\n",
    "\n",
    "    # if os.path.isfile(os.path.join(path, f\"{SessionName}.nwb\")):\n",
    "    #     os.remove(os.path.join(path, f\"{SessionName}.nwb\"))\n",
    "\n",
    "    print('Saving NWB File..')\n",
    "    io = NWBHDF5IO(os.path.join(path, f\"{SessionName}.nwb\"), \"w\") \n",
    "    io.write(nwbfile)\n",
    "    io.close()\n",
    "    print(f\"File {i} saved.\")\n",
    "    i += 1\n",
    "    # except: print(f'This file did not work: {SessionName}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### ADDITIONAL BOX ################################################\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "Adding psth meta to nwb. file. This is now added to the creation process - no need to run this box again.\n",
    "\"\"\"\n",
    "\n",
    "inventory   = '/braintree/home/aliya277/inventory/'\n",
    "all_files = os.listdir(inventory)\n",
    "i = 0\n",
    "print(f'Number of all files: {len(all_files)}')\n",
    "for folder in all_files[0:1]:\n",
    "    print(f'Adding psth meta to nwb: {folder}')\n",
    "    path = os.path.join(inventory, folder)\n",
    "    SessionName = folder\n",
    "    filename = os.path.join(path, f\"{SessionName}.nwb\")\n",
    "\n",
    "    if 'h5Files' in os.listdir(path):\n",
    "\n",
    "        filename_h5 = os.listdir(os.path.join(path, 'h5Files'))[0]\n",
    "        file = h5py.File(os.path.join(path, 'h5Files', filename_h5),'r+') \n",
    "        meta = np.array([np.array(file['meta']['start_time_ms']), np.array(file['meta']['stop_time_ms']), np.array(file['meta']['tb_ms'])])\n",
    "        file.close()\n",
    "\n",
    "        with NWBHDF5IO(filename, \"r+\") as io:\n",
    "            read_nwbfile = io.read()\n",
    "\n",
    "            try:\n",
    "                read_nwbfile.add_scratch(\n",
    "                    meta,\n",
    "                    name=\"psth meta\",\n",
    "                    description=\"start_time_ms, stop_time_ms, tb_ms\",\n",
    "                    )\n",
    "                \n",
    "                io.write(read_nwbfile)\n",
    "            except: pass # meta already exists\n",
    "\n",
    "            io.close()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Check if All Files are Written and can be Opened ##############\n",
    "###############################################################################\n",
    "\n",
    "inventory   = '/braintree/home/aliya277/inventory/'\n",
    "all_files = os.listdir(inventory)\n",
    "i = 0\n",
    "for folder in all_files:\n",
    "        path = os.path.join(inventory, folder)\n",
    "        try:\n",
    "                io = NWBHDF5IO(os.path.join(path, f\"{folder}.nwb\"), \"r\") \n",
    "                nwbfile = io.read()\n",
    "                # display(nwbfile)\n",
    "                io.close()\n",
    "        except: print(f'{i}: This File can not be opened: {folder}')\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Validate All Files Using pwnyb, nwbinspectors #################\n",
    "###############################################################################\n",
    "\n",
    "from pynwb import validate\n",
    "from nwbinspector import inspect_nwbfile\n",
    "from dandi.validate import validate as dandival\n",
    "\n",
    "inventory   = '/braintree/home/aliya277/inventory/'\n",
    "all_nwb_paths = []\n",
    "all_files = os.listdir(inventory)\n",
    "for folder in all_files:\n",
    "    path = os.path.join(inventory, folder, f'{folder}.nwb')\n",
    "    all_nwb_paths.append(path)\n",
    "    \n",
    "pynwb_validation = validate(paths = all_nwb_paths)\n",
    "dandi_validation = dandival(all_files)\n",
    "\n",
    "nwbinspector_validation = []\n",
    "for path in all_nwb_paths:\n",
    "    results = list(inspect_nwbfile(nwbfile_path=path))\n",
    "    nwbinspector_validation.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pynwb_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwbinspector_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Add PSTH if Needed ############################################\n",
    "###############################################################################\n",
    "\n",
    "inventory   = '/braintree/home/aliya277/inventory/'\n",
    "all_files = os.listdir(inventory)\n",
    "for folder in all_files:\n",
    "        path = os.path.join(inventory, folder)\n",
    "        try:\n",
    "                io = NWBHDF5IO(os.path.join(path, f\"{folder}.nwb\"), \"r\") \n",
    "                nwbfile = io.read()\n",
    "                io.close()\n",
    "        except: print(f'This File can not be opened: {folder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "579b41b5614846578a7c03a80dec0efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='session_description:', layout=Layout(max_height='40px', max_width='…"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwb2widget(nwbfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'h5Files' in os.listdir(path):\n",
    "#     print('Opening PSTH...')\n",
    "#     filename = os.listdir(os.path.join(path, 'h5Files'))[0]\n",
    "#     file = h5py.File(os.path.join(path, 'h5Files', filename),'r+') \n",
    "#     data = file['psth'][:]\n",
    "#     file.close()\n",
    "\n",
    "\n",
    "#     print('Adding PSTH...')\n",
    "#     nwbfile.add_scratch(\n",
    "#         data,\n",
    "#         name=\"psth\",\n",
    "#         description=\"psth, uncorrected [channels x stimuli x reps x timebins]\",\n",
    "#         )\n",
    "    \n",
    "# else: counter +=1\n",
    "\n",
    "# try: io.close()\n",
    "# except:pass\n",
    "\n",
    "# if os.path.isfile(os.path.join(path, f\"{SessionName}.nwb\")):\n",
    "#         os.remove(os.path.join(path, f\"{SessionName}.nwb\"))\n",
    "\n",
    "# print('Saving NWB File..')\n",
    "# io = NWBHDF5IO(os.path.join(path, f\"{SessionName}.nwb\"), \"w\") \n",
    "# io.write(nwbfile)\n",
    "# io.close()\n",
    "# print(\"File saved.\")\n",
    "\n",
    "#     # psth    = get_psth_from_nwb(nwbfile, path, start_time, stop_time, timebin)\n",
    "#     # data    = psth['psth']\n",
    "\n",
    "# print(f'{counter} SpikeTimes do not have an h5 file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "inventory   = '/braintree/home/aliya277/inventory/'\n",
    "\n",
    "def find_directories_without_extension(root_dir, extension):\n",
    "    directory_paths = []\n",
    "    for foldername, subfolders, filenames in os.walk(root_dir):\n",
    "        depth = foldername[len(root_dir):].count(os.sep)\n",
    "        if depth ==  0:\n",
    "            # Check if any file in the directory has the specified extension\n",
    "            if not any(filename.endswith(extension) for filename in filenames):\n",
    "                directory_paths.append(os.path.join(root_dir, foldername))\n",
    "                # print(os.path.join(root_dir, foldername))\n",
    "    return directory_paths[1:]\n",
    "\n",
    "print(len(find_directories_without_extension(inventory, '.nwb')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "for obj in gc.get_objects():   # Browse through ALL objects\n",
    "    if isinstance(obj, h5py.File):   # Just HDF5 files\n",
    "        try:\n",
    "            obj.close()\n",
    "        except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def read_names(filename):\n",
    "    assignment  = filename.split('.')[0].split('-')[1]\n",
    "    number      = filename.split('.')[0].split('-')[2]\n",
    "    return np.asarray([assignment, number])\n",
    "\n",
    "def create_nwb(config, path):\n",
    "\n",
    "    SessionDate = path.split('/')[-1].split('.')[-2].split('_')[0]\n",
    "    SessionTime = path.split('/')[-1].split('.')[-2].split('_')[1]\n",
    "    SubjectName = path.split('/')[-1].split('.')[1].split('_')[1]\n",
    "    date_format = \"%Y%m%d %H%M%S\"\n",
    "\n",
    "    if config['subject']['subject_id'].lower() != SubjectName.lower():\n",
    "        raise ValueError(\"Subject Name incorrect.\")\n",
    "        \n",
    "    session_start_time_ = datetime.strptime(SessionDate+' '+SessionTime, date_format)\n",
    "    # Define the timezone you want to use (e.g., 'US/Eastern' for Boston)\n",
    "    desired_timezone = pytz.timezone('US/Eastern')\n",
    "    session_start_time = desired_timezone.localize(session_start_time_)\n",
    "\n",
    "    ################ CREATE NWB FILE WITH METADATA ################################\n",
    "    ###############################################################################\n",
    "    nwbfile = NWBFile(\n",
    "        session_description     = config['session_info']['session_description'],\n",
    "        identifier              = config['metadata']['identifier'],\n",
    "        session_start_time      = session_start_time,\n",
    "        file_create_date        = config['metadata']['file_create_date'],\n",
    "        experimenter            = config['general']['lab_info']['experimenter'],\n",
    "        experiment_description  = config['general']['experiment_info']['experiment_description'],\n",
    "        session_id              = config['session_info']['session_id'],\n",
    "        lab                     = config['general']['lab_info']['lab'],                     \n",
    "        institution             = config['general']['lab_info']['institution'],                                    \n",
    "        keywords                = config['general']['experiment_info']['keywords'],\n",
    "        protocol                = config['general']['experiment_info']['protocol'],\n",
    "        related_publications    = config['general']['experiment_info']['related_publications'],\n",
    "        surgery                 = config['general']['experiment_info']['surgery']\n",
    "    )\n",
    "\n",
    "    ################ CREATE SUBJECT ################################################\n",
    "    ################################################################################\n",
    "    nwbfile.subject = Subject(\n",
    "        subject_id  = config['subject']['subject_id'],\n",
    "        date_of_birth= config['subject']['date_of_birth'],\n",
    "        species     = config['subject']['species'],\n",
    "        sex         = config['subject']['sex'],\n",
    "    )\n",
    "\n",
    "    ################ CREATE HARDWARE LINKS #########################################\n",
    "    ################################################################################\n",
    "    nwbfile.create_device(\n",
    "        name        = config['hardware']['system_name'], \n",
    "        description = config['hardware']['system_description'], \n",
    "        manufacturer= config['hardware']['system_manuf']\n",
    "    )\n",
    "\n",
    "    nwbfile.create_device(\n",
    "        name        = config['hardware']['adapter_manuf'], \n",
    "        description = config['hardware']['adapter_description'], \n",
    "        manufacturer= config['hardware']['adapter_manuf']\n",
    "    )\n",
    "\n",
    "    nwbfile.create_device(\n",
    "        name        = config['hardware']['monitor_name'], \n",
    "        description = config['hardware']['monitor_description'], \n",
    "        manufacturer= config['hardware']['monitor_manuf']\n",
    "    )\n",
    "\n",
    "    nwbfile.create_device(\n",
    "        name        = config['hardware']['photodiode_name'], \n",
    "        description = config['hardware']['photodiode_description'], \n",
    "        manufacturer= config['hardware']['photodiode_manuf']\n",
    "    )\n",
    "    \n",
    "    nwbfile.create_device(\n",
    "        name        = 'Software Used', \n",
    "        description = str(['Mworks Client: '+config['software']['mwclient_version'],\\\n",
    "                        'Mworks Server: '+config['software']['mwserver_version'],\\\n",
    "                        'OS: '+config['software']['OS'],\\\n",
    "                        'Intan :'+config['software']['intan_version']])\n",
    "    )\n",
    "\n",
    "    ################ CREATE ELECTRODE LINKS ########################################\n",
    "    ################################################################################\n",
    "    electrodes = nwbfile.create_device(\n",
    "        name        = config['hardware']['electrode_name'], \n",
    "        description = config['hardware']['electrode_description'], \n",
    "        manufacturer= config['hardware']['electrode_manuf']\n",
    "    )\n",
    "\n",
    "    all_files = sorted(os.listdir(os.path.join(path, 'SpikeTimes')))\n",
    "    \n",
    "    name_accumulator = []\n",
    "    for file in all_files:\n",
    "        name_accumulator.append(read_names(file))\n",
    "    names = np.vstack(name_accumulator)\n",
    "\n",
    "    nwbfile.add_electrode_column(name=\"label\", description=\"label of electrode\")\n",
    "    groups, count_groups = np.unique(names[:,0], return_counts =True)\n",
    "    ids                  = names[:,1]\n",
    "    counter              = 0\n",
    "    # create ElectrodeGroups A, B, C, ..\n",
    "    for group, count_group in zip(groups, count_groups):\n",
    "        electrode_group = nwbfile.create_electrode_group(\n",
    "            name        = \"group_{}\".format(group),\n",
    "            description = \"Serialnumber: {}. Adapter Version: {}\".format(config['array_info']['array_{}'.format(group)]['serialnumber'],\\\n",
    "                            config['array_info']['array_{}'.format(group)]['adapterversion']),\n",
    "            device      = electrodes,\n",
    "            location    = 'hemisphere, region, subregion: '+str([config['array_info']['array_{}'.format(group)]['hemisphere'],\\\n",
    "                                config['array_info']['array_{}'.format(group)]['region'],\n",
    "                                config['array_info']['array_{}'.format(group)]['subregion']]),\n",
    "            position    = config['array_info']['array_{}'.format(group)]['position']\n",
    "        )\n",
    "\n",
    "        # create Electrodes 001, 002, ..., 032 in ElectrodeGroups per channel\n",
    "        for ichannel in range(count_group):\n",
    "            nwbfile.add_electrode(\n",
    "                group       = electrode_group,\n",
    "                label       = ids[counter],\n",
    "                location    = 'row, col, elec'+str(json.loads(config['array_info']['intan_electrode_labeling_[row,col,id]'])[counter])\n",
    "            )\n",
    "            counter += 1     \n",
    "\n",
    "    ################ ADD SPIKE TIMES ###############################################\n",
    "    ################################################################################\n",
    "\n",
    "    nwbfile.add_unit_column(name=\"unit\", description=\"millisecond\") \n",
    "    for filename, i in zip(sorted(os.listdir(os.path.join(path, 'SpikeTimes'))), range(len(os.listdir(os.path.join(path, 'SpikeTimes'))))):\n",
    "        [assignment, number] = read_names(filename)\n",
    "        file_path = os.path.join(path, 'SpikeTimes', filename)\n",
    "        data = scipy.io.loadmat(file_path, squeeze_me=True,\n",
    "                        variable_names='spike_time_ms')['spike_time_ms']\n",
    "        nwbfile.add_unit(\n",
    "            spike_times = data, \n",
    "            electrodes  = [i],\n",
    "            electrode_group = nwbfile.electrode_groups[f'group_{assignment}'], \n",
    "            unit = 'ms'\n",
    "        )\n",
    "\n",
    "    ################ ADD TRIAL TIMES ###############################################\n",
    "    ################################################################################\n",
    "    last_spike = data[-1]\n",
    "    del data\n",
    "    with open(os.path.join(path, 'NWBInfo.txt')) as f:\n",
    "        lines = f.readlines()\n",
    "        line1 = lines[0].split(',')[0]\n",
    "        StimOnnOff = [float(line1.split('/')[0]),float(line1.split('/')[1])] \n",
    "\n",
    "    on_start  = 0\n",
    "    on_dur    = StimOnnOff[1]\n",
    "    off_dur   = StimOnnOff[1]\n",
    "\n",
    "    \n",
    "    nwbfile.add_trial_column(name=\"unit\", description=\"millisecond\")\n",
    "    while on_start < last_spike:\n",
    "\n",
    "        nwbfile.add_trial(\n",
    "            start_time= float(on_start),\n",
    "            stop_time = float(on_start+on_dur),\n",
    "            unit = 'ms')\n",
    "    \n",
    "        on_start += on_dur+off_dur\n",
    "\n",
    "    return nwbfile\n",
    "\n",
    "def get_psth_from_nwb(nwbfile, path, start_time, stop_time, timebin, n_stimuli=None):\n",
    "\n",
    "    # Find the MWORKS File\n",
    "    with open(os.path.join(path, 'NWBInfo.txt')) as f:\n",
    "        lines = f.readlines()\n",
    "        line2 = lines[0].split(',')[1]\n",
    "        ind = line2.split('/').index('intanproc')\n",
    "        mwk_file = glob.glob(os.path.join('/', *line2.split('/')[0:ind], 'mworksproc', \\\n",
    "                '_'.join(map(str, line2.split('/')[ind+1].split('_')[0:3]))+'*_mwk.csv'))   \n",
    "        \n",
    "    assert len(mwk_file) == 1\n",
    "    mwk_file = mwk_file[0]\n",
    "    assert os.path.isfile(mwk_file)==True\n",
    "\n",
    "    ################ MODIFIED FROM THE SPIKE-TOOLS-CHONG CODE ######################\n",
    "    ################################################################################\n",
    "    \n",
    "    mwk_data = pd.read_csv(mwk_file)\n",
    "    mwk_data = mwk_data[mwk_data.fixation_correct == 1]\n",
    "    if 'photodiode_on_us' in mwk_data.keys():\n",
    "        samp_on_ms = np.asarray(mwk_data['photodiode_on_us']) / 1000.\n",
    "        logging.info('Using photodiode signal for sample on time')\n",
    "    else:\n",
    "        samp_on_ms = np.asarray(mwk_data['samp_on_us']) / 1000.\n",
    "        logging.info('Using MWorks digital signal for sample on time')\n",
    "    \n",
    "    # Load spikeTime file for current channel\n",
    "    spikeTimes = nwbfile.units[:].spike_times\n",
    "    # Re-order the psth to image x reps\n",
    "    max_number_of_reps = max(np.bincount(mwk_data['stimulus_presented']))  # Max reps obtained for any image\n",
    "    if max_number_of_reps == 0:\n",
    "        exit()\n",
    "    mwk_data['stimulus_presented'] = mwk_data['stimulus_presented'].astype(int)  # To avoid indexing errors\n",
    "    \n",
    "    if n_stimuli is None:\n",
    "            image_numbers = np.unique(mwk_data['stimulus_presented'])  # TODO: if not all images are shown (for eg, exp cut short), you'll have to manually type in total # images\n",
    "    else:\n",
    "        image_numbers = np.arange(1,n_stimuli+1) # all of my image starts with #1\n",
    "\n",
    "    timebase = np.arange(start_time, stop_time, timebin)\n",
    "    PSTH = np.full((len(image_numbers), max_number_of_reps, len(timebase),spikeTimes.shape[0]), np.nan)\n",
    "\n",
    "    for num in range(spikeTimes.shape[0]):\n",
    "        spikeTime = np.asanyarray(spikeTimes[num])\n",
    "        osamp = 10\n",
    "        psth_bin = np.zeros((len(samp_on_ms), osamp*(stop_time-start_time)))\n",
    "        psth_matrix = np.full((len(samp_on_ms), len(timebase)), np.nan)\n",
    "\n",
    "        for i in range(len(samp_on_ms)):\n",
    "\n",
    "            sidx = np.floor(osamp*(spikeTime[(spikeTime>=(samp_on_ms[i]+start_time))*(spikeTime<(samp_on_ms[i]+stop_time))]-(samp_on_ms[i]+start_time))).astype(int)\n",
    "            psth_bin[i, sidx] = 1\n",
    "            psth_matrix[i] = np.sum(np.reshape(psth_bin[i], [len(timebase), osamp*timebin]), axis=1)\n",
    "        \n",
    "        \n",
    "        psth = np.full((len(image_numbers), max_number_of_reps, len(timebase)), np.nan)  # Re-ordered PSTH\n",
    "\n",
    "        for i, image_num in enumerate(image_numbers):\n",
    "            index_in_table = np.where(mwk_data.stimulus_presented == image_num)[0]\n",
    "            selected_cells = psth_matrix[index_in_table, :]\n",
    "            psth[i, :selected_cells.shape[0], :] = selected_cells\n",
    "\n",
    "        logging.info(psth.shape)\n",
    "        # Save psth data\n",
    "        PSTH[:,:,:,num] = psth\n",
    "        \n",
    "    meta = {'start_time_ms': start_time, 'stop_time_ms': stop_time, 'tb_ms': timebin}\n",
    "    cmbined_psth = {'psth': PSTH, 'meta': meta}\n",
    "\n",
    "    return cmbined_psth\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory   = '/braintree/home/aliya277/dandi_brainscore/inventory'\n",
    "start_time  = 0\n",
    "stop_time   = 300\n",
    "timebin     = 10\n",
    "counter = 0\n",
    "\n",
    "\n",
    "for folder, num_file in tqdm(zip(os.listdir(inventory), range(len(os.listdir(inventory)))), \\\n",
    "    total = len(os.listdir(inventory)), desc='Processing ...'):\n",
    "\n",
    "\n",
    "    path = os.path.join(inventory, folder)\n",
    "    SessionName = folder\n",
    "    config_path = '/om/user/aliya277/dandi_brainscore'\n",
    "    with open(os.path.join(config_path,\"config_nwb.yaml\") , \"r\") as f:\n",
    "            config = yaml.load(f, Loader = yaml.FullLoader)\n",
    "\n",
    "    print('Creating NWB file...')\n",
    "    nwbfile = create_nwb(config,path)\n",
    "    \n",
    "\n",
    "    if 'h5Files' in os.listdir(path):\n",
    "        print('Opening PSTH...')\n",
    "        filename = os.listdir(os.path.join(path, 'h5Files'))[0]\n",
    "        file = h5py.File(os.path.join(path, 'h5Files', filename),'r+') \n",
    "        data = file['psth'][:]\n",
    "        file.close()\n",
    "\n",
    "\n",
    "        print('Adding PSTH...')\n",
    "        nwbfile.add_scratch(\n",
    "            data,\n",
    "            name=\"psth\",\n",
    "            description=\"psth, uncorrected [channels x stimuli x reps x timebins]\",\n",
    "            )\n",
    "        \n",
    "    else: counter +=1\n",
    "    \n",
    "    try: io.close()\n",
    "    except:pass\n",
    "\n",
    "    if os.path.isfile(os.path.join(path, f\"{SessionName}.nwb\")):\n",
    "         os.remove(os.path.join(path, f\"{SessionName}.nwb\"))\n",
    "\n",
    "    print('Saving NWB File..')\n",
    "    io = NWBHDF5IO(os.path.join(path, f\"{SessionName}.nwb\"), \"w\") \n",
    "    io.write(nwbfile)\n",
    "    io.close()\n",
    "    print(\"File saved.\")\n",
    "\n",
    "        # psth    = get_psth_from_nwb(nwbfile, path, start_time, stop_time, timebin)\n",
    "        # data    = psth['psth']\n",
    "\n",
    "print(f'{counter} SpikeTimes do not have an h5 file.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_names(filename):\n",
    "    assignment  = filename.split('.')[0].split('-')[1]\n",
    "    number      = filename.split('.')[0].split('-')[2]\n",
    "    return np.asarray([assignment, number])\n",
    "\n",
    "def create_nwb(config, path):\n",
    "\n",
    "    desired_timezone = pytz.timezone('US/Eastern')\n",
    "\n",
    "    ################ CREATE NWB FILE WITH METADATA ################################\n",
    "    ###############################################################################\n",
    "    nwbfile = NWBFile(\n",
    "        session_description     = config['session_info']['session_description'],\n",
    "        identifier              = config['metadata']['identifier'],\n",
    "        session_start_time      = desired_timezone.localize(config['metadata']['session_start_time']),\n",
    "        file_create_date        = desired_timezone.localize(config['metadata']['file_create_date']),\n",
    "        experimenter            = config['general']['lab_info']['experimenter'],\n",
    "        experiment_description  = config['general']['experiment_info']['experiment_description'],\n",
    "        session_id              = config['session_info']['session_id'],\n",
    "        lab                     = config['general']['lab_info']['lab'],                     \n",
    "        institution             = config['general']['lab_info']['institution'],                                    \n",
    "        keywords                = config['general']['experiment_info']['keywords'],\n",
    "        surgery                 = config['general']['experiment_info']['surgery']\n",
    "    )\n",
    "\n",
    "    ################ CREATE SUBJECT ################################################\n",
    "    ################################################################################\n",
    "    nwbfile.subject = Subject(\n",
    "        subject_id  = config['subject']['subject_id'],\n",
    "        date_of_birth= config['subject']['date_of_birth'],\n",
    "        species     = config['subject']['species'],\n",
    "        sex         = config['subject']['sex'],\n",
    "    )\n",
    "\n",
    "    ################ CREATE HARDWARE LINKS #########################################\n",
    "    ################################################################################\n",
    "    nwbfile.create_device(\n",
    "        name        = config['hardware']['system_name'], \n",
    "        description = config['hardware']['system_description'], \n",
    "        manufacturer= config['hardware']['system_manuf']\n",
    "    )\n",
    "\n",
    "    nwbfile.create_device(\n",
    "        name        = config['hardware']['adapter_manuf'], \n",
    "        description = config['hardware']['adapter_description'], \n",
    "        manufacturer= config['hardware']['adapter_manuf']\n",
    "    )\n",
    "\n",
    "    nwbfile.create_device(\n",
    "        name        = config['hardware']['monitor_name'], \n",
    "        description = config['hardware']['monitor_description'], \n",
    "        manufacturer= config['hardware']['monitor_manuf']\n",
    "    )\n",
    "\n",
    "    nwbfile.create_device(\n",
    "        name        = config['hardware']['photodiode_name'], \n",
    "        description = config['hardware']['photodiode_description'], \n",
    "        manufacturer= config['hardware']['photodiode_manuf']\n",
    "    )\n",
    "    \n",
    "    nwbfile.create_device(\n",
    "        name        = 'Software Used', \n",
    "        description = str(['Mworks Client: '+config['software']['mwclient_version'],\\\n",
    "                        'Mworks Server: '+config['software']['mwserver_version'],\\\n",
    "                        'OS: '+config['software']['OS'],\\\n",
    "                        'Intan :'+config['software']['intan_version']])\n",
    "    )\n",
    "\n",
    "    ################ CREATE ELECTRODE LINKS ########################################\n",
    "    ################################################################################\n",
    "    electrodes = nwbfile.create_device(\n",
    "        name        = config['hardware']['electrode_name'], \n",
    "        description = config['hardware']['electrode_description'], \n",
    "        manufacturer= config['hardware']['electrode_manuf']\n",
    "    )\n",
    "\n",
    "    all_files = sorted(os.listdir(os.path.join(path, 'SpikeTimes')))\n",
    "    \n",
    "    name_accumulator = []\n",
    "    for file in all_files:\n",
    "        name_accumulator.append(read_names(file))\n",
    "    names = np.vstack(name_accumulator)\n",
    "\n",
    "    nwbfile.add_electrode_column(name=\"label\", description=\"label of electrode\")\n",
    "    groups, count_groups = np.unique(names[:,0], return_counts =True)\n",
    "    ids                  = names[:,1]\n",
    "    counter              = 0\n",
    "    # create ElectrodeGroups A, B, C, ..\n",
    "    for group, count_group in zip(groups, count_groups):\n",
    "        if len(groups) == 6:\n",
    "            electrode_description = \"Serialnumber: {}. Adapter Version: {}\".format(config['array_info']['array_{}'.format(group)]['serialnumber'],\\\n",
    "                            config['array_info']['array_{}'.format(group)]['adapterversion']),\n",
    "        else: \n",
    "            electrode_description = \"Serialnumber: {}\".format(config['array_info']['array_{}'.format(group)]['serialnumber']),\n",
    "                \n",
    "        \n",
    "        electrode_group = nwbfile.create_electrode_group(\n",
    "            name        = \"group_{}\".format(group),\n",
    "            description = electrode_description[0],\n",
    "            device      = electrodes,\n",
    "            location    = 'hemisphere, region, subregion: '+str([config['array_info']['array_{}'.format(group)]['hemisphere'],\\\n",
    "                                config['array_info']['array_{}'.format(group)]['region'],\n",
    "                                config['array_info']['array_{}'.format(group)]['subregion']]),\n",
    "            position    = config['array_info']['array_{}'.format(group)]['position']\n",
    "        )\n",
    "\n",
    "        # create Electrodes 001, 002, ..., 032 in ElectrodeGroups per channel\n",
    "        for ichannel in range(count_group):\n",
    "            nwbfile.add_electrode(\n",
    "                group       = electrode_group,\n",
    "                label       = ids[counter],\n",
    "                location    = 'row, col, elec'+str(json.loads(config['array_info']['intan_electrode_labeling_[row,col,id]'])[counter])\n",
    "            )\n",
    "            counter += 1     \n",
    "\n",
    "    ################ ADD SPIKE TIMES ###############################################\n",
    "    ################################################################################\n",
    "\n",
    "    nwbfile.add_unit_column(name=\"unit\", description=\"millisecond\") \n",
    "    for filename, i in zip(sorted(os.listdir(os.path.join(path, 'SpikeTimes'))), range(len(os.listdir(os.path.join(path, 'SpikeTimes'))))):\n",
    "        [assignment, number] = read_names(filename)\n",
    "        file_path = os.path.join(path, 'SpikeTimes', filename)\n",
    "        data = scipy.io.loadmat(file_path, squeeze_me=True,\n",
    "                        variable_names='spike_time_ms')['spike_time_ms']\n",
    "        nwbfile.add_unit(\n",
    "            spike_times = data, \n",
    "            electrodes  = [i],\n",
    "            electrode_group = nwbfile.electrode_groups[f'group_{assignment}'], \n",
    "            unit = 'ms'\n",
    "        )\n",
    "\n",
    "    ################ ADD TRIAL TIMES ###############################################\n",
    "    ################################################################################\n",
    "    last_spike = data[-1]\n",
    "    del data\n",
    "    \n",
    "    try: \n",
    "        [on, off] = config['session_info']['session_description'].split(', ')[3].split(': ')[-1].split(\"/\")\n",
    "        on_start  = 0\n",
    "        on_dur    = int(on)\n",
    "        off_dur   = int(off)\n",
    "\n",
    "        \n",
    "        nwbfile.add_trial_column(name=\"unit\", description=\"millisecond\")\n",
    "        while on_start < last_spike:\n",
    "\n",
    "            nwbfile.add_trial(\n",
    "                start_time= float(on_start),\n",
    "                stop_time = float(on_start+on_dur),\n",
    "                unit = 'ms')\n",
    "        \n",
    "            on_start += on_dur+off_dur\n",
    "    except: pass \n",
    "\n",
    "    ################ ADD PSTH IF AVAIL #############################################\n",
    "    ################################################################################\n",
    "    if 'h5Files' in os.listdir(path):\n",
    "\n",
    "        filename = os.listdir(os.path.join(path, 'h5Files'))[0]\n",
    "        file = h5py.File(os.path.join(path, 'h5Files', filename),'r+') \n",
    "        data = file['psth'][:]\n",
    "        file.close()\n",
    "\n",
    "        nwbfile.add_scratch(\n",
    "            data,\n",
    "            name=\"psth\",\n",
    "            description=\"psth, uncorrected [stimuli x reps x timebins x channels]\",\n",
    "            )\n",
    "        \n",
    "\n",
    "    return nwbfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file did not work: norm_FOSS.sub_pico.20230823_124104.proc\n",
    "tail: output.log: Datei abgeschnitten\n",
    "This file did not work: norm_FOSS.sub_pico.20230803_105856.proc\n",
    "tail: output.log: Datei abgeschnitten\n",
    "This file did not work: exp_gratingsAdap_s3.sub_pico.20230801_163355.proc\n",
    "tail: output.log: Datei abgeschnitten\n",
    "tail: output.log: Datei abgeschnitten\n",
    "This file did not work: norm_FOSS.sub_pico.20230628_124854.proc\n",
    "tail: output.log: Datei abgeschnitten\n",
    "node069\n",
    "tail: output.log: Datei abgeschnitten\n",
    "tail: output.log: Datei abgeschnitten\n",
    "node069\n",
    "This file did not work: norm_FOSS.sub_pico.20220929_170635.proc\n",
    "5127.proc\n",
    "tail: output.log: Datei abgeschnitten\n",
    "tail: output.log: Datei abgeschnitten\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "OSS.sub_pico.20230125_144402.proc\n",
    "tail: output.log: Datei abgeschnitten\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "This file did not work: norm_FOSS.sub_pico.20230127_160227.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230126_150653.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230505_130540.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230531_134209.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230516_120137.proc\n",
    "This file did not work: exp_robustness_guy_d1_v34.sub_pico.20230906_122650.proc\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "This file did not work: norm_FOSS.sub_pico.20230510_111142.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230428_111937.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20220902_145351.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20220615_113442.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20220907_142157.proc\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "This file did not work: norm_FOSS.sub_pico.20230713_141950.proc\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "This file did not work: norm_FOSS.sub_pico.20230817_141050.proc\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "This file did not work: norm_FOSS.sub_pico.20230328_145456.proc\n",
    "This file did not work: exp_gratingsAdap_s1.sub_pico.20230801_151117.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230606_134244.proc\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "This file did not work: norm_FOSS.sub_pico.20230621_140747.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230814_122811.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230821_120107.proc\n",
    "This file did not work: exp_Mayo_day_5.sub_pico.20230818_101245.proc\n",
    "This file did not work: norm_HVM.sub_pico.20230404_142414.proc\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "This file did not work: norm_FOSS.sub_pico.20230626_131126.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230629_132813.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230711_142158.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230630_135832.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230725_113504.proc\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "This file did not work: norm_FOSS.sub_pico.20230830_110931.proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = 0\n",
    "stop_time = 300\n",
    "timebin = 10\n",
    "\n",
    "\n",
    "def get_psth_from_nwb(nwbfile, path, start_time, stop_time, timebin, n_stimuli=None):\n",
    "\n",
    "    # Find the MWORKS File\n",
    "    with open(os.path.join(path, 'NWBInfo.txt')) as f:\n",
    "        lines = f.readlines()\n",
    "        line2 = lines[0].split(',')[1]\n",
    "        ind = line2.split('/').index('intanproc')\n",
    "        mwk_file = os.path.join('/', *line2.split('/')[0:ind], 'mworksproc', line2.split('/')[ind+1]+'_mwk.csv')\n",
    "    assert os.path.isfile(mwk_file)==True\n",
    "\n",
    "    ################ MODIFIED FROM THE SPIKE-TOOLS-CHONG CODE ######################\n",
    "    ################################################################################\n",
    "    \n",
    "    mwk_data = pd.read_csv(mwk_file)\n",
    "    mwk_data = mwk_data[mwk_data.fixation_correct == 1]\n",
    "    if 'photodiode_on_us' in mwk_data.keys():\n",
    "        samp_on_ms = np.asarray(mwk_data['photodiode_on_us']) / 1000.\n",
    "        logging.info('Using photodiode signal for sample on time')\n",
    "    else:\n",
    "        samp_on_ms = np.asarray(mwk_data['samp_on_us']) / 1000.\n",
    "        logging.info('Using MWorks digital signal for sample on time')\n",
    "    \n",
    "    # Load spikeTime file for current channel\n",
    "    spikeTimes = nwbfile.units[:].spike_times\n",
    "    # Re-order the psth to image x reps\n",
    "    max_number_of_reps = max(np.bincount(mwk_data['stimulus_presented']))  # Max reps obtained for any image\n",
    "    if max_number_of_reps == 0:\n",
    "        exit()\n",
    "    mwk_data['stimulus_presented'] = mwk_data['stimulus_presented'].astype(int)  # To avoid indexing errors\n",
    "    \n",
    "    if n_stimuli is None:\n",
    "            image_numbers = np.unique(mwk_data['stimulus_presented'])  # TODO: if not all images are shown (for eg, exp cut short), you'll have to manually type in total # images\n",
    "    else:\n",
    "        image_numbers = np.arange(1,n_stimuli+1) # all of my image starts with #1\n",
    "\n",
    "    timebase = np.arange(start_time, stop_time, timebin)\n",
    "    PSTH = np.full((len(image_numbers), max_number_of_reps, len(timebase),spikeTimes.shape[0]), np.nan)\n",
    "\n",
    "    for num in range(spikeTimes.shape[0]):\n",
    "        spikeTime = np.asanyarray(spikeTimes[num])\n",
    "        osamp = 10\n",
    "        psth_bin = np.zeros((len(samp_on_ms), osamp*(stop_time-start_time)))\n",
    "        psth_matrix = np.full((len(samp_on_ms), len(timebase)), np.nan)\n",
    "\n",
    "        for i in range(len(samp_on_ms)):\n",
    "\n",
    "            sidx = np.floor(osamp*(spikeTime[(spikeTime>=(samp_on_ms[i]+start_time))*(spikeTime<(samp_on_ms[i]+stop_time))]-(samp_on_ms[i]+start_time))).astype(int)\n",
    "            psth_bin[i, sidx] = 1\n",
    "            psth_matrix[i] = np.sum(np.reshape(psth_bin[i], [len(timebase), osamp*timebin]), axis=1)\n",
    "        \n",
    "        \n",
    "        psth = np.full((len(image_numbers), max_number_of_reps, len(timebase)), np.nan)  # Re-ordered PSTH\n",
    "\n",
    "        for i, image_num in enumerate(image_numbers):\n",
    "            index_in_table = np.where(mwk_data.stimulus_presented == image_num)[0]\n",
    "            selected_cells = psth_matrix[index_in_table, :]\n",
    "            psth[i, :selected_cells.shape[0], :] = selected_cells\n",
    "\n",
    "        logging.info(psth.shape)\n",
    "        # Save psth data\n",
    "        PSTH[:,:,:,num] = psth\n",
    "        \n",
    "    meta = {'start_time_ms': start_time, 'stop_time_ms': stop_time, 'tb_ms': timebin}\n",
    "    cmbined_psth = {'psth': PSTH, 'meta': meta}\n",
    "\n",
    "    return cmbined_psth\n",
    "    \n",
    "# psth = get_psth_from_nwb(nwbfile, path, start_time, stop_time, timebin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_nwb(nwbfile, path):\n",
    "    \n",
    "    SessionName = path.split('/')[-1]\n",
    "\n",
    "    # Crete Temporary path on BrainTree (Openmind Did not Allow Saving of H5 Files.)\n",
    "    temp_path = '/braintree/home/aliya277/dandi_brainscore/inventory'\n",
    "    try: os.mkdir(temp_path)\n",
    "    except: pass\n",
    "\n",
    "    # Save NWB File on Braintree\n",
    "    io = NWBHDF5IO(os.path.join(temp_path, f\"{SessionName}.nwb\"), \"w\") \n",
    "    io.write(nwbfile)\n",
    "    io.close()\n",
    "\n",
    "    # Copy NWB to Inventory Directory on Openmind\n",
    "    shutil.copy2(os.path.join(temp_path, f\"{SessionName}.nwb\"), os.path.join(path, f\"{SessionName}.nwb\"))   \n",
    "    \n",
    "    # Remove Temporary File and Folder on Braintree\n",
    "    os.remove(os.path.join(temp_path, f\"{SessionName}.nwb\"))\n",
    "    os.rmdir(os.path.join(temp_path))\n",
    "\n",
    "    return nwbfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'psth' already exists in NWBFile 'root'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/om/user/aliya277/dandi_brainscore/create_proc_nwb.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bopenmind.mit.edu/om/user/aliya277/dandi_brainscore/create_proc_nwb.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m data \u001b[39m=\u001b[39m psth[\u001b[39m'\u001b[39m\u001b[39mpsth\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bopenmind.mit.edu/om/user/aliya277/dandi_brainscore/create_proc_nwb.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m nwbfile\u001b[39m.\u001b[39;49madd_scratch(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bopenmind.mit.edu/om/user/aliya277/dandi_brainscore/create_proc_nwb.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     data,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bopenmind.mit.edu/om/user/aliya277/dandi_brainscore/create_proc_nwb.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpsth\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bopenmind.mit.edu/om/user/aliya277/dandi_brainscore/create_proc_nwb.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpsth, uncorrected [channels x stimuli x reps x timebins]\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bopenmind.mit.edu/om/user/aliya277/dandi_brainscore/create_proc_nwb.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.10/site-packages/hdmf/utils.py:644\u001b[0m, in \u001b[0;36mdocval.<locals>.dec.<locals>.func_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfunc_call\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    643\u001b[0m     pargs \u001b[39m=\u001b[39m _check_args(args, kwargs)\n\u001b[0;32m--> 644\u001b[0m     \u001b[39mreturn\u001b[39;00m func(args[\u001b[39m0\u001b[39;49m], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.10/site-packages/pynwb/file.py:1097\u001b[0m, in \u001b[0;36mNWBFile.add_scratch\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m     \u001b[39mif\u001b[39;00m description \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1095\u001b[0m         warn(\u001b[39m'\u001b[39m\u001b[39mThe description argument is ignored when adding an NWBContainer, ScratchData, or \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1096\u001b[0m              \u001b[39m'\u001b[39m\u001b[39mDynamicTable to scratch.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1097\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_scratch(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.10/site-packages/hdmf/utils.py:644\u001b[0m, in \u001b[0;36mdocval.<locals>.dec.<locals>.func_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfunc_call\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    643\u001b[0m     pargs \u001b[39m=\u001b[39m _check_args(args, kwargs)\n\u001b[0;32m--> 644\u001b[0m     \u001b[39mreturn\u001b[39;00m func(args[\u001b[39m0\u001b[39;49m], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.10/site-packages/hdmf/container.py:1031\u001b[0m, in \u001b[0;36mMultiContainerInterface.__make_add.<locals>._func\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1029\u001b[0m     \u001b[39mif\u001b[39;00m tmp\u001b[39m.\u001b[39mname \u001b[39min\u001b[39;00m d:\n\u001b[1;32m   1030\u001b[0m         msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m already exists in \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (tmp\u001b[39m.\u001b[39mname, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[0;32m-> 1031\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1032\u001b[0m     d[tmp\u001b[39m.\u001b[39mname] \u001b[39m=\u001b[39m tmp\n\u001b[1;32m   1033\u001b[0m \u001b[39mreturn\u001b[39;00m container\n",
      "\u001b[0;31mValueError\u001b[0m: 'psth' already exists in NWBFile 'root'"
     ]
    }
   ],
   "source": [
    "data = psth['psth']\n",
    "\n",
    "nwbfile.add_scratch(\n",
    "    data,\n",
    "    name=\"psth\",\n",
    "    description=\"psth, uncorrected [channels x stimuli x reps x timebins]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 86, 21, 30)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwbfile.scratch['psth'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5path = '/om/user/aliya277/inventory/norm_FOSS.sub_pico.20230823_124104.proc/h5Files/230823.pico.rsvp.normalizers.experiment_psth_raw.h5'\n",
    "\n",
    "def open_h5(path):\n",
    "    \n",
    "    filename = path.split('/')[-1]\n",
    "    # Crete Temporary path on BrainTree (Openmind Did not Allow Saving of H5 Files.)\n",
    "    temp_path = '/braintree/home/aliya277/dandi_brainscore/inventory'\n",
    "    try: os.mkdir(temp_path)\n",
    "    except: pass\n",
    "\n",
    "\n",
    "    # Copy File to Temporary Path\n",
    "    shutil.copy2(os.path.join(path), os.path.join(temp_path, filename))   \n",
    "\n",
    "    file = h5py.File(os.path.join(temp_path, filename),'r+')    \n",
    "\n",
    "    return file\n",
    "\n",
    "def close_h5(path):\n",
    "\n",
    "    filename = path.split('/')[-1]\n",
    "    temp_path = '/braintree/home/aliya277/dandi_brainscore/inventory'\n",
    "    os.remove(os.path.join(temp_path, filename))\n",
    "    os.rmdir(temp_path)\n",
    "\n",
    "file = open_h5(h5path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(psth['psth'], file['psth'][:], equal_nan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "0      [69.2, 135.55, 207.55, 242.9, 281.95, 331.5, 3...\n",
       "1      [22.1, 57.3, 63.9, 135.35, 167.1, 175.6, 225.8...\n",
       "2      [58.1, 69.15, 167.7, 171.1, 171.3, 286.9, 362....\n",
       "3      [16.9, 158.04999999999998, 237.9, 304.09999999...\n",
       "4      [49.75, 68.9, 87.65, 115.1, 342.95, 412.650000...\n",
       "                             ...                        \n",
       "187    [2.6, 5.3, 81.3, 155.35, 185.35, 251.649999999...\n",
       "188    [35.2, 164.14999999999998, 164.70000000000002,...\n",
       "189    [35.2, 155.35, 251.8, 286.95, 287.1, 287.4, 29...\n",
       "190    [2.6, 162.35, 198.8, 251.5, 251.7, 287.2, 294....\n",
       "191    [46.449999999999996, 73.85, 81.44999999999999,...\n",
       "Name: spike_times, Length: 192, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwbfile.units[:].spike_times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547d15e3cf1a43ceb91679bc5cf39827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='session_description:', layout=Layout(max_height='40px', max_width='…"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwb2widget(nwbfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dandibs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
