{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used after the inventory is created. It is creating config files for each recording and creating nwb files for spike times and psth (if avail). Finally validating the nwb files, it then uploades the files to dandi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from uuid import uuid4\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import os, yaml, glob, json\n",
    "import pandas as pd\n",
    "from nwbwidgets import nwb2widget\n",
    "from pynwb import NWBHDF5IO, NWBFile\n",
    "from pynwb.file import Subject\n",
    "import shutil\n",
    "import logging\n",
    "import h5py\n",
    "import pytz\n",
    "from tqdm import tqdm\n",
    "from utils.nwb_helper import  create_nwb, calc_psth\n",
    "from utils.config_helper import create_yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create NWB with SpikeTimes and PSTH (.mat) if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Create Custom Config Files for Each Recording #################\n",
    "###############################################################################\n",
    "\n",
    "df = pd.read_excel( '/braintree/home/aliya277/dandi_brainscore/pico_inventory_new.xlsx'  )\n",
    "SubjectName = 'pico'\n",
    "storage_dir = '/braintree/home/aliya277/inventory_new'\n",
    "array_meta_path  = '/braintree/data2/active/users/sgouldin/array-metadata'\n",
    "\n",
    "for index, DataFrame in df.iterrows():\n",
    "        \n",
    "    if DataFrame['Has SpikeTime'] == 1:\n",
    "\n",
    "        num_files = len(os.listdir(DataFrame['Path: SpikeTimes']))\n",
    "        if num_files == 192: \n",
    "            array_metadata = os.path.join(array_meta_path, '021023_pico_mapping_noCIT_adapter_version.json')\n",
    "            adapter_info_avail = True\n",
    "        elif num_files == 288: \n",
    "            array_metadata = os.path.join(array_meta_path,'pico_firstmapping_Lhem_2023.json')\n",
    "            adapter_info_avail = False\n",
    "\n",
    "        create_yaml(storage_dir, DataFrame, array_metadata, adapter_info_avail)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Merge nwb file per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating NWB File for exp_novel500.sub_pico.20230530_132227.proc\n",
      "Saving NWB File.\n"
     ]
    }
   ],
   "source": [
    "############### Iterate through every File with SpikeTime and Create NWB ######\n",
    "###############################################################################\n",
    "\n",
    "df = pd.read_excel( '/braintree/home/aliya277/dandi_brainscore/pico_inventory_new.xlsx'  )\n",
    "SubjectName = 'pico'\n",
    "storage_dir = '/braintree/home/aliya277/inventory_new'\n",
    " \n",
    "\n",
    "i = 1\n",
    "for index, DataFrame in df.iterrows():\n",
    "        \n",
    "    if DataFrame['Has SpikeTime'] == 1:\n",
    "\n",
    "        date = f\"20{DataFrame['date']}\"\n",
    "        if len(str(DataFrame['time'])) != 6: time = f\"0{DataFrame['time']}\"\n",
    "        else: time = str(DataFrame['time'])\n",
    "        \n",
    "        if DataFrame['ImageSet'] == 'normalizers':\n",
    "            directory = f'norm_FOSS.sub_pico.{date}_{time}.proc'\n",
    "        elif DataFrame['ImageSet'] == 'normalizers-HVM':\n",
    "            directory = f'norm_HVM.sub_pico.{date}_{time}.proc'\n",
    "        else: \n",
    "            directory = f\"exp_{DataFrame['ImageSet']}.sub_pico.{date}_{time}.proc\"\n",
    "\n",
    "        \n",
    "        imagesetdir = os.path.join(storage_dir, \".\".join(directory.split(\".\")[0:1]))\n",
    "        subjectdir  = os.path.join(storage_dir, imagesetdir, \".\".join(directory.split(\".\")[0:2]))\n",
    "        subjectdir_date  = os.path.join(subjectdir, \".\".join(directory.split(\".\")[0:2])+'.'+date)\n",
    "        \n",
    "        if i > 169:\n",
    "            print(f'Creating NWB File for {directory}')\n",
    "            with open(os.path.join(subjectdir_date,directory,f\"config_nwb.yaml\") , \"r\") as f:\n",
    "                config = yaml.load(f, Loader = yaml.FullLoader)\n",
    "            \n",
    "            nwbfile = create_nwb(config, os.path.join(subjectdir_date,directory))\n",
    "            \n",
    "            print('Saving NWB File.')\n",
    "            io = NWBHDF5IO(os.path.join(os.path.join(subjectdir_date,directory), f\"{directory}.nwb\"), \"w\") \n",
    "            io.write(nwbfile)\n",
    "            io.close()\n",
    "            print(f\"File {i} saved.\")\n",
    "\n",
    "        i += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Iterate through every File with SpikeTime and Create NWB ######\n",
    "###############################################################################\n",
    "\n",
    "df = pd.read_excel( '/braintree/home/aliya277/dandi_brainscore/pico_inventory_new.xlsx'  )\n",
    "SubjectName = 'pico'\n",
    "storage_dir = '/braintree/home/aliya277/inventory_new'\n",
    " \n",
    "\n",
    "i = 1\n",
    "for index, DataFrame in df.iterrows():\n",
    "        \n",
    "    if DataFrame['Has SpikeTime'] == 1:\n",
    "\n",
    "        date = f\"20{DataFrame['date']}\"\n",
    "        if len(str(DataFrame['time'])) != 6: time = f\"0{DataFrame['time']}\"\n",
    "        else: time = str(DataFrame['time'])\n",
    "        \n",
    "        if DataFrame['ImageSet'] == 'normalizers':\n",
    "            directory = f'norm_FOSS.sub_pico.{date}_{time}.proc'\n",
    "        elif DataFrame['ImageSet'] == 'normalizers-HVM':\n",
    "            directory = f'norm_HVM.sub_pico.{date}_{time}.proc'\n",
    "        else: \n",
    "            directory = f\"exp_{DataFrame['ImageSet']}.sub_pico.{date}_{time}.proc\"\n",
    "\n",
    "        \n",
    "        imagesetdir = os.path.join(storage_dir, \".\".join(directory.split(\".\")[0:1]))\n",
    "        subjectdir  = os.path.join(storage_dir, imagesetdir, \".\".join(directory.split(\".\")[0:2]))\n",
    "        subjectdir_date  = os.path.join(subjectdir, \".\".join(directory.split(\".\")[0:2])+'.'+date)\n",
    "        \n",
    "        print(f'Creating NWB File for {directory}')\n",
    "        with open(os.path.join(subjectdir_date,directory,f\"config_nwb.yaml\") , \"r\") as f:\n",
    "            config = yaml.load(f, Loader = yaml.FullLoader)\n",
    "        \n",
    "        nwbfile = create_nwb(config, os.path.join(subjectdir_date,directory))\n",
    "\n",
    "        if os.path.isfile(os.path.join(os.path.join(subjectdir,directory), f\"{directory}.nwb\")):\n",
    "            os.remove(os.path.join(os.path.join(subjectdir,directory), f\"{directory}.nwb\"))\n",
    "\n",
    "        print('Saving NWB File.')\n",
    "        io = NWBHDF5IO(os.path.join(os.path.join(subjectdir,directory), f\"{directory}.nwb\"), \"w\") \n",
    "        io.write(nwbfile)\n",
    "        io.close()\n",
    "        print(f\"File {i} saved.\")\n",
    "\n",
    "\n",
    "        i += 1\n",
    "    \n",
    "print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n"
     ]
    }
   ],
   "source": [
    "############### Check if All Files are Written and can be Opened ##############\n",
    "###############################################################################\n",
    "\n",
    "df = pd.read_excel( '/braintree/home/aliya277/dandi_brainscore/pico_inventory.xlsx'  )\n",
    "SubjectName = 'pico'\n",
    "storage_dir = '/braintree/home/aliya277/inventory'\n",
    "array_meta_path  = '/braintree/data2/active/users/sgouldin/array-metadata'\n",
    " \n",
    "\n",
    "i = 0\n",
    "for index, DataFrame in df.iterrows():\n",
    "        \n",
    "    if DataFrame['Has SpikeTime'] == 1:\n",
    "\n",
    "        date = f\"20{DataFrame['date']}\"\n",
    "        if len(str(DataFrame['time'])) != 6: time = f\"0{DataFrame['time']}\"\n",
    "        else: time = str(DataFrame['time'])\n",
    "        \n",
    "        if DataFrame['ImageSet'] == 'normalizers':\n",
    "            directory = f'norm_FOSS.sub_pico.{date}_{time}.proc'\n",
    "        elif DataFrame['ImageSet'] == 'normalizers-HVM':\n",
    "            directory = f'norm_HVM.sub_pico.{date}_{time}.proc'\n",
    "        else: \n",
    "            directory = f\"exp_{DataFrame['ImageSet']}.sub_pico.{date}_{time}.proc\"\n",
    "\n",
    "        \n",
    "        imagesetdir = os.path.join(storage_dir, \".\".join(directory.split(\".\")[0:1]))\n",
    "        subjectdir  = os.path.join(storage_dir, imagesetdir, \".\".join(directory.split(\".\")[0:2]))\n",
    "\n",
    "        try:\n",
    "                io = NWBHDF5IO(os.path.join(subjectdir,directory, f\"{directory}.nwb\"), \"r\") \n",
    "                nwbfile = io.read()\n",
    "                # display(nwbfile)\n",
    "                io.close()\n",
    "        except: print(f'{i}: This File can not be opened: {directory}')\n",
    "\n",
    "        i += 1\n",
    "    \n",
    "print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute PSTH if not available and add to NWB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: norm_FOSS.sub_pico.20220616_111545.proc\n",
      "mworks file does not exist.\n",
      "file: norm_FOSS.sub_pico.20220615_113442.proc\n",
      "mworks file does not exist.\n",
      "file: norm_FOSS.sub_pico.20220907_142157.proc\n",
      "mworks file does not exist.\n",
      "file: norm_FOSS.sub_pico.20220929_170635.proc\n",
      "mworks file does not exist.\n",
      "file: norm_FOSS.sub_pico.20230328_145456.proc\n",
      "True\n",
      "file: norm_FOSS.sub_pico.20230428_111937.proc\n"
     ]
    }
   ],
   "source": [
    "############### Open nwb file with .csv #######################################\n",
    "###############################################################################\n",
    "\n",
    "df = pd.read_excel( '/braintree/home/aliya277/dandi_brainscore/pico_inventory.xlsx'  )\n",
    "SubjectName = 'pico'\n",
    "storage_dir = '/braintree/home/aliya277/inventory'\n",
    "array_meta_path  = '/braintree/data2/active/users/sgouldin/array-metadata'\n",
    "\n",
    "counter = 0\n",
    "for index, DataFrame in df.iterrows():\n",
    "        \n",
    "    if DataFrame['Has SpikeTime'] == 1 and DataFrame['Has h5']== 1:\n",
    "\n",
    "        date = f\"20{DataFrame['date']}\"\n",
    "        if len(str(DataFrame['time'])) != 6: time = f\"0{DataFrame['time']}\"\n",
    "        else: time = str(DataFrame['time'])\n",
    "        \n",
    "        if DataFrame['ImageSet'] == 'normalizers':\n",
    "            directory = f'norm_FOSS.sub_pico.{date}_{time}.proc'\n",
    "        elif DataFrame['ImageSet'] == 'normalizers-HVM':\n",
    "            directory = f'norm_HVM.sub_pico.{date}_{time}.proc'\n",
    "        else: \n",
    "            directory = f\"exp_{DataFrame['ImageSet']}.sub_pico.{date}_{time}.proc\"\n",
    "\n",
    "        \n",
    "        imagesetdir = os.path.join(storage_dir, \".\".join(directory.split(\".\")[0:1]))\n",
    "        subjectdir  = os.path.join(storage_dir, imagesetdir, \".\".join(directory.split(\".\")[0:2]))\n",
    "        mworks_dir    = os.path.join('/', *DataFrame['Path: intanraw'].split('/')[:10], 'mworksproc', DataFrame['Path: intanraw'].split('/')[11]+'_mwk.csv')\n",
    "        \n",
    "            \n",
    "        io = NWBHDF5IO(os.path.join(subjectdir,directory, f\"{directory}.nwb\"), \"r\") \n",
    "        nwbfile = io.read()\n",
    "\n",
    "        psth = nwbfile.scratch['psth'][:]\n",
    "        [start_time_ms, stop_time_ms, tb_ms] = nwbfile.scratch['psth meta'][:] \n",
    "        print('file:', directory)\n",
    "        if os.path.isfile(mworks_dir):\n",
    "            new_psth = calc_psth(nwbfile, mworks_dir, start_time_ms, stop_time_ms, tb_ms, n_stimuli = None)\n",
    "            diff = np.abs(new_psth - psth)\n",
    "\n",
    "            # Define a tolerance\n",
    "            tolerance = 0.01\n",
    "\n",
    "            # Find indices where the difference exceeds the tolerance\n",
    "            diff_indices = np.where(diff > tolerance)\n",
    "            if diff_indices[0].size:\n",
    "                print(\"The arrays differ at the following indices:\")\n",
    "                for index in diff_indices[0]:\n",
    "                    print(f\"Index {index}: newpsth has {array1[index]}, psth has {array2[index]}, difference is {diff[index]}\")\n",
    "            else:\n",
    "                print(\"The arrays are almost the same within the given tolerance.\")\n",
    "\n",
    "        else: print('mworks file does not exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: norm_FOSS.sub_pico.20230328_145456.proc\n",
      "True\n",
      "file: norm_FOSS.sub_pico.20230428_111937.proc\n",
      "False\n",
      "nan\n",
      "[[[[ 2.  4.  3. ...  5.  2.  3.]\n",
      "   [ 1.  0.  2. ...  4.  2.  3.]\n",
      "   [ 2.  3.  2. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 0.  2.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  1.  0. ...  0.  0.  0.]]\n",
      "\n",
      "  [[ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 6.  4.  4. ...  4.  5.  6.]\n",
      "   ...\n",
      "   [ 1.  3.  1. ...  2.  1.  0.]\n",
      "   [ 1.  1.  2. ...  2.  1.  2.]\n",
      "   [ 2.  3.  2. ...  2.  1.  3.]]\n",
      "\n",
      "  [[ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  1.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 0.  1.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 1.  1.  4. ...  2.  7.  1.]\n",
      "   [ 1.  1.  3. ...  1.  1.  0.]\n",
      "   [ 0.  1.  0. ...  0.  0.  1.]\n",
      "   ...\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      "  [[ 4.  7.  5. ...  3.  3.  5.]\n",
      "   [ 2.  0.  0. ...  2.  3.  1.]\n",
      "   [ 0.  0.  1. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 1.  1.  1. ...  1.  1.  1.]\n",
      "   [ 0.  1.  0. ...  0.  0.  0.]\n",
      "   [ 1.  2.  1. ...  2.  4.  2.]]\n",
      "\n",
      "  [[nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   ...\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]]]\n",
      "\n",
      "\n",
      " [[[ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  2.  1.  2.]\n",
      "   ...\n",
      "   [ 3.  4.  2. ...  1.  1.  1.]\n",
      "   [ 0.  0.  2. ...  1.  2.  1.]\n",
      "   [ 4.  3.  3. ...  3.  4.  1.]]\n",
      "\n",
      "  [[ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  1.]\n",
      "   [ 0.  1.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 4.  3.  2. ...  2.  3.  4.]\n",
      "   [ 1.  2.  0. ...  1.  1.  1.]\n",
      "   [ 1.  1.  2. ...  0.  0.  1.]]\n",
      "\n",
      "  [[ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 0.  0.  0. ...  3.  1.  4.]\n",
      "   [ 0.  1.  1. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.  0.  0. ...  0.  1.  0.]\n",
      "   [ 1.  2.  0. ...  4.  6.  0.]\n",
      "   [ 1.  1.  1. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 0.  1.  2. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  1.]]\n",
      "\n",
      "  [[ 1.  3.  1. ...  0.  0.  0.]\n",
      "   [ 0.  1.  0. ...  0.  0.  2.]\n",
      "   [ 0.  1.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 4.  5.  5. ...  6.  4.  3.]\n",
      "   [ 1.  0.  0. ...  0.  1.  1.]]\n",
      "\n",
      "  [[nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   ...\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]]]\n",
      "\n",
      "\n",
      " [[[ 4.  5.  4. ...  2.  1.  1.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 1.  4.  3. ...  5.  4.  1.]\n",
      "   ...\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  1.  0. ...  0.  0.  0.]\n",
      "   [ 1.  2.  1. ...  1.  0.  0.]]\n",
      "\n",
      "  [[ 1.  2.  1. ...  3.  3.  1.]\n",
      "   [ 0.  0.  0. ...  4.  3.  2.]\n",
      "   [ 2.  2.  1. ...  1.  2.  2.]\n",
      "   ...\n",
      "   [ 0.  1.  0. ...  2.  3.  0.]\n",
      "   [ 3.  4.  3. ...  0.  0.  0.]\n",
      "   [ 1.  0.  0. ...  2.  2.  1.]]\n",
      "\n",
      "  [[ 0.  0.  0. ...  0.  0.  1.]\n",
      "   [ 4.  5.  6. ...  6.  5.  4.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 0.  3.  1. ...  4.  2.  1.]\n",
      "   [ 2.  0.  0. ...  0.  0.  0.]\n",
      "   [ 3.  2.  1. ...  3.  3.  3.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  2. ...  1.  1.  1.]\n",
      "   [ 2.  2.  2. ...  1.  2.  1.]\n",
      "   ...\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      "  [[ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 5.  5.  2. ...  3.  3.  2.]\n",
      "   [ 1.  0.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      "  [[nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   ...\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[ 3.  3.  4. ...  2.  3.  3.]\n",
      "   [ 3.  4.  3. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 0.  0.  1. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  2.  0. ...  0.  0.  0.]]\n",
      "\n",
      "  [[ 1.  1.  0. ...  1.  1.  3.]\n",
      "   [ 0.  0.  0. ...  3.  3.  1.]\n",
      "   [ 1.  3.  2. ...  5.  5.  2.]\n",
      "   ...\n",
      "   [ 0.  0.  2. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  1.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      "  [[ 5.  6.  6. ...  3.  4.  1.]\n",
      "   [ 0.  0.  0. ...  3.  4.  1.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  1.  0. ...  0.  0.  0.]\n",
      "   [ 2.  3.  1. ...  4.  5.  3.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 1.  2.  1. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  1.  0.  0.]\n",
      "   [ 3.  2.  3. ...  1.  1.  4.]\n",
      "   ...\n",
      "   [ 2.  2.  3. ...  0.  0.  0.]\n",
      "   [ 0.  1.  1. ...  0.  0.  1.]\n",
      "   [ 0.  0.  3. ...  2.  2.  1.]]\n",
      "\n",
      "  [[ 0.  1.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  1.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      "  [[nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   ...\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]]]\n",
      "\n",
      "\n",
      " [[[ 0.  0.  0. ...  0.  1.  0.]\n",
      "   [ 0.  5.  2. ...  2.  3.  1.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 0.  0.  0. ...  1.  2.  1.]\n",
      "   [ 1.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      "  [[ 2.  0.  1. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  4.  2.  1.]\n",
      "   [ 1.  0.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 0.  0.  0. ...  1.  1.  1.]\n",
      "   [ 2.  4.  2. ...  2.  2.  0.]\n",
      "   [ 2.  5.  4. ...  4.  4.  0.]]\n",
      "\n",
      "  [[ 0.  1.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 1.  0.  4. ...  3.  3.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  2.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 0.  1.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      "  [[ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  1.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  1.  0. ...  0.  0.  0.]]\n",
      "\n",
      "  [[nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   ...\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]]]\n",
      "\n",
      "\n",
      " [[[ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  1.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 4.  3.  3. ...  3.  1.  2.]\n",
      "   [ 0.  1.  1. ...  1.  4.  1.]\n",
      "   [ 1.  0.  0. ...  1.  0.  0.]]\n",
      "\n",
      "  [[ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  1.  0. ...  0.  0.  0.]\n",
      "   [ 0.  1.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 1.  4.  1. ...  1.  1.  2.]\n",
      "   [ 2.  1.  2. ...  1.  2.  0.]\n",
      "   [ 1.  5.  4. ...  4.  2.  2.]]\n",
      "\n",
      "  [[ 2.  2.  3. ...  2.  2.  2.]\n",
      "   [ 0.  1.  0. ...  0.  1.  1.]\n",
      "   [ 1.  2.  1. ...  1.  0.  2.]\n",
      "   ...\n",
      "   [ 0.  1.  0. ...  1.  2.  1.]\n",
      "   [ 2.  4.  2. ...  1.  0.  0.]\n",
      "   [ 0.  0.  1. ...  2.  4.  3.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.  0.  0. ...  1.  0.  1.]\n",
      "   [ 0.  1.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  1.  0. ...  0.  0.  0.]]\n",
      "\n",
      "  [[ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 1.  0.  0. ...  1.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 2.  0.  0. ...  1.  2.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      "  [[nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   ...\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]]]]\n",
      "[[[[ 2.  4.  3. ...  5.  2.  3.]\n",
      "   [ 1.  0.  2. ...  4.  2.  3.]\n",
      "   [ 2.  3.  2. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 0.  2.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  1.  0. ...  0.  0.  0.]]\n",
      "\n",
      "  [[ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 6.  4.  4. ...  4.  5.  6.]\n",
      "   ...\n",
      "   [ 1.  3.  1. ...  2.  1.  0.]\n",
      "   [ 1.  1.  2. ...  2.  1.  2.]\n",
      "   [ 2.  3.  2. ...  2.  1.  3.]]\n",
      "\n",
      "  [[ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  1.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 0.  1.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 1.  1.  4. ...  2.  7.  1.]\n",
      "   [ 1.  1.  3. ...  1.  1.  0.]\n",
      "   [ 0.  1.  0. ...  0.  0.  1.]\n",
      "   ...\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      "  [[ 4.  7.  5. ...  3.  3.  5.]\n",
      "   [ 2.  0.  0. ...  2.  3.  1.]\n",
      "   [ 0.  0.  1. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 1.  1.  1. ...  1.  1.  1.]\n",
      "   [ 0.  1.  0. ...  0.  0.  0.]\n",
      "   [ 1.  2.  1. ...  2.  4.  2.]]\n",
      "\n",
      "  [[nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   ...\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]]]\n",
      "\n",
      "\n",
      " [[[ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  2.  1.  2.]\n",
      "   ...\n",
      "   [ 3.  4.  2. ...  1.  1.  1.]\n",
      "   [ 0.  0.  2. ...  1.  2.  1.]\n",
      "   [ 4.  3.  3. ...  3.  4.  1.]]\n",
      "\n",
      "  [[ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  1.]\n",
      "   [ 0.  1.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 4.  3.  2. ...  2.  3.  4.]\n",
      "   [ 1.  2.  0. ...  1.  1.  1.]\n",
      "   [ 1.  1.  2. ...  0.  0.  1.]]\n",
      "\n",
      "  [[ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 0.  0.  0. ...  3.  1.  4.]\n",
      "   [ 0.  1.  1. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.  0.  0. ...  0.  1.  0.]\n",
      "   [ 1.  2.  0. ...  4.  6.  0.]\n",
      "   [ 1.  1.  1. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 0.  1.  2. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  1.]]\n",
      "\n",
      "  [[ 1.  3.  1. ...  0.  0.  0.]\n",
      "   [ 0.  1.  0. ...  0.  0.  2.]\n",
      "   [ 0.  1.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 4.  5.  5. ...  6.  4.  3.]\n",
      "   [ 1.  0.  0. ...  0.  1.  1.]]\n",
      "\n",
      "  [[nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   ...\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]]]\n",
      "\n",
      "\n",
      " [[[ 4.  5.  4. ...  2.  1.  1.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 1.  4.  3. ...  5.  4.  1.]\n",
      "   ...\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  1.  0. ...  0.  0.  0.]\n",
      "   [ 1.  2.  1. ...  1.  0.  0.]]\n",
      "\n",
      "  [[ 1.  2.  1. ...  3.  3.  1.]\n",
      "   [ 0.  0.  0. ...  4.  3.  2.]\n",
      "   [ 2.  2.  1. ...  1.  2.  2.]\n",
      "   ...\n",
      "   [ 0.  1.  0. ...  2.  3.  0.]\n",
      "   [ 3.  4.  3. ...  0.  0.  0.]\n",
      "   [ 1.  0.  0. ...  2.  2.  1.]]\n",
      "\n",
      "  [[ 0.  0.  0. ...  0.  0.  1.]\n",
      "   [ 4.  5.  6. ...  6.  5.  4.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 0.  3.  1. ...  4.  2.  1.]\n",
      "   [ 2.  0.  0. ...  0.  0.  0.]\n",
      "   [ 3.  2.  1. ...  3.  3.  3.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  2. ...  1.  1.  1.]\n",
      "   [ 2.  2.  2. ...  1.  2.  1.]\n",
      "   ...\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      "  [[ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 5.  5.  2. ...  3.  3.  2.]\n",
      "   [ 1.  0.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      "  [[nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   ...\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[ 3.  3.  4. ...  2.  3.  3.]\n",
      "   [ 3.  4.  3. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 0.  0.  1. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  2.  0. ...  0.  0.  0.]]\n",
      "\n",
      "  [[ 1.  1.  0. ...  1.  1.  3.]\n",
      "   [ 0.  0.  0. ...  3.  3.  1.]\n",
      "   [ 1.  3.  2. ...  5.  5.  2.]\n",
      "   ...\n",
      "   [ 0.  0.  2. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  1.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      "  [[ 5.  6.  6. ...  3.  4.  1.]\n",
      "   [ 0.  0.  0. ...  3.  4.  1.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  1.  0. ...  0.  0.  0.]\n",
      "   [ 2.  3.  1. ...  4.  5.  3.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 1.  2.  1. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  1.  0.  0.]\n",
      "   [ 3.  2.  3. ...  1.  1.  4.]\n",
      "   ...\n",
      "   [ 2.  2.  3. ...  0.  0.  0.]\n",
      "   [ 0.  1.  1. ...  0.  0.  1.]\n",
      "   [ 0.  0.  3. ...  2.  2.  1.]]\n",
      "\n",
      "  [[ 0.  1.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  1.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      "  [[nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   ...\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]]]\n",
      "\n",
      "\n",
      " [[[ 0.  0.  0. ...  0.  1.  0.]\n",
      "   [ 0.  5.  2. ...  2.  3.  1.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 0.  0.  0. ...  1.  2.  1.]\n",
      "   [ 1.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      "  [[ 2.  0.  1. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  4.  2.  1.]\n",
      "   [ 1.  0.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 0.  0.  0. ...  1.  1.  1.]\n",
      "   [ 2.  4.  2. ...  2.  2.  0.]\n",
      "   [ 2.  5.  4. ...  4.  4.  0.]]\n",
      "\n",
      "  [[ 0.  1.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 1.  0.  4. ...  3.  3.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  2.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 0.  1.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      "  [[ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  1.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  1.  0. ...  0.  0.  0.]]\n",
      "\n",
      "  [[nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   ...\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]]]\n",
      "\n",
      "\n",
      " [[[ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  1.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 4.  3.  3. ...  3.  1.  2.]\n",
      "   [ 0.  1.  1. ...  1.  4.  1.]\n",
      "   [ 1.  0.  0. ...  1.  0.  0.]]\n",
      "\n",
      "  [[ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  1.  0. ...  0.  0.  0.]\n",
      "   [ 0.  1.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 1.  4.  1. ...  1.  1.  2.]\n",
      "   [ 2.  1.  2. ...  1.  2.  0.]\n",
      "   [ 1.  5.  4. ...  4.  2.  2.]]\n",
      "\n",
      "  [[ 2.  2.  3. ...  2.  2.  2.]\n",
      "   [ 0.  1.  0. ...  0.  1.  1.]\n",
      "   [ 1.  2.  1. ...  1.  0.  2.]\n",
      "   ...\n",
      "   [ 0.  1.  0. ...  1.  2.  1.]\n",
      "   [ 2.  4.  2. ...  1.  0.  0.]\n",
      "   [ 0.  0.  1. ...  2.  4.  3.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.  0.  0. ...  1.  0.  1.]\n",
      "   [ 0.  1.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  1.  0. ...  0.  0.  0.]]\n",
      "\n",
      "  [[ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 1.  0.  0. ...  1.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   ...\n",
      "   [ 2.  0.  0. ...  1.  2.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      "  [[nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   ...\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]\n",
      "   [nan nan nan ... nan nan nan]]]]\n",
      "file: norm_FOSS.sub_pico.20230508_120630.proc\n",
      "file: norm_FOSS.sub_pico.20230509_112900.proc\n"
     ]
    }
   ],
   "source": [
    "############### Open nwb file with .csv #######################################\n",
    "###############################################################################\n",
    "\n",
    "df = pd.read_excel( '/braintree/home/aliya277/dandi_brainscore/pico_inventory.xlsx'  )\n",
    "SubjectName = 'pico'\n",
    "storage_dir = '/braintree/home/aliya277/inventory'\n",
    "array_meta_path  = '/braintree/data2/active/users/sgouldin/array-metadata'\n",
    "\n",
    "counter = 0\n",
    "for index, DataFrame in df.iterrows():\n",
    "        \n",
    "    if DataFrame['Has SpikeTime'] == 1:\n",
    "\n",
    "        date = f\"20{DataFrame['date']}\"\n",
    "        if len(str(DataFrame['time'])) != 6: time = f\"0{DataFrame['time']}\"\n",
    "        else: time = str(DataFrame['time'])\n",
    "        \n",
    "        if DataFrame['ImageSet'] == 'normalizers':\n",
    "            directory = f'norm_FOSS.sub_pico.{date}_{time}.proc'\n",
    "        elif DataFrame['ImageSet'] == 'normalizers-HVM':\n",
    "            directory = f'norm_HVM.sub_pico.{date}_{time}.proc'\n",
    "        else: \n",
    "            directory = f\"exp_{DataFrame['ImageSet']}.sub_pico.{date}_{time}.proc\"\n",
    "\n",
    "        \n",
    "        imagesetdir = os.path.join(storage_dir, \".\".join(directory.split(\".\")[0:1]))\n",
    "        subjectdir  = os.path.join(storage_dir, imagesetdir, \".\".join(directory.split(\".\")[0:2]))\n",
    "        mworks_dir    = os.path.join('/', *DataFrame['Path: intanraw'].split('/')[:10], 'mworksproc', DataFrame['Path: intanraw'].split('/')[11]+'_mwk.csv')\n",
    "        \n",
    "        if os.path.isfile(mworks_dir):\n",
    "            \n",
    "            io = NWBHDF5IO(os.path.join(subjectdir,directory, f\"{directory}.nwb\"), \"r\") \n",
    "            nwbfile = io.read()\n",
    "\n",
    "            # for string in nwbfile.session_description.split(', '):\n",
    "            #     if string.startswith('ON/OFF'):\n",
    "            #         on_off = string.split(\":\")[-1]\n",
    "            #         try:\n",
    "            #             on_off = on_off.split('/')\n",
    "            #         except: pass\n",
    "            #         on_off = [eval(i) for i in on_off]\n",
    "            #     if string.startswith('Visual'):\n",
    "            #         vis_deg = eval(string.split(':')[-1])\n",
    "\n",
    "            # spikeTimes = nwbfile.units['spike_times'][:]\n",
    "            try:    \n",
    "                psth = nwbfile.scratch['psth'][:] #[stimuli x reps x timebins x channels]\n",
    "                counter +=1\n",
    "                if counter == 1:\n",
    "                    [start_time_ms, stop_time_ms, tb_ms] = nwbfile.scratch['psth meta'][:] \n",
    "                    print('file:', directory)\n",
    "                    new_psth = calc_psth(nwbfile, mworks_dir, start_time_ms, stop_time_ms, tb_ms, n_stimuli = None)\n",
    "                    print(np.array_equal(psth, new_psth))\n",
    "                    if np.array_equal(psth, new_psth) == False: \n",
    "                        print(np.linalg.norm(psth - new_psth))\n",
    "                        print(psth)\n",
    "                        print(new_psth)\n",
    "\n",
    "                # try:\n",
    "                #     # stimulus presentation times, i.e. start of each trial\n",
    "                #     stim_start_time_ms_ms = nwbfile.intervals['trials']['start_time'][:] \n",
    "                #     stim_stop_time_ms  = nwbfile.intervals['trials']['stop_time'][:]\n",
    "                #     assert 'ms' == nwbfile.intervals['trials']['unit'][:][0]\n",
    "                # except:\n",
    "                #     stim_start_time_ms = None\n",
    "                #     stim_stop_time_ms = None\n",
    "                \n",
    "            except: psth = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file norm_FOSS.sub_pico.20230221_124922.proc\n",
      "file norm_FOSS.sub_pico.20230303_152601.proc\n",
      "file norm_FOSS.sub_pico.20230804_152622.proc\n",
      "file norm_FOSS.sub_pico.20230214_134820.proc\n",
      "file norm_FOSS.sub_pico.20230301_140442.proc\n",
      "file norm_FOSS.sub_pico.20230217_140904.proc\n",
      "file norm_FOSS.sub_pico.20230215_145155.proc\n",
      "file norm_FOSS.sub_pico.20230227_133424.proc\n",
      "file norm_FOSS.sub_pico.20230223_132511.proc\n",
      "file exp_HVM-var6-subset-2023.sub_pico.20230303_180103.proc\n",
      "file exp_domain-transfer-2023.sub_pico.20230216_150919.proc\n",
      "file exp_domain-transfer-2023.sub_pico.20230217_153101.proc\n",
      "file exp_domain-transfer-2023.sub_pico.20230221_130510.proc\n",
      "file exp_domain-transfer-2023.sub_pico.20230223_134049.proc\n",
      "file exp_domain-transfer-2023.sub_pico.20230227_135006.proc\n",
      "file exp_domain-transfer-2023.sub_pico.20230214_140610.proc\n",
      "file exp_domain-transfer-2023.sub_pico.20230215_161322.proc\n",
      "file exp_domain-transfer-2023.sub_pico.20230301_153048.proc\n",
      "file exp_HVM-var6-2023.sub_pico.20230221_142542.proc\n",
      "file exp_HVM-var6-2023.sub_pico.20230215_150717.proc\n",
      "file exp_HVM-var6-2023.sub_pico.20230227_151407.proc\n",
      "file exp_HVM-var6-2023.sub_pico.20230301_142202.proc\n",
      "file exp_HVM-var6-2023.sub_pico.20230303_154230.proc\n",
      "file exp_facescrub-small.sub_pico.20230329_130459.proc\n",
      "file exp_facescrub-small.sub_pico.20230330_124309.proc\n",
      "file exp_shinecut.sub_pico.20230406_144835.proc\n",
      "file exp_images_in_context2.sub_pico.20230407_113258.proc\n",
      "file exp_robustness_guy_d0_v26.sub_pico.20230621_142345.proc\n",
      "file exp_robustness_guy_d1_v26.sub_pico.20230622_131538.proc\n",
      "file exp_mayo.sub_pico.20230804_154202.proc\n",
      "file exp_mayo.sub_pico.20230808_143723.proc\n",
      "file exp_mayo.sub_pico.20230808_162123.proc\n",
      "file exp_mayo.sub_pico.20230808_175311.proc\n",
      "file exp_robustness_chong_d0_v7.sub_pico.20230717_130030.proc\n",
      "file exp_robustness_guy_d1_v7_chong_dryrun.sub_pico.20230721_124133.proc\n",
      "file exp_robustness_guy_d0_v35.sub_pico.20230912_105316.proc\n",
      "file exp_robustness_guy_d1_v35.sub_pico.20230913_105856.proc\n",
      "file exp_robustness_guy_d1_v35.sub_pico.20230913_130428.proc\n",
      "file exp_robustness_guy_d1_v36.sub_pico.20230914_105649.proc\n",
      "file exp_robustness_guy_d0_v37.sub_pico.20230919_095857.proc\n",
      "file exp_robustness_guy_d1_v37.sub_pico.20230920_112814.proc\n"
     ]
    }
   ],
   "source": [
    "############### Open nwb file with .csv #######################################\n",
    "###############################################################################\n",
    "\n",
    "df = pd.read_excel( '/braintree/home/aliya277/dandi_brainscore/pico_inventory.xlsx'  )\n",
    "SubjectName = 'pico'\n",
    "storage_dir = '/braintree/home/aliya277/inventory'\n",
    "array_meta_path  = '/braintree/data2/active/users/sgouldin/array-metadata'\n",
    "\n",
    "start_time_ms, stop_time_ms, tb_ms = \n",
    "\n",
    "\n",
    "for index, DataFrame in df.iterrows():\n",
    "        \n",
    "    if DataFrame['Has SpikeTime'] == 1 and DataFrame['Has h5'] == 0:\n",
    "\n",
    "        date = f\"20{DataFrame['date']}\"\n",
    "        if len(str(DataFrame['time'])) != 6: time = f\"0{DataFrame['time']}\"\n",
    "        else: time = str(DataFrame['time'])\n",
    "        \n",
    "        if DataFrame['ImageSet'] == 'normalizers':\n",
    "            directory = f'norm_FOSS.sub_pico.{date}_{time}.proc'\n",
    "        elif DataFrame['ImageSet'] == 'normalizers-HVM':\n",
    "            directory = f'norm_HVM.sub_pico.{date}_{time}.proc'\n",
    "        else: \n",
    "            directory = f\"exp_{DataFrame['ImageSet']}.sub_pico.{date}_{time}.proc\"\n",
    "\n",
    "        \n",
    "        imagesetdir = os.path.join(storage_dir, \".\".join(directory.split(\".\")[0:1]))\n",
    "        subjectdir  = os.path.join(storage_dir, imagesetdir, \".\".join(directory.split(\".\")[0:2]))\n",
    "        mworks_dir    = os.path.join('/', *DataFrame['Path: intanraw'].split('/')[:10], 'mworksproc', DataFrame['Path: intanraw'].split('/')[11]+'_mwk.csv')\n",
    "        \n",
    "        if os.path.isfile(mworks_dir):\n",
    "            print('file', directory)\n",
    "            io = NWBHDF5IO(os.path.join(subjectdir,directory, f\"{directory}.nwb\"), \"r\") \n",
    "            nwbfile = io.read()\n",
    "\n",
    "            new_psth = calc_psth(nwbfile, mworks_dir, start_time_ms, stop_time_ms, tb_ms, n_stimuli = None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Validate All Files Using pwnyb, nwbinspectors #################\n",
    "###############################################################################\n",
    "\n",
    "from pynwb import validate\n",
    "from nwbinspector import inspect_nwbfile\n",
    "from dandi.validate import validate as dandival\n",
    "\n",
    "\n",
    "df = pd.read_excel( '/braintree/home/aliya277/dandi_brainscore/pico_inventory.xlsx'  )\n",
    "SubjectName = 'pico'\n",
    "storage_dir = '/braintree/home/aliya277/inventory'\n",
    "array_meta_path  = '/braintree/data2/active/users/sgouldin/array-metadata'\n",
    " \n",
    "all_nwb_paths = []\n",
    "for index, DataFrame in df.iterrows():\n",
    "        \n",
    "    if DataFrame['Has SpikeTime'] == 1:\n",
    "\n",
    "        date = f\"20{DataFrame['date']}\"\n",
    "        if len(str(DataFrame['time'])) != 6: time = f\"0{DataFrame['time']}\"\n",
    "        else: time = str(DataFrame['time'])\n",
    "        \n",
    "        if DataFrame['ImageSet'] == 'normalizers':\n",
    "            directory = f'norm_FOSS.sub_pico.{date}_{time}.proc'\n",
    "        elif DataFrame['ImageSet'] == 'normalizers-HVM':\n",
    "            directory = f'norm_HVM.sub_pico.{date}_{time}.proc'\n",
    "        else: \n",
    "            directory = f\"exp_{DataFrame['ImageSet']}.sub_pico.{date}_{time}.proc\"\n",
    "            \n",
    "        imagesetdir = os.path.join(storage_dir, \".\".join(directory.split(\".\")[0:1]))\n",
    "        subjectdir  = os.path.join(storage_dir, imagesetdir, \".\".join(directory.split(\".\")[0:2]))\n",
    "\n",
    "        all_nwb_paths.append(os.path.join(subjectdir,directory, f\"{directory}.nwb\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Files for 0:1\n",
      "([], 0)\n",
      "Checking Files for 1:2\n",
      "([], 0)\n",
      "Checking Files for 2:3\n",
      "([], 0)\n",
      "Checking Files for 3:4\n",
      "([], 0)\n",
      "Checking Files for 4:5\n",
      "([], 0)\n",
      "Checking Files for 5:6\n",
      "([], 0)\n",
      "Checking Files for 6:7\n",
      "([], 0)\n",
      "Checking Files for 7:8\n",
      "([], 0)\n",
      "Checking Files for 8:9\n",
      "([], 0)\n",
      "Checking Files for 9:10\n",
      "([], 0)\n",
      "Checking Files for 10:11\n",
      "([], 0)\n",
      "Checking Files for 11:12\n",
      "([], 0)\n",
      "Checking Files for 12:13\n",
      "([], 0)\n",
      "Checking Files for 13:14\n",
      "([], 0)\n",
      "Checking Files for 14:15\n",
      "([], 0)\n",
      "Checking Files for 15:16\n",
      "([], 0)\n",
      "Checking Files for 16:17\n",
      "([], 0)\n",
      "Checking Files for 17:18\n",
      "([], 0)\n",
      "Checking Files for 18:19\n",
      "([], 0)\n",
      "Checking Files for 19:20\n",
      "([], 0)\n",
      "Checking Files for 20:21\n",
      "([], 0)\n",
      "Checking Files for 21:22\n",
      "([], 0)\n",
      "Checking Files for 22:23\n",
      "([], 0)\n",
      "Checking Files for 23:24\n",
      "([], 0)\n",
      "Checking Files for 24:25\n",
      "([], 0)\n",
      "Checking Files for 25:26\n",
      "([], 0)\n",
      "Checking Files for 26:27\n",
      "([], 0)\n",
      "Checking Files for 27:28\n",
      "([], 0)\n",
      "Checking Files for 28:29\n",
      "([], 0)\n",
      "Checking Files for 29:30\n",
      "([], 0)\n",
      "Checking Files for 30:31\n",
      "([], 0)\n",
      "Checking Files for 31:32\n",
      "([], 0)\n",
      "Checking Files for 32:33\n",
      "([], 0)\n",
      "Checking Files for 33:34\n",
      "([], 0)\n",
      "Checking Files for 34:35\n",
      "([], 0)\n",
      "Checking Files for 35:36\n",
      "([], 0)\n",
      "Checking Files for 36:37\n",
      "([], 0)\n",
      "Checking Files for 37:38\n",
      "([], 0)\n",
      "Checking Files for 38:39\n",
      "([], 0)\n",
      "Checking Files for 39:40\n",
      "([], 0)\n",
      "Checking Files for 40:41\n",
      "([], 0)\n",
      "Checking Files for 41:42\n",
      "([], 0)\n",
      "Checking Files for 42:43\n",
      "([], 0)\n",
      "Checking Files for 43:44\n",
      "([], 0)\n",
      "Checking Files for 44:45\n",
      "([], 0)\n",
      "Checking Files for 45:46\n",
      "([], 0)\n",
      "Checking Files for 46:47\n",
      "([], 0)\n",
      "Checking Files for 47:48\n",
      "([], 0)\n",
      "Checking Files for 48:49\n",
      "([], 0)\n",
      "Checking Files for 49:50\n",
      "([], 0)\n",
      "Checking Files for 50:51\n",
      "([], 0)\n",
      "Checking Files for 51:52\n",
      "([], 0)\n",
      "Checking Files for 52:53\n",
      "([], 0)\n",
      "Checking Files for 53:54\n",
      "([], 0)\n",
      "Checking Files for 54:55\n",
      "([], 0)\n",
      "Checking Files for 55:56\n",
      "([], 0)\n",
      "Checking Files for 56:57\n",
      "([], 0)\n",
      "Checking Files for 57:58\n",
      "([], 0)\n",
      "Checking Files for 58:59\n",
      "([], 0)\n",
      "Checking Files for 59:60\n",
      "([], 0)\n",
      "Checking Files for 60:61\n",
      "([], 0)\n",
      "Checking Files for 61:62\n",
      "([], 0)\n",
      "Checking Files for 62:63\n",
      "([], 0)\n",
      "Checking Files for 63:64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/braintree/home/aliya277/dandi_brainscore/create_proc_nwb.ipynb Cell 7\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbraintree-gpu-1.mit.edu/braintree/home/aliya277/dandi_brainscore/create_proc_nwb.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39melse\u001b[39;00m: i \u001b[39m=\u001b[39m num_files\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbraintree-gpu-1.mit.edu/braintree/home/aliya277/dandi_brainscore/create_proc_nwb.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mChecking Files for \u001b[39m\u001b[39m{\u001b[39;00mj\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bbraintree-gpu-1.mit.edu/braintree/home/aliya277/dandi_brainscore/create_proc_nwb.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m pynwb_validation \u001b[39m=\u001b[39m validate(paths \u001b[39m=\u001b[39;49m all_nwb_paths[j:i])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbraintree-gpu-1.mit.edu/braintree/home/aliya277/dandi_brainscore/create_proc_nwb.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(pynwb_validation)\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/hdmf/utils.py:648\u001b[0m, in \u001b[0;36mdocval.<locals>.dec.<locals>.func_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfunc_call\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    647\u001b[0m     pargs \u001b[39m=\u001b[39m _check_args(args, kwargs)\n\u001b[0;32m--> 648\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/pynwb/validate.py:143\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m io_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(path\u001b[39m=\u001b[39mpath, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m, driver\u001b[39m=\u001b[39mdriver)\n\u001b[1;32m    142\u001b[0m \u001b[39mif\u001b[39;00m use_cached_namespaces:\n\u001b[0;32m--> 143\u001b[0m     cached_namespaces, manager, namespace_dependencies \u001b[39m=\u001b[39m _get_cached_namespaces_to_validate(\n\u001b[1;32m    144\u001b[0m         path\u001b[39m=\u001b[39;49mpath, driver\u001b[39m=\u001b[39;49mdriver\n\u001b[1;32m    145\u001b[0m     )\n\u001b[1;32m    146\u001b[0m     io_kwargs\u001b[39m.\u001b[39mupdate(manager\u001b[39m=\u001b[39mmanager)\n\u001b[1;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(cached_namespaces):\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/pynwb/validate.py:61\u001b[0m, in \u001b[0;36m_get_cached_namespaces_to_validate\u001b[0;34m(path, driver)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m NWBHDF5IO  \u001b[39m# TODO: modularize to avoid circular import\u001b[39;00m\n\u001b[1;32m     58\u001b[0m catalog \u001b[39m=\u001b[39m NamespaceCatalog(\n\u001b[1;32m     59\u001b[0m     group_spec_cls\u001b[39m=\u001b[39mNWBGroupSpec, dataset_spec_cls\u001b[39m=\u001b[39mNWBDatasetSpec, spec_namespace_cls\u001b[39m=\u001b[39mNWBNamespace\n\u001b[1;32m     60\u001b[0m )\n\u001b[0;32m---> 61\u001b[0m namespace_dependencies \u001b[39m=\u001b[39m NWBHDF5IO\u001b[39m.\u001b[39;49mload_namespaces(namespace_catalog\u001b[39m=\u001b[39;49mcatalog, path\u001b[39m=\u001b[39;49mpath, driver\u001b[39m=\u001b[39;49mdriver)\n\u001b[1;32m     63\u001b[0m \u001b[39m# Determine which namespaces are the most specific (i.e. extensions) and validate against those\u001b[39;00m\n\u001b[1;32m     64\u001b[0m candidate_namespaces \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(namespace_dependencies\u001b[39m.\u001b[39mkeys())\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/hdmf/utils.py:644\u001b[0m, in \u001b[0;36mdocval.<locals>.dec.<locals>.func_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfunc_call\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    643\u001b[0m     pargs \u001b[39m=\u001b[39m _check_args(args, kwargs)\n\u001b[0;32m--> 644\u001b[0m     \u001b[39mreturn\u001b[39;00m func(args[\u001b[39m0\u001b[39;49m], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/hdmf/backends/hdf5/h5tools.py:170\u001b[0m, in \u001b[0;36mHDF5IO.load_namespaces\u001b[0;34m(cls, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39mif\u001b[39;00m file_obj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# need to close the file object that we just opened\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mwith\u001b[39;00m open_file_obj:\n\u001b[0;32m--> 170\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m__load_namespaces(namespace_catalog, namespaces, open_file_obj)\n\u001b[1;32m    171\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m__load_namespaces(namespace_catalog, namespaces, open_file_obj)\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/hdmf/backends/hdf5/h5tools.py:204\u001b[0m, in \u001b[0;36mHDF5IO.__load_namespaces\u001b[0;34m(cls, namespace_catalog, namespaces, file_obj)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mfor\u001b[39;00m ns \u001b[39min\u001b[39;00m order:\n\u001b[1;32m    203\u001b[0m     reader \u001b[39m=\u001b[39m readers[ns]\n\u001b[0;32m--> 204\u001b[0m     d\u001b[39m.\u001b[39mupdate(namespace_catalog\u001b[39m.\u001b[39;49mload_namespaces(\u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m__ns_spec_path, reader\u001b[39m=\u001b[39;49mreader))\n\u001b[1;32m    206\u001b[0m \u001b[39mreturn\u001b[39;00m d\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/hdmf/utils.py:644\u001b[0m, in \u001b[0;36mdocval.<locals>.dec.<locals>.func_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfunc_call\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    643\u001b[0m     pargs \u001b[39m=\u001b[39m _check_args(args, kwargs)\n\u001b[0;32m--> 644\u001b[0m     \u001b[39mreturn\u001b[39;00m func(args[\u001b[39m0\u001b[39;49m], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/hdmf/spec/namespace.py:537\u001b[0m, in \u001b[0;36mNamespaceCatalog.load_namespaces\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[39m# now load specs into namespace\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[39mfor\u001b[39;00m ns \u001b[39min\u001b[39;00m to_load:\n\u001b[0;32m--> 537\u001b[0m     ret[ns[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load_namespace(ns, reader, resolve\u001b[39m=\u001b[39;49mresolve)\n\u001b[1;32m    538\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__included_specs[ns_path_key] \u001b[39m=\u001b[39m ret\n\u001b[1;32m    539\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/hdmf/spec/namespace.py:459\u001b[0m, in \u001b[0;36mNamespaceCatalog.__load_namespace\u001b[0;34m(self, namespace, reader, resolve)\u001b[0m\n\u001b[1;32m    457\u001b[0m     registered_types \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[1;32m    458\u001b[0m     \u001b[39mfor\u001b[39;00m ndt \u001b[39min\u001b[39;00m types_to_load:\n\u001b[0;32m--> 459\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__register_type(ndt, inc_ns, catalog, registered_types)\n\u001b[1;32m    460\u001b[0m     included_types[s[\u001b[39m'\u001b[39m\u001b[39mnamespace\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39msorted\u001b[39m(registered_types))\n\u001b[1;32m    461\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/hdmf/spec/namespace.py:471\u001b[0m, in \u001b[0;36mNamespaceCatalog.__register_type\u001b[0;34m(self, ndt, inc_ns, catalog, registered_types)\u001b[0m\n\u001b[1;32m    469\u001b[0m spec \u001b[39m=\u001b[39m inc_ns\u001b[39m.\u001b[39mget_spec(ndt)\n\u001b[1;32m    470\u001b[0m spec_file \u001b[39m=\u001b[39m inc_ns\u001b[39m.\u001b[39mcatalog\u001b[39m.\u001b[39mget_spec_source_file(ndt)\n\u001b[0;32m--> 471\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__register_dependent_types(spec, inc_ns, catalog, registered_types)\n\u001b[1;32m    472\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(spec, DatasetSpec):\n\u001b[1;32m    473\u001b[0m     built_spec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_spec_cls\u001b[39m.\u001b[39mbuild_spec(spec)\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/hdmf/spec/namespace.py:500\u001b[0m, in \u001b[0;36mNamespaceCatalog.__register_dependent_types\u001b[0;34m(self, spec, inc_ns, catalog, registered_types)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(spec, GroupSpec):\n\u001b[1;32m    499\u001b[0m     \u001b[39mfor\u001b[39;00m child_spec \u001b[39min\u001b[39;00m (spec\u001b[39m.\u001b[39mgroups \u001b[39m+\u001b[39m spec\u001b[39m.\u001b[39mdatasets \u001b[39m+\u001b[39m spec\u001b[39m.\u001b[39mlinks):\n\u001b[0;32m--> 500\u001b[0m         __register_dependent_types_helper(child_spec, inc_ns, catalog, registered_types)\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/hdmf/spec/namespace.py:491\u001b[0m, in \u001b[0;36mNamespaceCatalog.__register_dependent_types.<locals>.__register_dependent_types_helper\u001b[0;34m(spec, inc_ns, catalog, registered_types)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__register_type(spec\u001b[39m.\u001b[39mdata_type_def, inc_ns, catalog, registered_types)\n\u001b[1;32m    490\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# spec is a LinkSpec\u001b[39;00m\n\u001b[0;32m--> 491\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__register_type(spec\u001b[39m.\u001b[39;49mtarget_type, inc_ns, catalog, registered_types)\n\u001b[1;32m    492\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(spec, GroupSpec):\n\u001b[1;32m    493\u001b[0m     \u001b[39mfor\u001b[39;00m child_spec \u001b[39min\u001b[39;00m (spec\u001b[39m.\u001b[39mgroups \u001b[39m+\u001b[39m spec\u001b[39m.\u001b[39mdatasets \u001b[39m+\u001b[39m spec\u001b[39m.\u001b[39mlinks):\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/hdmf/spec/namespace.py:471\u001b[0m, in \u001b[0;36mNamespaceCatalog.__register_type\u001b[0;34m(self, ndt, inc_ns, catalog, registered_types)\u001b[0m\n\u001b[1;32m    469\u001b[0m spec \u001b[39m=\u001b[39m inc_ns\u001b[39m.\u001b[39mget_spec(ndt)\n\u001b[1;32m    470\u001b[0m spec_file \u001b[39m=\u001b[39m inc_ns\u001b[39m.\u001b[39mcatalog\u001b[39m.\u001b[39mget_spec_source_file(ndt)\n\u001b[0;32m--> 471\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__register_dependent_types(spec, inc_ns, catalog, registered_types)\n\u001b[1;32m    472\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(spec, DatasetSpec):\n\u001b[1;32m    473\u001b[0m     built_spec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_spec_cls\u001b[39m.\u001b[39mbuild_spec(spec)\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/hdmf/spec/namespace.py:500\u001b[0m, in \u001b[0;36mNamespaceCatalog.__register_dependent_types\u001b[0;34m(self, spec, inc_ns, catalog, registered_types)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(spec, GroupSpec):\n\u001b[1;32m    499\u001b[0m     \u001b[39mfor\u001b[39;00m child_spec \u001b[39min\u001b[39;00m (spec\u001b[39m.\u001b[39mgroups \u001b[39m+\u001b[39m spec\u001b[39m.\u001b[39mdatasets \u001b[39m+\u001b[39m spec\u001b[39m.\u001b[39mlinks):\n\u001b[0;32m--> 500\u001b[0m         __register_dependent_types_helper(child_spec, inc_ns, catalog, registered_types)\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/hdmf/spec/namespace.py:491\u001b[0m, in \u001b[0;36mNamespaceCatalog.__register_dependent_types.<locals>.__register_dependent_types_helper\u001b[0;34m(spec, inc_ns, catalog, registered_types)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__register_type(spec\u001b[39m.\u001b[39mdata_type_def, inc_ns, catalog, registered_types)\n\u001b[1;32m    490\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# spec is a LinkSpec\u001b[39;00m\n\u001b[0;32m--> 491\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__register_type(spec\u001b[39m.\u001b[39;49mtarget_type, inc_ns, catalog, registered_types)\n\u001b[1;32m    492\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(spec, GroupSpec):\n\u001b[1;32m    493\u001b[0m     \u001b[39mfor\u001b[39;00m child_spec \u001b[39min\u001b[39;00m (spec\u001b[39m.\u001b[39mgroups \u001b[39m+\u001b[39m spec\u001b[39m.\u001b[39mdatasets \u001b[39m+\u001b[39m spec\u001b[39m.\u001b[39mlinks):\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/hdmf/spec/namespace.py:471\u001b[0m, in \u001b[0;36mNamespaceCatalog.__register_type\u001b[0;34m(self, ndt, inc_ns, catalog, registered_types)\u001b[0m\n\u001b[1;32m    469\u001b[0m spec \u001b[39m=\u001b[39m inc_ns\u001b[39m.\u001b[39mget_spec(ndt)\n\u001b[1;32m    470\u001b[0m spec_file \u001b[39m=\u001b[39m inc_ns\u001b[39m.\u001b[39mcatalog\u001b[39m.\u001b[39mget_spec_source_file(ndt)\n\u001b[0;32m--> 471\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__register_dependent_types(spec, inc_ns, catalog, registered_types)\n\u001b[1;32m    472\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(spec, DatasetSpec):\n\u001b[1;32m    473\u001b[0m     built_spec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_spec_cls\u001b[39m.\u001b[39mbuild_spec(spec)\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/hdmf/spec/namespace.py:497\u001b[0m, in \u001b[0;36mNamespaceCatalog.__register_dependent_types\u001b[0;34m(self, spec, inc_ns, catalog, registered_types)\u001b[0m\n\u001b[1;32m    494\u001b[0m             __register_dependent_types_helper(child_spec, inc_ns, catalog, registered_types)\n\u001b[1;32m    496\u001b[0m \u001b[39mif\u001b[39;00m spec\u001b[39m.\u001b[39mdata_type_inc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__register_type(spec\u001b[39m.\u001b[39;49mdata_type_inc, inc_ns, catalog, registered_types)\n\u001b[1;32m    498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(spec, GroupSpec):\n\u001b[1;32m    499\u001b[0m     \u001b[39mfor\u001b[39;00m child_spec \u001b[39min\u001b[39;00m (spec\u001b[39m.\u001b[39mgroups \u001b[39m+\u001b[39m spec\u001b[39m.\u001b[39mdatasets \u001b[39m+\u001b[39m spec\u001b[39m.\u001b[39mlinks):\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/hdmf/spec/namespace.py:475\u001b[0m, in \u001b[0;36mNamespaceCatalog.__register_type\u001b[0;34m(self, ndt, inc_ns, catalog, registered_types)\u001b[0m\n\u001b[1;32m    473\u001b[0m     built_spec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_spec_cls\u001b[39m.\u001b[39mbuild_spec(spec)\n\u001b[1;32m    474\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 475\u001b[0m     built_spec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroup_spec_cls\u001b[39m.\u001b[39;49mbuild_spec(spec)\n\u001b[1;32m    476\u001b[0m registered_types\u001b[39m.\u001b[39madd(ndt)\n\u001b[1;32m    477\u001b[0m catalog\u001b[39m.\u001b[39mregister_spec(built_spec, spec_file)\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/hdmf/spec/spec.py:99\u001b[0m, in \u001b[0;36mConstructableDict.build_spec\u001b[0;34m(cls, spec_dict)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[39mif\u001b[39;00m x[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m] \u001b[39min\u001b[39;00m vargs:\n\u001b[1;32m     98\u001b[0m         kwargs[x[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m vargs\u001b[39m.\u001b[39mget(x[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 99\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/hdmf/utils.py:644\u001b[0m, in \u001b[0;36mdocval.<locals>.dec.<locals>.func_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfunc_call\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    643\u001b[0m     pargs \u001b[39m=\u001b[39m _check_args(args, kwargs)\n\u001b[0;32m--> 644\u001b[0m     \u001b[39mreturn\u001b[39;00m func(args[\u001b[39m0\u001b[39;49m], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/pynwb/spec.py:156\u001b[0m, in \u001b[0;36mNWBGroupSpec.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39m@docval\u001b[39m(\u001b[39m*\u001b[39mdeepcopy(_group_docval))\n\u001b[1;32m    155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 156\u001b[0m     kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_translate_kwargs(kwargs)\n\u001b[1;32m    157\u001b[0m     \u001b[39m# set data_type_inc to NWBData only if it is not specified and the type is not an HDMF base type\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[39m# NOTE: CSRMatrix in hdmf-common-schema does not have a data_type_inc but should not inherit from\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[39m# NWBContainer. This will be fixed in hdmf-common-schema 1.2.1.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[39mif\u001b[39;00m kwargs[\u001b[39m'\u001b[39m\u001b[39mdata_type_inc\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m kwargs[\u001b[39m'\u001b[39m\u001b[39mdata_type_def\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39mNone\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39mContainer\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCSRMatrix\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/pynwb/spec.py:113\u001b[0m, in \u001b[0;36mBaseStorageOverride._translate_kwargs\u001b[0;34m(cls, kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m proxy \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\n\u001b[1;32m    112\u001b[0m kwargs[proxy\u001b[39m.\u001b[39mdef_key()] \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mdef_key())\n\u001b[0;32m--> 113\u001b[0m kwargs[proxy\u001b[39m.\u001b[39minc_key()] \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49minc_key())\n\u001b[1;32m    114\u001b[0m \u001b[39mreturn\u001b[39;00m kwargs\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/pynwb/spec.py:78\u001b[0m, in \u001b[0;36mBaseStorageOverride.inc_key\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39m    \u001b[39m\u001b[39m''' Get the key used to store data type on an instance'''\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m__type_key\n\u001b[0;32m---> 78\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     79\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minc_key\u001b[39m(\u001b[39mcls\u001b[39m):\n\u001b[1;32m     80\u001b[0m \u001b[39m    \u001b[39m\u001b[39m''' Get the key used to define a data_type include.'''\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m__inc_key\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_files = len(all_nwb_paths)\n",
    "\n",
    "for i in range(63,num_files):\n",
    "    j = i\n",
    "    if i + 1 < num_files: i += 1\n",
    "    else: i = num_files\n",
    "    print(f\"Checking Files for {j}:{i}\")\n",
    "    pynwb_validation = validate(paths = all_nwb_paths[j:i])\n",
    "    print(pynwb_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object validate at 0x7fe643ee4170>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dandi_validation = dandival(all_nwb_paths)\n",
    "dandi_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwbinspector_validation = []\n",
    "for path in all_nwb_paths:\n",
    "    results = list(inspect_nwbfile(nwbfile_path=path))\n",
    "    print(results)\n",
    "    nwbinspector_validation.append(results)\n",
    "nwbinspector_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Add PSTH if Needed ############################################\n",
    "###############################################################################\n",
    "\n",
    "inventory   = '/braintree/home/aliya277/inventory/'\n",
    "all_files = os.listdir(inventory)\n",
    "for folder in all_files:\n",
    "        path = os.path.join(inventory, folder)\n",
    "        try:\n",
    "                io = NWBHDF5IO(os.path.join(path, f\"{folder}.nwb\"), \"r\") \n",
    "                nwbfile = io.read()\n",
    "                io.close()\n",
    "        except: print(f'This File can not be opened: {folder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "579b41b5614846578a7c03a80dec0efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='session_description:', layout=Layout(max_height='40px', max_width='"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwb2widget(nwbfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'h5Files' in os.listdir(path):\n",
    "#     print('Opening PSTH...')\n",
    "#     filename = os.listdir(os.path.join(path, 'h5Files'))[0]\n",
    "#     file = h5py.File(os.path.join(path, 'h5Files', filename),'r+') \n",
    "#     data = file['psth'][:]\n",
    "#     file.close()\n",
    "\n",
    "\n",
    "#     print('Adding PSTH...')\n",
    "#     nwbfile.add_scratch(\n",
    "#         data,\n",
    "#         name=\"psth\",\n",
    "#         description=\"psth, uncorrected [channels x stimuli x reps x timebins]\",\n",
    "#         )\n",
    "    \n",
    "# else: counter +=1\n",
    "\n",
    "# try: io.close()\n",
    "# except:pass\n",
    "\n",
    "# if os.path.isfile(os.path.join(path, f\"{SessionName}.nwb\")):\n",
    "#         os.remove(os.path.join(path, f\"{SessionName}.nwb\"))\n",
    "\n",
    "# print('Saving NWB File..')\n",
    "# io = NWBHDF5IO(os.path.join(path, f\"{SessionName}.nwb\"), \"w\") \n",
    "# io.write(nwbfile)\n",
    "# io.close()\n",
    "# print(\"File saved.\")\n",
    "\n",
    "#     # psth    = get_psth_from_nwb(nwbfile, path, start_time, stop_time, timebin)\n",
    "#     # data    = psth['psth']\n",
    "\n",
    "# print(f'{counter} SpikeTimes do not have an h5 file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "inventory   = '/braintree/home/aliya277/inventory/'\n",
    "\n",
    "def find_directories_without_extension(root_dir, extension):\n",
    "    directory_paths = []\n",
    "    for foldername, subfolders, filenames in os.walk(root_dir):\n",
    "        depth = foldername[len(root_dir):].count(os.sep)\n",
    "        if depth ==  0:\n",
    "            # Check if any file in the directory has the specified extension\n",
    "            if not any(filename.endswith(extension) for filename in filenames):\n",
    "                directory_paths.append(os.path.join(root_dir, foldername))\n",
    "                # print(os.path.join(root_dir, foldername))\n",
    "    return directory_paths[1:]\n",
    "\n",
    "print(len(find_directories_without_extension(inventory, '.nwb')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "for obj in gc.get_objects():   # Browse through ALL objects\n",
    "    if isinstance(obj, h5py.File):   # Just HDF5 files\n",
    "        try:\n",
    "            obj.close()\n",
    "        except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def read_names(filename):\n",
    "    assignment  = filename.split('.')[0].split('-')[1]\n",
    "    number      = filename.split('.')[0].split('-')[2]\n",
    "    return np.asarray([assignment, number])\n",
    "\n",
    "def create_nwb(config, path):\n",
    "\n",
    "    SessionDate = path.split('/')[-1].split('.')[-2].split('_')[0]\n",
    "    SessionTime = path.split('/')[-1].split('.')[-2].split('_')[1]\n",
    "    SubjectName = path.split('/')[-1].split('.')[1].split('_')[1]\n",
    "    date_format = \"%Y%m%d %H%M%S\"\n",
    "\n",
    "    if config['subject']['subject_id'].lower() != SubjectName.lower():\n",
    "        raise ValueError(\"Subject Name incorrect.\")\n",
    "        \n",
    "    session_start_time_ = datetime.strptime(SessionDate+' '+SessionTime, date_format)\n",
    "    # Define the timezone you want to use (e.g., 'US/Eastern' for Boston)\n",
    "    desired_timezone = pytz.timezone('US/Eastern')\n",
    "    session_start_time = desired_timezone.localize(session_start_time_)\n",
    "\n",
    "    ################ CREATE NWB FILE WITH METADATA ################################\n",
    "    ###############################################################################\n",
    "    nwbfile = NWBFile(\n",
    "        session_description     = config['session_info']['session_description'],\n",
    "        identifier              = config['metadata']['identifier'],\n",
    "        session_start_time      = session_start_time,\n",
    "        file_create_date        = config['metadata']['file_create_date'],\n",
    "        experimenter            = config['general']['lab_info']['experimenter'],\n",
    "        experiment_description  = config['general']['experiment_info']['experiment_description'],\n",
    "        session_id              = config['session_info']['session_id'],\n",
    "        lab                     = config['general']['lab_info']['lab'],                     \n",
    "        institution             = config['general']['lab_info']['institution'],                                    \n",
    "        keywords                = config['general']['experiment_info']['keywords'],\n",
    "        protocol                = config['general']['experiment_info']['protocol'],\n",
    "        related_publications    = config['general']['experiment_info']['related_publications'],\n",
    "        surgery                 = config['general']['experiment_info']['surgery']\n",
    "    )\n",
    "\n",
    "    ################ CREATE SUBJECT ################################################\n",
    "    ################################################################################\n",
    "    nwbfile.subject = Subject(\n",
    "        subject_id  = config['subject']['subject_id'],\n",
    "        date_of_birth= config['subject']['date_of_birth'],\n",
    "        species     = config['subject']['species'],\n",
    "        sex         = config['subject']['sex'],\n",
    "    )\n",
    "\n",
    "    ################ CREATE HARDWARE LINKS #########################################\n",
    "    ################################################################################\n",
    "    nwbfile.create_device(\n",
    "        name        = config['hardware']['system_name'], \n",
    "        description = config['hardware']['system_description'], \n",
    "        manufacturer= config['hardware']['system_manuf']\n",
    "    )\n",
    "\n",
    "    nwbfile.create_device(\n",
    "        name        = config['hardware']['adapter_manuf'], \n",
    "        description = config['hardware']['adapter_description'], \n",
    "        manufacturer= config['hardware']['adapter_manuf']\n",
    "    )\n",
    "\n",
    "    nwbfile.create_device(\n",
    "        name        = config['hardware']['monitor_name'], \n",
    "        description = config['hardware']['monitor_description'], \n",
    "        manufacturer= config['hardware']['monitor_manuf']\n",
    "    )\n",
    "\n",
    "    nwbfile.create_device(\n",
    "        name        = config['hardware']['photodiode_name'], \n",
    "        description = config['hardware']['photodiode_description'], \n",
    "        manufacturer= config['hardware']['photodiode_manuf']\n",
    "    )\n",
    "    \n",
    "    nwbfile.create_device(\n",
    "        name        = 'Software Used', \n",
    "        description = str(['Mworks Client: '+config['software']['mwclient_version'],\\\n",
    "                        'Mworks Server: '+config['software']['mwserver_version'],\\\n",
    "                        'OS: '+config['software']['OS'],\\\n",
    "                        'Intan :'+config['software']['intan_version']])\n",
    "    )\n",
    "\n",
    "    ################ CREATE ELECTRODE LINKS ########################################\n",
    "    ################################################################################\n",
    "    electrodes = nwbfile.create_device(\n",
    "        name        = config['hardware']['electrode_name'], \n",
    "        description = config['hardware']['electrode_description'], \n",
    "        manufacturer= config['hardware']['electrode_manuf']\n",
    "    )\n",
    "\n",
    "    all_files = sorted(os.listdir(os.path.join(path, 'SpikeTimes')))\n",
    "    \n",
    "    name_accumulator = []\n",
    "    for file in all_files:\n",
    "        name_accumulator.append(read_names(file))\n",
    "    names = np.vstack(name_accumulator)\n",
    "\n",
    "    nwbfile.add_electrode_column(name=\"label\", description=\"label of electrode\")\n",
    "    groups, count_groups = np.unique(names[:,0], return_counts =True)\n",
    "    ids                  = names[:,1]\n",
    "    counter              = 0\n",
    "    # create ElectrodeGroups A, B, C, ..\n",
    "    for group, count_group in zip(groups, count_groups):\n",
    "        electrode_group = nwbfile.create_electrode_group(\n",
    "            name        = \"group_{}\".format(group),\n",
    "            description = \"Serialnumber: {}. Adapter Version: {}\".format(config['array_info']['array_{}'.format(group)]['serialnumber'],\\\n",
    "                            config['array_info']['array_{}'.format(group)]['adapterversion']),\n",
    "            device      = electrodes,\n",
    "            location    = 'hemisphere, region, subregion: '+str([config['array_info']['array_{}'.format(group)]['hemisphere'],\\\n",
    "                                config['array_info']['array_{}'.format(group)]['region'],\n",
    "                                config['array_info']['array_{}'.format(group)]['subregion']]),\n",
    "            position    = config['array_info']['array_{}'.format(group)]['position']\n",
    "        )\n",
    "\n",
    "        # create Electrodes 001, 002, ..., 032 in ElectrodeGroups per channel\n",
    "        for ichannel in range(count_group):\n",
    "            nwbfile.add_electrode(\n",
    "                group       = electrode_group,\n",
    "                label       = ids[counter],\n",
    "                location    = 'row, col, elec'+str(json.loads(config['array_info']['intan_electrode_labeling_[row,col,id]'])[counter])\n",
    "            )\n",
    "            counter += 1     \n",
    "\n",
    "    ################ ADD SPIKE TIMES ###############################################\n",
    "    ################################################################################\n",
    "\n",
    "    nwbfile.add_unit_column(name=\"unit\", description=\"millisecond\") \n",
    "    for filename, i in zip(sorted(os.listdir(os.path.join(path, 'SpikeTimes'))), range(len(os.listdir(os.path.join(path, 'SpikeTimes'))))):\n",
    "        [assignment, number] = read_names(filename)\n",
    "        file_path = os.path.join(path, 'SpikeTimes', filename)\n",
    "        data = scipy.io.loadmat(file_path, squeeze_me=True,\n",
    "                        variable_names='spike_time_ms')['spike_time_ms']\n",
    "        nwbfile.add_unit(\n",
    "            spike_times = data, \n",
    "            electrodes  = [i],\n",
    "            electrode_group = nwbfile.electrode_groups[f'group_{assignment}'], \n",
    "            unit = 'ms'\n",
    "        )\n",
    "\n",
    "    ################ ADD TRIAL TIMES ###############################################\n",
    "    ################################################################################\n",
    "    last_spike = data[-1]\n",
    "    del data\n",
    "    with open(os.path.join(path, 'NWBInfo.txt')) as f:\n",
    "        lines = f.readlines()\n",
    "        line1 = lines[0].split(',')[0]\n",
    "        StimOnnOff = [float(line1.split('/')[0]),float(line1.split('/')[1])] \n",
    "\n",
    "    on_start  = 0\n",
    "    on_dur    = StimOnnOff[1]\n",
    "    off_dur   = StimOnnOff[1]\n",
    "\n",
    "    \n",
    "    nwbfile.add_trial_column(name=\"unit\", description=\"millisecond\")\n",
    "    while on_start < last_spike:\n",
    "\n",
    "        nwbfile.add_trial(\n",
    "            start_time= float(on_start),\n",
    "            stop_time = float(on_start+on_dur),\n",
    "            unit = 'ms')\n",
    "    \n",
    "        on_start += on_dur+off_dur\n",
    "\n",
    "    return nwbfile\n",
    "\n",
    "def get_psth_from_nwb(nwbfile, path, start_time, stop_time, timebin, n_stimuli=None):\n",
    "\n",
    "    # Find the MWORKS File\n",
    "    with open(os.path.join(path, 'NWBInfo.txt')) as f:\n",
    "        lines = f.readlines()\n",
    "        line2 = lines[0].split(',')[1]\n",
    "        ind = line2.split('/').index('intanproc')\n",
    "        mwk_file = glob.glob(os.path.join('/', *line2.split('/')[0:ind], 'mworksproc', \\\n",
    "                '_'.join(map(str, line2.split('/')[ind+1].split('_')[0:3]))+'*_mwk.csv'))   \n",
    "        \n",
    "    assert len(mwk_file) == 1\n",
    "    mwk_file = mwk_file[0]\n",
    "    assert os.path.isfile(mwk_file)==True\n",
    "\n",
    "    ################ MODIFIED FROM THE SPIKE-TOOLS-CHONG CODE ######################\n",
    "    ################################################################################\n",
    "    \n",
    "    mwk_data = pd.read_csv(mwk_file)\n",
    "    mwk_data = mwk_data[mwk_data.fixation_correct == 1]\n",
    "    if 'photodiode_on_us' in mwk_data.keys():\n",
    "        samp_on_ms = np.asarray(mwk_data['photodiode_on_us']) / 1000.\n",
    "        logging.info('Using photodiode signal for sample on time')\n",
    "    else:\n",
    "        samp_on_ms = np.asarray(mwk_data['samp_on_us']) / 1000.\n",
    "        logging.info('Using MWorks digital signal for sample on time')\n",
    "    \n",
    "    # Load spikeTime file for current channel\n",
    "    spikeTimes = nwbfile.units[:].spike_times\n",
    "    # Re-order the psth to image x reps\n",
    "    max_number_of_reps = max(np.bincount(mwk_data['stimulus_presented']))  # Max reps obtained for any image\n",
    "    if max_number_of_reps == 0:\n",
    "        exit()\n",
    "    mwk_data['stimulus_presented'] = mwk_data['stimulus_presented'].astype(int)  # To avoid indexing errors\n",
    "    \n",
    "    if n_stimuli is None:\n",
    "            image_numbers = np.unique(mwk_data['stimulus_presented'])  # TODO: if not all images are shown (for eg, exp cut short), you'll have to manually type in total # images\n",
    "    else:\n",
    "        image_numbers = np.arange(1,n_stimuli+1) # all of my image starts with #1\n",
    "\n",
    "    timebase = np.arange(start_time, stop_time, timebin)\n",
    "    PSTH = np.full((len(image_numbers), max_number_of_reps, len(timebase),spikeTimes.shape[0]), np.nan)\n",
    "\n",
    "    for num in range(spikeTimes.shape[0]):\n",
    "        spikeTime = np.asanyarray(spikeTimes[num])\n",
    "        osamp = 10\n",
    "        psth_bin = np.zeros((len(samp_on_ms), osamp*(stop_time-start_time)))\n",
    "        psth_matrix = np.full((len(samp_on_ms), len(timebase)), np.nan)\n",
    "\n",
    "        for i in range(len(samp_on_ms)):\n",
    "\n",
    "            sidx = np.floor(osamp*(spikeTime[(spikeTime>=(samp_on_ms[i]+start_time))*(spikeTime<(samp_on_ms[i]+stop_time))]-(samp_on_ms[i]+start_time))).astype(int)\n",
    "            psth_bin[i, sidx] = 1\n",
    "            psth_matrix[i] = np.sum(np.reshape(psth_bin[i], [len(timebase), osamp*timebin]), axis=1)\n",
    "        \n",
    "        \n",
    "        psth = np.full((len(image_numbers), max_number_of_reps, len(timebase)), np.nan)  # Re-ordered PSTH\n",
    "\n",
    "        for i, image_num in enumerate(image_numbers):\n",
    "            index_in_table = np.where(mwk_data.stimulus_presented == image_num)[0]\n",
    "            selected_cells = psth_matrix[index_in_table, :]\n",
    "            psth[i, :selected_cells.shape[0], :] = selected_cells\n",
    "\n",
    "        logging.info(psth.shape)\n",
    "        # Save psth data\n",
    "        PSTH[:,:,:,num] = psth\n",
    "        \n",
    "    meta = {'start_time_ms': start_time, 'stop_time_ms': stop_time, 'tb_ms': timebin}\n",
    "    cmbined_psth = {'psth': PSTH, 'meta': meta}\n",
    "\n",
    "    return cmbined_psth\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory   = '/braintree/home/aliya277/dandi_brainscore/inventory'\n",
    "start_time  = 0\n",
    "stop_time   = 300\n",
    "timebin     = 10\n",
    "counter = 0\n",
    "\n",
    "\n",
    "for folder, num_file in tqdm(zip(os.listdir(inventory), range(len(os.listdir(inventory)))), \\\n",
    "    total = len(os.listdir(inventory)), desc='Processing ...'):\n",
    "\n",
    "\n",
    "    path = os.path.join(inventory, folder)\n",
    "    SessionName = folder\n",
    "    config_path = '/om/user/aliya277/dandi_brainscore'\n",
    "    with open(os.path.join(config_path,\"config_nwb.yaml\") , \"r\") as f:\n",
    "            config = yaml.load(f, Loader = yaml.FullLoader)\n",
    "\n",
    "    print('Creating NWB file...')\n",
    "    nwbfile = create_nwb(config,path)\n",
    "    \n",
    "\n",
    "    if 'h5Files' in os.listdir(path):\n",
    "        print('Opening PSTH...')\n",
    "        filename = os.listdir(os.path.join(path, 'h5Files'))[0]\n",
    "        file = h5py.File(os.path.join(path, 'h5Files', filename),'r+') \n",
    "        data = file['psth'][:]\n",
    "        file.close()\n",
    "\n",
    "\n",
    "        print('Adding PSTH...')\n",
    "        nwbfile.add_scratch(\n",
    "            data,\n",
    "            name=\"psth\",\n",
    "            description=\"psth, uncorrected [channels x stimuli x reps x timebins]\",\n",
    "            )\n",
    "        \n",
    "    else: counter +=1\n",
    "    \n",
    "    try: io.close()\n",
    "    except:pass\n",
    "\n",
    "    if os.path.isfile(os.path.join(path, f\"{SessionName}.nwb\")):\n",
    "         os.remove(os.path.join(path, f\"{SessionName}.nwb\"))\n",
    "\n",
    "    print('Saving NWB File..')\n",
    "    io = NWBHDF5IO(os.path.join(path, f\"{SessionName}.nwb\"), \"w\") \n",
    "    io.write(nwbfile)\n",
    "    io.close()\n",
    "    print(\"File saved.\")\n",
    "\n",
    "        # psth    = get_psth_from_nwb(nwbfile, path, start_time, stop_time, timebin)\n",
    "        # data    = psth['psth']\n",
    "\n",
    "print(f'{counter} SpikeTimes do not have an h5 file.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_names(filename):\n",
    "    assignment  = filename.split('.')[0].split('-')[1]\n",
    "    number      = filename.split('.')[0].split('-')[2]\n",
    "    return np.asarray([assignment, number])\n",
    "\n",
    "def create_nwb(config, path):\n",
    "\n",
    "    desired_timezone = pytz.timezone('US/Eastern')\n",
    "\n",
    "    ################ CREATE NWB FILE WITH METADATA ################################\n",
    "    ###############################################################################\n",
    "    nwbfile = NWBFile(\n",
    "        session_description     = config['session_info']['session_description'],\n",
    "        identifier              = config['metadata']['identifier'],\n",
    "        session_start_time      = desired_timezone.localize(config['metadata']['session_start_time']),\n",
    "        file_create_date        = desired_timezone.localize(config['metadata']['file_create_date']),\n",
    "        experimenter            = config['general']['lab_info']['experimenter'],\n",
    "        experiment_description  = config['general']['experiment_info']['experiment_description'],\n",
    "        session_id              = config['session_info']['session_id'],\n",
    "        lab                     = config['general']['lab_info']['lab'],                     \n",
    "        institution             = config['general']['lab_info']['institution'],                                    \n",
    "        keywords                = config['general']['experiment_info']['keywords'],\n",
    "        surgery                 = config['general']['experiment_info']['surgery']\n",
    "    )\n",
    "\n",
    "    ################ CREATE SUBJECT ################################################\n",
    "    ################################################################################\n",
    "    nwbfile.subject = Subject(\n",
    "        subject_id  = config['subject']['subject_id'],\n",
    "        date_of_birth= config['subject']['date_of_birth'],\n",
    "        species     = config['subject']['species'],\n",
    "        sex         = config['subject']['sex'],\n",
    "    )\n",
    "\n",
    "    ################ CREATE HARDWARE LINKS #########################################\n",
    "    ################################################################################\n",
    "    nwbfile.create_device(\n",
    "        name        = config['hardware']['system_name'], \n",
    "        description = config['hardware']['system_description'], \n",
    "        manufacturer= config['hardware']['system_manuf']\n",
    "    )\n",
    "\n",
    "    nwbfile.create_device(\n",
    "        name        = config['hardware']['adapter_manuf'], \n",
    "        description = config['hardware']['adapter_description'], \n",
    "        manufacturer= config['hardware']['adapter_manuf']\n",
    "    )\n",
    "\n",
    "    nwbfile.create_device(\n",
    "        name        = config['hardware']['monitor_name'], \n",
    "        description = config['hardware']['monitor_description'], \n",
    "        manufacturer= config['hardware']['monitor_manuf']\n",
    "    )\n",
    "\n",
    "    nwbfile.create_device(\n",
    "        name        = config['hardware']['photodiode_name'], \n",
    "        description = config['hardware']['photodiode_description'], \n",
    "        manufacturer= config['hardware']['photodiode_manuf']\n",
    "    )\n",
    "    \n",
    "    nwbfile.create_device(\n",
    "        name        = 'Software Used', \n",
    "        description = str(['Mworks Client: '+config['software']['mwclient_version'],\\\n",
    "                        'Mworks Server: '+config['software']['mwserver_version'],\\\n",
    "                        'OS: '+config['software']['OS'],\\\n",
    "                        'Intan :'+config['software']['intan_version']])\n",
    "    )\n",
    "\n",
    "    ################ CREATE ELECTRODE LINKS ########################################\n",
    "    ################################################################################\n",
    "    electrodes = nwbfile.create_device(\n",
    "        name        = config['hardware']['electrode_name'], \n",
    "        description = config['hardware']['electrode_description'], \n",
    "        manufacturer= config['hardware']['electrode_manuf']\n",
    "    )\n",
    "\n",
    "    all_files = sorted(os.listdir(os.path.join(path, 'SpikeTimes')))\n",
    "    \n",
    "    name_accumulator = []\n",
    "    for file in all_files:\n",
    "        name_accumulator.append(read_names(file))\n",
    "    names = np.vstack(name_accumulator)\n",
    "\n",
    "    nwbfile.add_electrode_column(name=\"label\", description=\"label of electrode\")\n",
    "    groups, count_groups = np.unique(names[:,0], return_counts =True)\n",
    "    ids                  = names[:,1]\n",
    "    counter              = 0\n",
    "    # create ElectrodeGroups A, B, C, ..\n",
    "    for group, count_group in zip(groups, count_groups):\n",
    "        if len(groups) == 6:\n",
    "            electrode_description = \"Serialnumber: {}. Adapter Version: {}\".format(config['array_info']['array_{}'.format(group)]['serialnumber'],\\\n",
    "                            config['array_info']['array_{}'.format(group)]['adapterversion']),\n",
    "        else: \n",
    "            electrode_description = \"Serialnumber: {}\".format(config['array_info']['array_{}'.format(group)]['serialnumber']),\n",
    "                \n",
    "        \n",
    "        electrode_group = nwbfile.create_electrode_group(\n",
    "            name        = \"group_{}\".format(group),\n",
    "            description = electrode_description[0],\n",
    "            device      = electrodes,\n",
    "            location    = 'hemisphere, region, subregion: '+str([config['array_info']['array_{}'.format(group)]['hemisphere'],\\\n",
    "                                config['array_info']['array_{}'.format(group)]['region'],\n",
    "                                config['array_info']['array_{}'.format(group)]['subregion']]),\n",
    "            position    = config['array_info']['array_{}'.format(group)]['position']\n",
    "        )\n",
    "\n",
    "        # create Electrodes 001, 002, ..., 032 in ElectrodeGroups per channel\n",
    "        for ichannel in range(count_group):\n",
    "            nwbfile.add_electrode(\n",
    "                group       = electrode_group,\n",
    "                label       = ids[counter],\n",
    "                location    = 'row, col, elec'+str(json.loads(config['array_info']['intan_electrode_labeling_[row,col,id]'])[counter])\n",
    "            )\n",
    "            counter += 1     \n",
    "\n",
    "    ################ ADD SPIKE TIMES ###############################################\n",
    "    ################################################################################\n",
    "\n",
    "    nwbfile.add_unit_column(name=\"unit\", description=\"millisecond\") \n",
    "    for filename, i in zip(sorted(os.listdir(os.path.join(path, 'SpikeTimes'))), range(len(os.listdir(os.path.join(path, 'SpikeTimes'))))):\n",
    "        [assignment, number] = read_names(filename)\n",
    "        file_path = os.path.join(path, 'SpikeTimes', filename)\n",
    "        data = scipy.io.loadmat(file_path, squeeze_me=True,\n",
    "                        variable_names='spike_time_ms')['spike_time_ms']\n",
    "        nwbfile.add_unit(\n",
    "            spike_times = data, \n",
    "            electrodes  = [i],\n",
    "            electrode_group = nwbfile.electrode_groups[f'group_{assignment}'], \n",
    "            unit = 'ms'\n",
    "        )\n",
    "\n",
    "    ################ ADD TRIAL TIMES ###############################################\n",
    "    ################################################################################\n",
    "    last_spike = data[-1]\n",
    "    del data\n",
    "    \n",
    "    try: \n",
    "        [on, off] = config['session_info']['session_description'].split(', ')[3].split(': ')[-1].split(\"/\")\n",
    "        on_start  = 0\n",
    "        on_dur    = int(on)\n",
    "        off_dur   = int(off)\n",
    "\n",
    "        \n",
    "        nwbfile.add_trial_column(name=\"unit\", description=\"millisecond\")\n",
    "        while on_start < last_spike:\n",
    "\n",
    "            nwbfile.add_trial(\n",
    "                start_time= float(on_start),\n",
    "                stop_time = float(on_start+on_dur),\n",
    "                unit = 'ms')\n",
    "        \n",
    "            on_start += on_dur+off_dur\n",
    "    except: pass \n",
    "\n",
    "    ################ ADD PSTH IF AVAIL #############################################\n",
    "    ################################################################################\n",
    "    if 'h5Files' in os.listdir(path):\n",
    "\n",
    "        filename = os.listdir(os.path.join(path, 'h5Files'))[0]\n",
    "        file = h5py.File(os.path.join(path, 'h5Files', filename),'r+') \n",
    "        data = file['psth'][:]\n",
    "        file.close()\n",
    "\n",
    "        nwbfile.add_scratch(\n",
    "            data,\n",
    "            name=\"psth\",\n",
    "            description=\"psth, uncorrected [stimuli x reps x timebins x channels]\",\n",
    "            )\n",
    "        \n",
    "\n",
    "    return nwbfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file did not work: norm_FOSS.sub_pico.20230823_124104.proc\n",
    "tail: output.log: Datei abgeschnitten\n",
    "This file did not work: norm_FOSS.sub_pico.20230803_105856.proc\n",
    "tail: output.log: Datei abgeschnitten\n",
    "This file did not work: exp_gratingsAdap_s3.sub_pico.20230801_163355.proc\n",
    "tail: output.log: Datei abgeschnitten\n",
    "tail: output.log: Datei abgeschnitten\n",
    "This file did not work: norm_FOSS.sub_pico.20230628_124854.proc\n",
    "tail: output.log: Datei abgeschnitten\n",
    "node069\n",
    "tail: output.log: Datei abgeschnitten\n",
    "tail: output.log: Datei abgeschnitten\n",
    "node069\n",
    "This file did not work: norm_FOSS.sub_pico.20220929_170635.proc\n",
    "5127.proc\n",
    "tail: output.log: Datei abgeschnitten\n",
    "tail: output.log: Datei abgeschnitten\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "OSS.sub_pico.20230125_144402.proc\n",
    "tail: output.log: Datei abgeschnitten\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "This file did not work: norm_FOSS.sub_pico.20230127_160227.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230126_150653.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230505_130540.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230531_134209.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230516_120137.proc\n",
    "This file did not work: exp_robustness_guy_d1_v34.sub_pico.20230906_122650.proc\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "This file did not work: norm_FOSS.sub_pico.20230510_111142.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230428_111937.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20220902_145351.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20220615_113442.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20220907_142157.proc\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "This file did not work: norm_FOSS.sub_pico.20230713_141950.proc\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "This file did not work: norm_FOSS.sub_pico.20230817_141050.proc\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "This file did not work: norm_FOSS.sub_pico.20230328_145456.proc\n",
    "This file did not work: exp_gratingsAdap_s1.sub_pico.20230801_151117.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230606_134244.proc\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "This file did not work: norm_FOSS.sub_pico.20230621_140747.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230814_122811.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230821_120107.proc\n",
    "This file did not work: exp_Mayo_day_5.sub_pico.20230818_101245.proc\n",
    "This file did not work: norm_HVM.sub_pico.20230404_142414.proc\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "This file did not work: norm_FOSS.sub_pico.20230626_131126.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230629_132813.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230711_142158.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230630_135832.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230725_113504.proc\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "This file did not work: norm_FOSS.sub_pico.20230830_110931.proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = 0\n",
    "stop_time = 300\n",
    "timebin = 10\n",
    "\n",
    "\n",
    "def get_psth_from_nwb(nwbfile, path, start_time, stop_time, timebin, n_stimuli=None):\n",
    "\n",
    "    # Find the MWORKS File\n",
    "    with open(os.path.join(path, 'NWBInfo.txt')) as f:\n",
    "        lines = f.readlines()\n",
    "        line2 = lines[0].split(',')[1]\n",
    "        ind = line2.split('/').index('intanproc')\n",
    "        mwk_file = os.path.join('/', *line2.split('/')[0:ind], 'mworksproc', line2.split('/')[ind+1]+'_mwk.csv')\n",
    "    assert os.path.isfile(mwk_file)==True\n",
    "\n",
    "    ################ MODIFIED FROM THE SPIKE-TOOLS-CHONG CODE ######################\n",
    "    ################################################################################\n",
    "    \n",
    "    mwk_data = pd.read_csv(mwk_file)\n",
    "    mwk_data = mwk_data[mwk_data.fixation_correct == 1]\n",
    "    if 'photodiode_on_us' in mwk_data.keys():\n",
    "        samp_on_ms = np.asarray(mwk_data['photodiode_on_us']) / 1000.\n",
    "        logging.info('Using photodiode signal for sample on time')\n",
    "    else:\n",
    "        samp_on_ms = np.asarray(mwk_data['samp_on_us']) / 1000.\n",
    "        logging.info('Using MWorks digital signal for sample on time')\n",
    "    \n",
    "    # Load spikeTime file for current channel\n",
    "    spikeTimes = nwbfile.units[:].spike_times\n",
    "    # Re-order the psth to image x reps\n",
    "    max_number_of_reps = max(np.bincount(mwk_data['stimulus_presented']))  # Max reps obtained for any image\n",
    "    if max_number_of_reps == 0:\n",
    "        exit()\n",
    "    mwk_data['stimulus_presented'] = mwk_data['stimulus_presented'].astype(int)  # To avoid indexing errors\n",
    "    \n",
    "    if n_stimuli is None:\n",
    "            image_numbers = np.unique(mwk_data['stimulus_presented'])  # TODO: if not all images are shown (for eg, exp cut short), you'll have to manually type in total # images\n",
    "    else:\n",
    "        image_numbers = np.arange(1,n_stimuli+1) # all of my image starts with #1\n",
    "\n",
    "    timebase = np.arange(start_time, stop_time, timebin)\n",
    "    PSTH = np.full((len(image_numbers), max_number_of_reps, len(timebase),spikeTimes.shape[0]), np.nan)\n",
    "\n",
    "    for num in range(spikeTimes.shape[0]):\n",
    "        spikeTime = np.asanyarray(spikeTimes[num])\n",
    "        osamp = 10\n",
    "        psth_bin = np.zeros((len(samp_on_ms), osamp*(stop_time-start_time)))\n",
    "        psth_matrix = np.full((len(samp_on_ms), len(timebase)), np.nan)\n",
    "\n",
    "        for i in range(len(samp_on_ms)):\n",
    "\n",
    "            sidx = np.floor(osamp*(spikeTime[(spikeTime>=(samp_on_ms[i]+start_time))*(spikeTime<(samp_on_ms[i]+stop_time))]-(samp_on_ms[i]+start_time))).astype(int)\n",
    "            psth_bin[i, sidx] = 1\n",
    "            psth_matrix[i] = np.sum(np.reshape(psth_bin[i], [len(timebase), osamp*timebin]), axis=1)\n",
    "        \n",
    "        \n",
    "        psth = np.full((len(image_numbers), max_number_of_reps, len(timebase)), np.nan)  # Re-ordered PSTH\n",
    "\n",
    "        for i, image_num in enumerate(image_numbers):\n",
    "            index_in_table = np.where(mwk_data.stimulus_presented == image_num)[0]\n",
    "            selected_cells = psth_matrix[index_in_table, :]\n",
    "            psth[i, :selected_cells.shape[0], :] = selected_cells\n",
    "\n",
    "        logging.info(psth.shape)\n",
    "        # Save psth data\n",
    "        PSTH[:,:,:,num] = psth\n",
    "        \n",
    "    meta = {'start_time_ms': start_time, 'stop_time_ms': stop_time, 'tb_ms': timebin}\n",
    "    cmbined_psth = {'psth': PSTH, 'meta': meta}\n",
    "\n",
    "    return cmbined_psth\n",
    "    \n",
    "# psth = get_psth_from_nwb(nwbfile, path, start_time, stop_time, timebin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_nwb(nwbfile, path):\n",
    "    \n",
    "    SessionName = path.split('/')[-1]\n",
    "\n",
    "    # Crete Temporary path on BrainTree (Openmind Did not Allow Saving of H5 Files.)\n",
    "    temp_path = '/braintree/home/aliya277/dandi_brainscore/inventory'\n",
    "    try: os.mkdir(temp_path)\n",
    "    except: pass\n",
    "\n",
    "    # Save NWB File on Braintree\n",
    "    io = NWBHDF5IO(os.path.join(temp_path, f\"{SessionName}.nwb\"), \"w\") \n",
    "    io.write(nwbfile)\n",
    "    io.close()\n",
    "\n",
    "    # Copy NWB to Inventory Directory on Openmind\n",
    "    shutil.copy2(os.path.join(temp_path, f\"{SessionName}.nwb\"), os.path.join(path, f\"{SessionName}.nwb\"))   \n",
    "    \n",
    "    # Remove Temporary File and Folder on Braintree\n",
    "    os.remove(os.path.join(temp_path, f\"{SessionName}.nwb\"))\n",
    "    os.rmdir(os.path.join(temp_path))\n",
    "\n",
    "    return nwbfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'psth' already exists in NWBFile 'root'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/om/user/aliya277/dandi_brainscore/create_proc_nwb.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bopenmind.mit.edu/om/user/aliya277/dandi_brainscore/create_proc_nwb.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m data \u001b[39m=\u001b[39m psth[\u001b[39m'\u001b[39m\u001b[39mpsth\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bopenmind.mit.edu/om/user/aliya277/dandi_brainscore/create_proc_nwb.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m nwbfile\u001b[39m.\u001b[39;49madd_scratch(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bopenmind.mit.edu/om/user/aliya277/dandi_brainscore/create_proc_nwb.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     data,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bopenmind.mit.edu/om/user/aliya277/dandi_brainscore/create_proc_nwb.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpsth\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bopenmind.mit.edu/om/user/aliya277/dandi_brainscore/create_proc_nwb.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpsth, uncorrected [channels x stimuli x reps x timebins]\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bopenmind.mit.edu/om/user/aliya277/dandi_brainscore/create_proc_nwb.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.10/site-packages/hdmf/utils.py:644\u001b[0m, in \u001b[0;36mdocval.<locals>.dec.<locals>.func_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfunc_call\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    643\u001b[0m     pargs \u001b[39m=\u001b[39m _check_args(args, kwargs)\n\u001b[0;32m--> 644\u001b[0m     \u001b[39mreturn\u001b[39;00m func(args[\u001b[39m0\u001b[39;49m], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.10/site-packages/pynwb/file.py:1097\u001b[0m, in \u001b[0;36mNWBFile.add_scratch\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m     \u001b[39mif\u001b[39;00m description \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1095\u001b[0m         warn(\u001b[39m'\u001b[39m\u001b[39mThe description argument is ignored when adding an NWBContainer, ScratchData, or \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1096\u001b[0m              \u001b[39m'\u001b[39m\u001b[39mDynamicTable to scratch.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1097\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_scratch(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.10/site-packages/hdmf/utils.py:644\u001b[0m, in \u001b[0;36mdocval.<locals>.dec.<locals>.func_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfunc_call\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    643\u001b[0m     pargs \u001b[39m=\u001b[39m _check_args(args, kwargs)\n\u001b[0;32m--> 644\u001b[0m     \u001b[39mreturn\u001b[39;00m func(args[\u001b[39m0\u001b[39;49m], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.10/site-packages/hdmf/container.py:1031\u001b[0m, in \u001b[0;36mMultiContainerInterface.__make_add.<locals>._func\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1029\u001b[0m     \u001b[39mif\u001b[39;00m tmp\u001b[39m.\u001b[39mname \u001b[39min\u001b[39;00m d:\n\u001b[1;32m   1030\u001b[0m         msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m already exists in \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (tmp\u001b[39m.\u001b[39mname, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[0;32m-> 1031\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1032\u001b[0m     d[tmp\u001b[39m.\u001b[39mname] \u001b[39m=\u001b[39m tmp\n\u001b[1;32m   1033\u001b[0m \u001b[39mreturn\u001b[39;00m container\n",
      "\u001b[0;31mValueError\u001b[0m: 'psth' already exists in NWBFile 'root'"
     ]
    }
   ],
   "source": [
    "data = psth['psth']\n",
    "\n",
    "nwbfile.add_scratch(\n",
    "    data,\n",
    "    name=\"psth\",\n",
    "    description=\"psth, uncorrected [channels x stimuli x reps x timebins]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 86, 21, 30)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwbfile.scratch['psth'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5path = '/om/user/aliya277/inventory/norm_FOSS.sub_pico.20230823_124104.proc/h5Files/230823.pico.rsvp.normalizers.experiment_psth_raw.h5'\n",
    "\n",
    "def open_h5(path):\n",
    "    \n",
    "    filename = path.split('/')[-1]\n",
    "    # Crete Temporary path on BrainTree (Openmind Did not Allow Saving of H5 Files.)\n",
    "    temp_path = '/braintree/home/aliya277/dandi_brainscore/inventory'\n",
    "    try: os.mkdir(temp_path)\n",
    "    except: pass\n",
    "\n",
    "\n",
    "    # Copy File to Temporary Path\n",
    "    shutil.copy2(os.path.join(path), os.path.join(temp_path, filename))   \n",
    "\n",
    "    file = h5py.File(os.path.join(temp_path, filename),'r+')    \n",
    "\n",
    "    return file\n",
    "\n",
    "def close_h5(path):\n",
    "\n",
    "    filename = path.split('/')[-1]\n",
    "    temp_path = '/braintree/home/aliya277/dandi_brainscore/inventory'\n",
    "    os.remove(os.path.join(temp_path, filename))\n",
    "    os.rmdir(temp_path)\n",
    "\n",
    "file = open_h5(h5path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(psth['psth'], file['psth'][:], equal_nan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "0      [69.2, 135.55, 207.55, 242.9, 281.95, 331.5, 3...\n",
       "1      [22.1, 57.3, 63.9, 135.35, 167.1, 175.6, 225.8...\n",
       "2      [58.1, 69.15, 167.7, 171.1, 171.3, 286.9, 362....\n",
       "3      [16.9, 158.04999999999998, 237.9, 304.09999999...\n",
       "4      [49.75, 68.9, 87.65, 115.1, 342.95, 412.650000...\n",
       "                             ...                        \n",
       "187    [2.6, 5.3, 81.3, 155.35, 185.35, 251.649999999...\n",
       "188    [35.2, 164.14999999999998, 164.70000000000002,...\n",
       "189    [35.2, 155.35, 251.8, 286.95, 287.1, 287.4, 29...\n",
       "190    [2.6, 162.35, 198.8, 251.5, 251.7, 287.2, 294....\n",
       "191    [46.449999999999996, 73.85, 81.44999999999999,...\n",
       "Name: spike_times, Length: 192, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwbfile.units[:].spike_times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547d15e3cf1a43ceb91679bc5cf39827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='session_description:', layout=Layout(max_height='40px', max_width='"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwb2widget(nwbfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dandibs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
