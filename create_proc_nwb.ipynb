{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from uuid import uuid4\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import os, yaml, glob, json\n",
    "import pandas as pd\n",
    "from nwbwidgets import nwb2widget\n",
    "from pynwb import NWBHDF5IO, NWBFile\n",
    "from pynwb.file import Subject\n",
    "import shutil\n",
    "import logging\n",
    "import h5py\n",
    "import pytz\n",
    "from tqdm import tqdm\n",
    "from utils.nwb_helper import  create_nwb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Create Custom Config Files for Each Recording #################\n",
    "###############################################################################\n",
    "from utils.config_helper import create_yaml\n",
    "import os\n",
    "dir = '/braintree/home/aliya277/inventory/'\n",
    "array_meta_path  = '/braintree/data2/active/users/sgouldin/array-metadata'\n",
    "for files in os.listdir(dir):\n",
    "    num_files = 0\n",
    "    path = os.path.join(dir, files)\n",
    "    try: \n",
    "        num_files = len(os.listdir(os.path.join(path,'SpikeTimes')))\n",
    "    except: pass\n",
    "    \n",
    "    if num_files == 192: \n",
    "        array_metadata = os.path.join(array_meta_path, '021023_pico_mapping_noCIT_adapter_version.json')\n",
    "        adapter_info_avail = True\n",
    "    elif num_files == 288: \n",
    "        array_metadata = os.path.join(array_meta_path,'pico_firstmapping_Lhem_2023.json')\n",
    "        adapter_info_avail = False\n",
    "        \n",
    "    create_yaml(path, array_metadata, adapter_info_avail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all files: 201\n",
      "Creating nwb for folder exp_Alireza_paradigm1.sub_pico.20230616_103724.proc\n",
      "Saving NWB File..\n",
      "File 0 saved.\n",
      "Creating nwb for folder exp_domain_transfer_2023.sub_pico.20230216_150919.proc\n",
      "Saving NWB File..\n",
      "File 1 saved.\n",
      "Creating nwb for folder exp_domain_transfer_2023.sub_pico.20230223_134049.proc\n",
      "Saving NWB File..\n",
      "File 2 saved.\n",
      "Creating nwb for folder exp_facesMSFDE.sub_pico.20230327_134955.proc\n",
      "Saving NWB File..\n",
      "File 3 saved.\n",
      "Creating nwb for folder exp_gratingsAdap_s3.sub_pico.20230801_163355.proc\n",
      "Saving NWB File..\n",
      "File 4 saved.\n",
      "Creating nwb for folder exp_gratingsAdap_s5.sub_pico.20230801_144134.proc\n",
      "Saving NWB File..\n",
      "File 5 saved.\n",
      "Creating nwb for folder exp_HVM_var6.sub_pico.20230221_142542.proc\n",
      "Saving NWB File..\n",
      "File 6 saved.\n",
      "Creating nwb for folder exp_HVM_var6.sub_pico.20230223_150327.proc\n",
      "Saving NWB File..\n",
      "File 7 saved.\n",
      "Creating nwb for folder exp_HVM-var6-subset.sub_pico.20230303_180103.proc\n",
      "Saving NWB File..\n",
      "File 8 saved.\n",
      "Creating nwb for folder exp_images_in_context3.sub_pico.20230531_135649.proc\n",
      "Saving NWB File..\n",
      "File 9 saved.\n",
      "Creating nwb for folder exp_images_in_context.sub_pico.20230324_121431.proc\n",
      "Saving NWB File..\n",
      "File 10 saved.\n",
      "Creating nwb for folder exp_Mayo_day_1.sub_pico.20230814_124244.proc\n",
      "Saving NWB File..\n",
      "File 11 saved.\n",
      "Creating nwb for folder exp_Mayo_day_1.sub_pico.20230814_143555.proc\n",
      "Saving NWB File..\n",
      "File 12 saved.\n",
      "Creating nwb for folder exp_Mayo_day_2.sub_pico.20230815_111611.proc\n",
      "Saving NWB File..\n",
      "File 13 saved.\n",
      "Creating nwb for folder exp_Mayo_day_5.sub_pico.20230818_121533.proc\n",
      "Saving NWB File..\n",
      "File 14 saved.\n",
      "Creating nwb for folder exp_Mayo_ONR.sub_pico.20230804_154202.proc\n",
      "Saving NWB File..\n",
      "File 15 saved.\n",
      "Creating nwb for folder exp_Mayo_ONR.sub_pico.20230808_143723.proc\n",
      "Saving NWB File..\n",
      "File 16 saved.\n",
      "Creating nwb for folder exp_moca.sub_pico.20230719_123506.proc\n",
      "Saving NWB File..\n",
      "File 17 saved.\n",
      "Creating nwb for folder exp_MURI1320-2023.sub_pico.20230131_183129.proc\n",
      "Saving NWB File..\n",
      "File 18 saved.\n",
      "Creating nwb for folder exp_MURI1320-2023.sub_pico.20230201_152725.proc\n",
      "Saving NWB File..\n",
      "File 19 saved.\n",
      "Creating nwb for folder exp_novel500.sub_pico.20230530_132227.proc\n",
      "Saving NWB File..\n",
      "File 20 saved.\n",
      "Creating nwb for folder exp_objectsize.sub_pico.20230505_132032.proc\n",
      "Saving NWB File..\n",
      "File 21 saved.\n",
      "Creating nwb for folder exp_robustness_guy_d0_v24.sub_pico.20230605_130743.proc\n",
      "Saving NWB File..\n",
      "File 22 saved.\n",
      "Creating nwb for folder exp_robustness_guy_d0_v24.sub_pico.20230606_140254.proc\n",
      "Saving NWB File..\n",
      "File 23 saved.\n",
      "Creating nwb for folder exp_robustness_guy_d0_v31.sub_pico.20230710_132817.proc\n",
      "Saving NWB File..\n",
      "File 24 saved.\n",
      "Creating nwb for folder exp_robustness_guy_d0_v37.sub_pico.20230919_095857.proc\n",
      "Saving NWB File..\n",
      "File 25 saved.\n",
      "Creating nwb for folder exp_robustness_guy_d0_v7_chong_dryrun.sub_pico.20230717_130030.proc\n",
      "Saving NWB File..\n",
      "File 26 saved.\n",
      "Creating nwb for folder exp_robustness_guy_d0_v7_chong.sub_pico.20230724_110235.proc\n",
      "Saving NWB File..\n",
      "File 27 saved.\n",
      "Creating nwb for folder exp_robustness_guy_d1_v26.sub_pico.20230622_131538.proc\n",
      "Saving NWB File..\n",
      "File 28 saved.\n",
      "Creating nwb for folder exp_robustness_guy_d3_v7_chong.sub_pico.20230727_155612.proc\n",
      "Saving NWB File..\n",
      "File 29 saved.\n",
      "Creating nwb for folder exp_robustness_guy_d7_v7_chong.sub_pico.20230731_110204.proc\n",
      "Saving NWB File..\n",
      "File 30 saved.\n",
      "Creating nwb for folder exp_Zaidi2022_facescrub-small.sub_pico.20230331_135015.proc\n",
      "Saving NWB File..\n"
     ]
    }
   ],
   "source": [
    "############### Iterate through every File and Create NWB #####################\n",
    "###############################################################################\n",
    "\n",
    "inventory   = '/braintree/home/aliya277/inventory/'\n",
    "all_files = os.listdir(inventory)\n",
    "i = 0\n",
    "print(f'Number of all files: {len(all_files)}')\n",
    "for folder in all_files[31:]:\n",
    "    print(f'Creating nwb for folder {folder}')\n",
    "    path = os.path.join(inventory, folder)\n",
    "    SessionName = folder\n",
    "    # try:\n",
    "    with open(os.path.join(path,\"config_nwb.yaml\") , \"r\") as f:\n",
    "        config = yaml.load(f, Loader = yaml.FullLoader)\n",
    "\n",
    "    nwbfile = create_nwb(config, path)\n",
    "\n",
    "    if os.path.isfile(os.path.join(path, f\"{SessionName}.nwb\")):\n",
    "        os.remove(os.path.join(path, f\"{SessionName}.nwb\"))\n",
    "\n",
    "    print('Saving NWB File..')\n",
    "    io = NWBHDF5IO(os.path.join(path, f\"{SessionName}.nwb\"), \"w\") \n",
    "    io.write(nwbfile)\n",
    "    io.close()\n",
    "    print(f\"File {i} saved.\")\n",
    "    i += 1\n",
    "    # except: print(f'This file did not work: {SessionName}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Check if All Files are Written and can be Opened ##############\n",
    "###############################################################################\n",
    "\n",
    "inventory   = '/braintree/home/aliya277/inventory/'\n",
    "all_files = os.listdir(inventory)\n",
    "for folder in all_files:\n",
    "        path = os.path.join(inventory, folder)\n",
    "        try:\n",
    "                io = NWBHDF5IO(os.path.join(path, f\"{folder}.nwb\"), \"r\") \n",
    "                nwbfile = io.read()\n",
    "                io.close()\n",
    "        except: print(f'This File can not be opened: {folder}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Validate All Files Using pwnyb, nwbinspectors #################\n",
    "###############################################################################\n",
    "\n",
    "import io, os, pynwb\n",
    "from pynwb import validate\n",
    "from nwbinspector import inspect_nwbfile\n",
    "\n",
    "inventory   = '/braintree/home/aliya277/inventory/'\n",
    "all_nwb_paths = []\n",
    "all_files = os.listdir(inventory)\n",
    "for folder in all_files:\n",
    "    path = os.path.join(inventory, folder, f'{folder}.nwb')\n",
    "    all_nwb_paths.append(path)\n",
    "    \n",
    "\n",
    "\n",
    "pynwb_validation = validate(paths = all_nwb_paths)\n",
    "\n",
    "nwbinspector_validation = []\n",
    "for path in all_nwb_paths:\n",
    "    results = list(inspect_nwbfile(nwbfile_path=path))\n",
    "    nwbinspector_validation.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Add PSTH if Needed ############################################\n",
    "###############################################################################\n",
    "\n",
    "inventory   = '/braintree/home/aliya277/inventory/'\n",
    "all_files = os.listdir(inventory)\n",
    "for folder in all_files:\n",
    "        path = os.path.join(inventory, folder)\n",
    "        try:\n",
    "                io = NWBHDF5IO(os.path.join(path, f\"{folder}.nwb\"), \"r\") \n",
    "                nwbfile = io.read()\n",
    "                io.close()\n",
    "        except: print(f'This File can not be opened: {folder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "579b41b5614846578a7c03a80dec0efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='session_description:', layout=Layout(max_height='40px', max_width='…"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwb2widget(nwbfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'h5Files' in os.listdir(path):\n",
    "#     print('Opening PSTH...')\n",
    "#     filename = os.listdir(os.path.join(path, 'h5Files'))[0]\n",
    "#     file = h5py.File(os.path.join(path, 'h5Files', filename),'r+') \n",
    "#     data = file['psth'][:]\n",
    "#     file.close()\n",
    "\n",
    "\n",
    "#     print('Adding PSTH...')\n",
    "#     nwbfile.add_scratch(\n",
    "#         data,\n",
    "#         name=\"psth\",\n",
    "#         description=\"psth, uncorrected [channels x stimuli x reps x timebins]\",\n",
    "#         )\n",
    "    \n",
    "# else: counter +=1\n",
    "\n",
    "# try: io.close()\n",
    "# except:pass\n",
    "\n",
    "# if os.path.isfile(os.path.join(path, f\"{SessionName}.nwb\")):\n",
    "#         os.remove(os.path.join(path, f\"{SessionName}.nwb\"))\n",
    "\n",
    "# print('Saving NWB File..')\n",
    "# io = NWBHDF5IO(os.path.join(path, f\"{SessionName}.nwb\"), \"w\") \n",
    "# io.write(nwbfile)\n",
    "# io.close()\n",
    "# print(\"File saved.\")\n",
    "\n",
    "#     # psth    = get_psth_from_nwb(nwbfile, path, start_time, stop_time, timebin)\n",
    "#     # data    = psth['psth']\n",
    "\n",
    "# print(f'{counter} SpikeTimes do not have an h5 file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "inventory   = '/braintree/home/aliya277/inventory/'\n",
    "\n",
    "def find_directories_without_extension(root_dir, extension):\n",
    "    directory_paths = []\n",
    "    for foldername, subfolders, filenames in os.walk(root_dir):\n",
    "        depth = foldername[len(root_dir):].count(os.sep)\n",
    "        if depth ==  0:\n",
    "            # Check if any file in the directory has the specified extension\n",
    "            if not any(filename.endswith(extension) for filename in filenames):\n",
    "                directory_paths.append(os.path.join(root_dir, foldername))\n",
    "                # print(os.path.join(root_dir, foldername))\n",
    "    return directory_paths[1:]\n",
    "\n",
    "print(len(find_directories_without_extension(inventory, '.nwb')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "for obj in gc.get_objects():   # Browse through ALL objects\n",
    "    if isinstance(obj, h5py.File):   # Just HDF5 files\n",
    "        try:\n",
    "            obj.close()\n",
    "        except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def read_names(filename):\n",
    "    assignment  = filename.split('.')[0].split('-')[1]\n",
    "    number      = filename.split('.')[0].split('-')[2]\n",
    "    return np.asarray([assignment, number])\n",
    "\n",
    "def create_nwb(config, path):\n",
    "\n",
    "    SessionDate = path.split('/')[-1].split('.')[-2].split('_')[0]\n",
    "    SessionTime = path.split('/')[-1].split('.')[-2].split('_')[1]\n",
    "    SubjectName = path.split('/')[-1].split('.')[1].split('_')[1]\n",
    "    date_format = \"%Y%m%d %H%M%S\"\n",
    "\n",
    "    if config['subject']['subject_id'].lower() != SubjectName.lower():\n",
    "        raise ValueError(\"Subject Name incorrect.\")\n",
    "        \n",
    "    session_start_time_ = datetime.strptime(SessionDate+' '+SessionTime, date_format)\n",
    "    # Define the timezone you want to use (e.g., 'US/Eastern' for Boston)\n",
    "    desired_timezone = pytz.timezone('US/Eastern')\n",
    "    session_start_time = desired_timezone.localize(session_start_time_)\n",
    "\n",
    "    ################ CREATE NWB FILE WITH METADATA ################################\n",
    "    ###############################################################################\n",
    "    nwbfile = NWBFile(\n",
    "        session_description     = config['session_info']['session_description'],\n",
    "        identifier              = config['metadata']['identifier'],\n",
    "        session_start_time      = session_start_time,\n",
    "        file_create_date        = config['metadata']['file_create_date'],\n",
    "        experimenter            = config['general']['lab_info']['experimenter'],\n",
    "        experiment_description  = config['general']['experiment_info']['experiment_description'],\n",
    "        session_id              = config['session_info']['session_id'],\n",
    "        lab                     = config['general']['lab_info']['lab'],                     \n",
    "        institution             = config['general']['lab_info']['institution'],                                    \n",
    "        keywords                = config['general']['experiment_info']['keywords'],\n",
    "        protocol                = config['general']['experiment_info']['protocol'],\n",
    "        related_publications    = config['general']['experiment_info']['related_publications'],\n",
    "        surgery                 = config['general']['experiment_info']['surgery']\n",
    "    )\n",
    "\n",
    "    ################ CREATE SUBJECT ################################################\n",
    "    ################################################################################\n",
    "    nwbfile.subject = Subject(\n",
    "        subject_id  = config['subject']['subject_id'],\n",
    "        date_of_birth= config['subject']['date_of_birth'],\n",
    "        species     = config['subject']['species'],\n",
    "        sex         = config['subject']['sex'],\n",
    "    )\n",
    "\n",
    "    ################ CREATE HARDWARE LINKS #########################################\n",
    "    ################################################################################\n",
    "    nwbfile.create_device(\n",
    "        name        = config['hardware']['system_name'], \n",
    "        description = config['hardware']['system_description'], \n",
    "        manufacturer= config['hardware']['system_manuf']\n",
    "    )\n",
    "\n",
    "    nwbfile.create_device(\n",
    "        name        = config['hardware']['adapter_manuf'], \n",
    "        description = config['hardware']['adapter_description'], \n",
    "        manufacturer= config['hardware']['adapter_manuf']\n",
    "    )\n",
    "\n",
    "    nwbfile.create_device(\n",
    "        name        = config['hardware']['monitor_name'], \n",
    "        description = config['hardware']['monitor_description'], \n",
    "        manufacturer= config['hardware']['monitor_manuf']\n",
    "    )\n",
    "\n",
    "    nwbfile.create_device(\n",
    "        name        = config['hardware']['photodiode_name'], \n",
    "        description = config['hardware']['photodiode_description'], \n",
    "        manufacturer= config['hardware']['photodiode_manuf']\n",
    "    )\n",
    "    \n",
    "    nwbfile.create_device(\n",
    "        name        = 'Software Used', \n",
    "        description = str(['Mworks Client: '+config['software']['mwclient_version'],\\\n",
    "                        'Mworks Server: '+config['software']['mwserver_version'],\\\n",
    "                        'OS: '+config['software']['OS'],\\\n",
    "                        'Intan :'+config['software']['intan_version']])\n",
    "    )\n",
    "\n",
    "    ################ CREATE ELECTRODE LINKS ########################################\n",
    "    ################################################################################\n",
    "    electrodes = nwbfile.create_device(\n",
    "        name        = config['hardware']['electrode_name'], \n",
    "        description = config['hardware']['electrode_description'], \n",
    "        manufacturer= config['hardware']['electrode_manuf']\n",
    "    )\n",
    "\n",
    "    all_files = sorted(os.listdir(os.path.join(path, 'SpikeTimes')))\n",
    "    \n",
    "    name_accumulator = []\n",
    "    for file in all_files:\n",
    "        name_accumulator.append(read_names(file))\n",
    "    names = np.vstack(name_accumulator)\n",
    "\n",
    "    nwbfile.add_electrode_column(name=\"label\", description=\"label of electrode\")\n",
    "    groups, count_groups = np.unique(names[:,0], return_counts =True)\n",
    "    ids                  = names[:,1]\n",
    "    counter              = 0\n",
    "    # create ElectrodeGroups A, B, C, ..\n",
    "    for group, count_group in zip(groups, count_groups):\n",
    "        electrode_group = nwbfile.create_electrode_group(\n",
    "            name        = \"group_{}\".format(group),\n",
    "            description = \"Serialnumber: {}. Adapter Version: {}\".format(config['array_info']['array_{}'.format(group)]['serialnumber'],\\\n",
    "                            config['array_info']['array_{}'.format(group)]['adapterversion']),\n",
    "            device      = electrodes,\n",
    "            location    = 'hemisphere, region, subregion: '+str([config['array_info']['array_{}'.format(group)]['hemisphere'],\\\n",
    "                                config['array_info']['array_{}'.format(group)]['region'],\n",
    "                                config['array_info']['array_{}'.format(group)]['subregion']]),\n",
    "            position    = config['array_info']['array_{}'.format(group)]['position']\n",
    "        )\n",
    "\n",
    "        # create Electrodes 001, 002, ..., 032 in ElectrodeGroups per channel\n",
    "        for ichannel in range(count_group):\n",
    "            nwbfile.add_electrode(\n",
    "                group       = electrode_group,\n",
    "                label       = ids[counter],\n",
    "                location    = 'row, col, elec'+str(json.loads(config['array_info']['intan_electrode_labeling_[row,col,id]'])[counter])\n",
    "            )\n",
    "            counter += 1     \n",
    "\n",
    "    ################ ADD SPIKE TIMES ###############################################\n",
    "    ################################################################################\n",
    "\n",
    "    nwbfile.add_unit_column(name=\"unit\", description=\"millisecond\") \n",
    "    for filename, i in zip(sorted(os.listdir(os.path.join(path, 'SpikeTimes'))), range(len(os.listdir(os.path.join(path, 'SpikeTimes'))))):\n",
    "        [assignment, number] = read_names(filename)\n",
    "        file_path = os.path.join(path, 'SpikeTimes', filename)\n",
    "        data = scipy.io.loadmat(file_path, squeeze_me=True,\n",
    "                        variable_names='spike_time_ms')['spike_time_ms']\n",
    "        nwbfile.add_unit(\n",
    "            spike_times = data, \n",
    "            electrodes  = [i],\n",
    "            electrode_group = nwbfile.electrode_groups[f'group_{assignment}'], \n",
    "            unit = 'ms'\n",
    "        )\n",
    "\n",
    "    ################ ADD TRIAL TIMES ###############################################\n",
    "    ################################################################################\n",
    "    last_spike = data[-1]\n",
    "    del data\n",
    "    with open(os.path.join(path, 'NWBInfo.txt')) as f:\n",
    "        lines = f.readlines()\n",
    "        line1 = lines[0].split(',')[0]\n",
    "        StimOnnOff = [float(line1.split('/')[0]),float(line1.split('/')[1])] \n",
    "\n",
    "    on_start  = 0\n",
    "    on_dur    = StimOnnOff[1]\n",
    "    off_dur   = StimOnnOff[1]\n",
    "\n",
    "    \n",
    "    nwbfile.add_trial_column(name=\"unit\", description=\"millisecond\")\n",
    "    while on_start < last_spike:\n",
    "\n",
    "        nwbfile.add_trial(\n",
    "            start_time= float(on_start),\n",
    "            stop_time = float(on_start+on_dur),\n",
    "            unit = 'ms')\n",
    "    \n",
    "        on_start += on_dur+off_dur\n",
    "\n",
    "    return nwbfile\n",
    "\n",
    "def get_psth_from_nwb(nwbfile, path, start_time, stop_time, timebin, n_stimuli=None):\n",
    "\n",
    "    # Find the MWORKS File\n",
    "    with open(os.path.join(path, 'NWBInfo.txt')) as f:\n",
    "        lines = f.readlines()\n",
    "        line2 = lines[0].split(',')[1]\n",
    "        ind = line2.split('/').index('intanproc')\n",
    "        mwk_file = glob.glob(os.path.join('/', *line2.split('/')[0:ind], 'mworksproc', \\\n",
    "                '_'.join(map(str, line2.split('/')[ind+1].split('_')[0:3]))+'*_mwk.csv'))   \n",
    "        \n",
    "    assert len(mwk_file) == 1\n",
    "    mwk_file = mwk_file[0]\n",
    "    assert os.path.isfile(mwk_file)==True\n",
    "\n",
    "    ################ MODIFIED FROM THE SPIKE-TOOLS-CHONG CODE ######################\n",
    "    ################################################################################\n",
    "    \n",
    "    mwk_data = pd.read_csv(mwk_file)\n",
    "    mwk_data = mwk_data[mwk_data.fixation_correct == 1]\n",
    "    if 'photodiode_on_us' in mwk_data.keys():\n",
    "        samp_on_ms = np.asarray(mwk_data['photodiode_on_us']) / 1000.\n",
    "        logging.info('Using photodiode signal for sample on time')\n",
    "    else:\n",
    "        samp_on_ms = np.asarray(mwk_data['samp_on_us']) / 1000.\n",
    "        logging.info('Using MWorks digital signal for sample on time')\n",
    "    \n",
    "    # Load spikeTime file for current channel\n",
    "    spikeTimes = nwbfile.units[:].spike_times\n",
    "    # Re-order the psth to image x reps\n",
    "    max_number_of_reps = max(np.bincount(mwk_data['stimulus_presented']))  # Max reps obtained for any image\n",
    "    if max_number_of_reps == 0:\n",
    "        exit()\n",
    "    mwk_data['stimulus_presented'] = mwk_data['stimulus_presented'].astype(int)  # To avoid indexing errors\n",
    "    \n",
    "    if n_stimuli is None:\n",
    "            image_numbers = np.unique(mwk_data['stimulus_presented'])  # TODO: if not all images are shown (for eg, exp cut short), you'll have to manually type in total # images\n",
    "    else:\n",
    "        image_numbers = np.arange(1,n_stimuli+1) # all of my image starts with #1\n",
    "\n",
    "    timebase = np.arange(start_time, stop_time, timebin)\n",
    "    PSTH = np.full((len(image_numbers), max_number_of_reps, len(timebase),spikeTimes.shape[0]), np.nan)\n",
    "\n",
    "    for num in range(spikeTimes.shape[0]):\n",
    "        spikeTime = np.asanyarray(spikeTimes[num])\n",
    "        osamp = 10\n",
    "        psth_bin = np.zeros((len(samp_on_ms), osamp*(stop_time-start_time)))\n",
    "        psth_matrix = np.full((len(samp_on_ms), len(timebase)), np.nan)\n",
    "\n",
    "        for i in range(len(samp_on_ms)):\n",
    "\n",
    "            sidx = np.floor(osamp*(spikeTime[(spikeTime>=(samp_on_ms[i]+start_time))*(spikeTime<(samp_on_ms[i]+stop_time))]-(samp_on_ms[i]+start_time))).astype(int)\n",
    "            psth_bin[i, sidx] = 1\n",
    "            psth_matrix[i] = np.sum(np.reshape(psth_bin[i], [len(timebase), osamp*timebin]), axis=1)\n",
    "        \n",
    "        \n",
    "        psth = np.full((len(image_numbers), max_number_of_reps, len(timebase)), np.nan)  # Re-ordered PSTH\n",
    "\n",
    "        for i, image_num in enumerate(image_numbers):\n",
    "            index_in_table = np.where(mwk_data.stimulus_presented == image_num)[0]\n",
    "            selected_cells = psth_matrix[index_in_table, :]\n",
    "            psth[i, :selected_cells.shape[0], :] = selected_cells\n",
    "\n",
    "        logging.info(psth.shape)\n",
    "        # Save psth data\n",
    "        PSTH[:,:,:,num] = psth\n",
    "        \n",
    "    meta = {'start_time_ms': start_time, 'stop_time_ms': stop_time, 'tb_ms': timebin}\n",
    "    cmbined_psth = {'psth': PSTH, 'meta': meta}\n",
    "\n",
    "    return cmbined_psth\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory   = '/braintree/home/aliya277/dandi_brainscore/inventory'\n",
    "start_time  = 0\n",
    "stop_time   = 300\n",
    "timebin     = 10\n",
    "counter = 0\n",
    "\n",
    "\n",
    "for folder, num_file in tqdm(zip(os.listdir(inventory), range(len(os.listdir(inventory)))), \\\n",
    "    total = len(os.listdir(inventory)), desc='Processing ...'):\n",
    "\n",
    "\n",
    "    path = os.path.join(inventory, folder)\n",
    "    SessionName = folder\n",
    "    config_path = '/om/user/aliya277/dandi_brainscore'\n",
    "    with open(os.path.join(config_path,\"config_nwb.yaml\") , \"r\") as f:\n",
    "            config = yaml.load(f, Loader = yaml.FullLoader)\n",
    "\n",
    "    print('Creating NWB file...')\n",
    "    nwbfile = create_nwb(config,path)\n",
    "    \n",
    "\n",
    "    if 'h5Files' in os.listdir(path):\n",
    "        print('Opening PSTH...')\n",
    "        filename = os.listdir(os.path.join(path, 'h5Files'))[0]\n",
    "        file = h5py.File(os.path.join(path, 'h5Files', filename),'r+') \n",
    "        data = file['psth'][:]\n",
    "        file.close()\n",
    "\n",
    "\n",
    "        print('Adding PSTH...')\n",
    "        nwbfile.add_scratch(\n",
    "            data,\n",
    "            name=\"psth\",\n",
    "            description=\"psth, uncorrected [channels x stimuli x reps x timebins]\",\n",
    "            )\n",
    "        \n",
    "    else: counter +=1\n",
    "    \n",
    "    try: io.close()\n",
    "    except:pass\n",
    "\n",
    "    if os.path.isfile(os.path.join(path, f\"{SessionName}.nwb\")):\n",
    "         os.remove(os.path.join(path, f\"{SessionName}.nwb\"))\n",
    "\n",
    "    print('Saving NWB File..')\n",
    "    io = NWBHDF5IO(os.path.join(path, f\"{SessionName}.nwb\"), \"w\") \n",
    "    io.write(nwbfile)\n",
    "    io.close()\n",
    "    print(\"File saved.\")\n",
    "\n",
    "        # psth    = get_psth_from_nwb(nwbfile, path, start_time, stop_time, timebin)\n",
    "        # data    = psth['psth']\n",
    "\n",
    "print(f'{counter} SpikeTimes do not have an h5 file.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_names(filename):\n",
    "    assignment  = filename.split('.')[0].split('-')[1]\n",
    "    number      = filename.split('.')[0].split('-')[2]\n",
    "    return np.asarray([assignment, number])\n",
    "\n",
    "def create_nwb(config, path):\n",
    "\n",
    "    desired_timezone = pytz.timezone('US/Eastern')\n",
    "\n",
    "    ################ CREATE NWB FILE WITH METADATA ################################\n",
    "    ###############################################################################\n",
    "    nwbfile = NWBFile(\n",
    "        session_description     = config['session_info']['session_description'],\n",
    "        identifier              = config['metadata']['identifier'],\n",
    "        session_start_time      = desired_timezone.localize(config['metadata']['session_start_time']),\n",
    "        file_create_date        = desired_timezone.localize(config['metadata']['file_create_date']),\n",
    "        experimenter            = config['general']['lab_info']['experimenter'],\n",
    "        experiment_description  = config['general']['experiment_info']['experiment_description'],\n",
    "        session_id              = config['session_info']['session_id'],\n",
    "        lab                     = config['general']['lab_info']['lab'],                     \n",
    "        institution             = config['general']['lab_info']['institution'],                                    \n",
    "        keywords                = config['general']['experiment_info']['keywords'],\n",
    "        surgery                 = config['general']['experiment_info']['surgery']\n",
    "    )\n",
    "\n",
    "    ################ CREATE SUBJECT ################################################\n",
    "    ################################################################################\n",
    "    nwbfile.subject = Subject(\n",
    "        subject_id  = config['subject']['subject_id'],\n",
    "        date_of_birth= config['subject']['date_of_birth'],\n",
    "        species     = config['subject']['species'],\n",
    "        sex         = config['subject']['sex'],\n",
    "    )\n",
    "\n",
    "    ################ CREATE HARDWARE LINKS #########################################\n",
    "    ################################################################################\n",
    "    nwbfile.create_device(\n",
    "        name        = config['hardware']['system_name'], \n",
    "        description = config['hardware']['system_description'], \n",
    "        manufacturer= config['hardware']['system_manuf']\n",
    "    )\n",
    "\n",
    "    nwbfile.create_device(\n",
    "        name        = config['hardware']['adapter_manuf'], \n",
    "        description = config['hardware']['adapter_description'], \n",
    "        manufacturer= config['hardware']['adapter_manuf']\n",
    "    )\n",
    "\n",
    "    nwbfile.create_device(\n",
    "        name        = config['hardware']['monitor_name'], \n",
    "        description = config['hardware']['monitor_description'], \n",
    "        manufacturer= config['hardware']['monitor_manuf']\n",
    "    )\n",
    "\n",
    "    nwbfile.create_device(\n",
    "        name        = config['hardware']['photodiode_name'], \n",
    "        description = config['hardware']['photodiode_description'], \n",
    "        manufacturer= config['hardware']['photodiode_manuf']\n",
    "    )\n",
    "    \n",
    "    nwbfile.create_device(\n",
    "        name        = 'Software Used', \n",
    "        description = str(['Mworks Client: '+config['software']['mwclient_version'],\\\n",
    "                        'Mworks Server: '+config['software']['mwserver_version'],\\\n",
    "                        'OS: '+config['software']['OS'],\\\n",
    "                        'Intan :'+config['software']['intan_version']])\n",
    "    )\n",
    "\n",
    "    ################ CREATE ELECTRODE LINKS ########################################\n",
    "    ################################################################################\n",
    "    electrodes = nwbfile.create_device(\n",
    "        name        = config['hardware']['electrode_name'], \n",
    "        description = config['hardware']['electrode_description'], \n",
    "        manufacturer= config['hardware']['electrode_manuf']\n",
    "    )\n",
    "\n",
    "    all_files = sorted(os.listdir(os.path.join(path, 'SpikeTimes')))\n",
    "    \n",
    "    name_accumulator = []\n",
    "    for file in all_files:\n",
    "        name_accumulator.append(read_names(file))\n",
    "    names = np.vstack(name_accumulator)\n",
    "\n",
    "    nwbfile.add_electrode_column(name=\"label\", description=\"label of electrode\")\n",
    "    groups, count_groups = np.unique(names[:,0], return_counts =True)\n",
    "    ids                  = names[:,1]\n",
    "    counter              = 0\n",
    "    # create ElectrodeGroups A, B, C, ..\n",
    "    for group, count_group in zip(groups, count_groups):\n",
    "        if len(groups) == 6:\n",
    "            electrode_description = \"Serialnumber: {}. Adapter Version: {}\".format(config['array_info']['array_{}'.format(group)]['serialnumber'],\\\n",
    "                            config['array_info']['array_{}'.format(group)]['adapterversion']),\n",
    "        else: \n",
    "            electrode_description = \"Serialnumber: {}\".format(config['array_info']['array_{}'.format(group)]['serialnumber']),\n",
    "                \n",
    "        \n",
    "        electrode_group = nwbfile.create_electrode_group(\n",
    "            name        = \"group_{}\".format(group),\n",
    "            description = electrode_description[0],\n",
    "            device      = electrodes,\n",
    "            location    = 'hemisphere, region, subregion: '+str([config['array_info']['array_{}'.format(group)]['hemisphere'],\\\n",
    "                                config['array_info']['array_{}'.format(group)]['region'],\n",
    "                                config['array_info']['array_{}'.format(group)]['subregion']]),\n",
    "            position    = config['array_info']['array_{}'.format(group)]['position']\n",
    "        )\n",
    "\n",
    "        # create Electrodes 001, 002, ..., 032 in ElectrodeGroups per channel\n",
    "        for ichannel in range(count_group):\n",
    "            nwbfile.add_electrode(\n",
    "                group       = electrode_group,\n",
    "                label       = ids[counter],\n",
    "                location    = 'row, col, elec'+str(json.loads(config['array_info']['intan_electrode_labeling_[row,col,id]'])[counter])\n",
    "            )\n",
    "            counter += 1     \n",
    "\n",
    "    ################ ADD SPIKE TIMES ###############################################\n",
    "    ################################################################################\n",
    "\n",
    "    nwbfile.add_unit_column(name=\"unit\", description=\"millisecond\") \n",
    "    for filename, i in zip(sorted(os.listdir(os.path.join(path, 'SpikeTimes'))), range(len(os.listdir(os.path.join(path, 'SpikeTimes'))))):\n",
    "        [assignment, number] = read_names(filename)\n",
    "        file_path = os.path.join(path, 'SpikeTimes', filename)\n",
    "        data = scipy.io.loadmat(file_path, squeeze_me=True,\n",
    "                        variable_names='spike_time_ms')['spike_time_ms']\n",
    "        nwbfile.add_unit(\n",
    "            spike_times = data, \n",
    "            electrodes  = [i],\n",
    "            electrode_group = nwbfile.electrode_groups[f'group_{assignment}'], \n",
    "            unit = 'ms'\n",
    "        )\n",
    "\n",
    "    ################ ADD TRIAL TIMES ###############################################\n",
    "    ################################################################################\n",
    "    last_spike = data[-1]\n",
    "    del data\n",
    "    \n",
    "    try: \n",
    "        [on, off] = config['session_info']['session_description'].split(', ')[3].split(': ')[-1].split(\"/\")\n",
    "        on_start  = 0\n",
    "        on_dur    = int(on)\n",
    "        off_dur   = int(off)\n",
    "\n",
    "        \n",
    "        nwbfile.add_trial_column(name=\"unit\", description=\"millisecond\")\n",
    "        while on_start < last_spike:\n",
    "\n",
    "            nwbfile.add_trial(\n",
    "                start_time= float(on_start),\n",
    "                stop_time = float(on_start+on_dur),\n",
    "                unit = 'ms')\n",
    "        \n",
    "            on_start += on_dur+off_dur\n",
    "    except: pass \n",
    "\n",
    "    ################ ADD PSTH IF AVAIL #############################################\n",
    "    ################################################################################\n",
    "    if 'h5Files' in os.listdir(path):\n",
    "\n",
    "        filename = os.listdir(os.path.join(path, 'h5Files'))[0]\n",
    "        file = h5py.File(os.path.join(path, 'h5Files', filename),'r+') \n",
    "        data = file['psth'][:]\n",
    "        file.close()\n",
    "\n",
    "        nwbfile.add_scratch(\n",
    "            data,\n",
    "            name=\"psth\",\n",
    "            description=\"psth, uncorrected [stimuli x reps x timebins x channels]\",\n",
    "            )\n",
    "        \n",
    "\n",
    "    return nwbfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file did not work: norm_FOSS.sub_pico.20230823_124104.proc\n",
    "tail: output.log: Datei abgeschnitten\n",
    "This file did not work: norm_FOSS.sub_pico.20230803_105856.proc\n",
    "tail: output.log: Datei abgeschnitten\n",
    "This file did not work: exp_gratingsAdap_s3.sub_pico.20230801_163355.proc\n",
    "tail: output.log: Datei abgeschnitten\n",
    "tail: output.log: Datei abgeschnitten\n",
    "This file did not work: norm_FOSS.sub_pico.20230628_124854.proc\n",
    "tail: output.log: Datei abgeschnitten\n",
    "node069\n",
    "tail: output.log: Datei abgeschnitten\n",
    "tail: output.log: Datei abgeschnitten\n",
    "node069\n",
    "This file did not work: norm_FOSS.sub_pico.20220929_170635.proc\n",
    "5127.proc\n",
    "tail: output.log: Datei abgeschnitten\n",
    "tail: output.log: Datei abgeschnitten\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "OSS.sub_pico.20230125_144402.proc\n",
    "tail: output.log: Datei abgeschnitten\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "This file did not work: norm_FOSS.sub_pico.20230127_160227.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230126_150653.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230505_130540.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230531_134209.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230516_120137.proc\n",
    "This file did not work: exp_robustness_guy_d1_v34.sub_pico.20230906_122650.proc\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "This file did not work: norm_FOSS.sub_pico.20230510_111142.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230428_111937.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20220902_145351.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20220615_113442.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20220907_142157.proc\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "This file did not work: norm_FOSS.sub_pico.20230713_141950.proc\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "This file did not work: norm_FOSS.sub_pico.20230817_141050.proc\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "This file did not work: norm_FOSS.sub_pico.20230328_145456.proc\n",
    "This file did not work: exp_gratingsAdap_s1.sub_pico.20230801_151117.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230606_134244.proc\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "This file did not work: norm_FOSS.sub_pico.20230621_140747.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230814_122811.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230821_120107.proc\n",
    "This file did not work: exp_Mayo_day_5.sub_pico.20230818_101245.proc\n",
    "This file did not work: norm_HVM.sub_pico.20230404_142414.proc\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "This file did not work: norm_FOSS.sub_pico.20230626_131126.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230629_132813.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230711_142158.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230630_135832.proc\n",
    "This file did not work: norm_FOSS.sub_pico.20230725_113504.proc\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "Saving NWB File..\n",
    "File saved.\n",
    "This file did not work: norm_FOSS.sub_pico.20230830_110931.proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = 0\n",
    "stop_time = 300\n",
    "timebin = 10\n",
    "\n",
    "\n",
    "def get_psth_from_nwb(nwbfile, path, start_time, stop_time, timebin, n_stimuli=None):\n",
    "\n",
    "    # Find the MWORKS File\n",
    "    with open(os.path.join(path, 'NWBInfo.txt')) as f:\n",
    "        lines = f.readlines()\n",
    "        line2 = lines[0].split(',')[1]\n",
    "        ind = line2.split('/').index('intanproc')\n",
    "        mwk_file = os.path.join('/', *line2.split('/')[0:ind], 'mworksproc', line2.split('/')[ind+1]+'_mwk.csv')\n",
    "    assert os.path.isfile(mwk_file)==True\n",
    "\n",
    "    ################ MODIFIED FROM THE SPIKE-TOOLS-CHONG CODE ######################\n",
    "    ################################################################################\n",
    "    \n",
    "    mwk_data = pd.read_csv(mwk_file)\n",
    "    mwk_data = mwk_data[mwk_data.fixation_correct == 1]\n",
    "    if 'photodiode_on_us' in mwk_data.keys():\n",
    "        samp_on_ms = np.asarray(mwk_data['photodiode_on_us']) / 1000.\n",
    "        logging.info('Using photodiode signal for sample on time')\n",
    "    else:\n",
    "        samp_on_ms = np.asarray(mwk_data['samp_on_us']) / 1000.\n",
    "        logging.info('Using MWorks digital signal for sample on time')\n",
    "    \n",
    "    # Load spikeTime file for current channel\n",
    "    spikeTimes = nwbfile.units[:].spike_times\n",
    "    # Re-order the psth to image x reps\n",
    "    max_number_of_reps = max(np.bincount(mwk_data['stimulus_presented']))  # Max reps obtained for any image\n",
    "    if max_number_of_reps == 0:\n",
    "        exit()\n",
    "    mwk_data['stimulus_presented'] = mwk_data['stimulus_presented'].astype(int)  # To avoid indexing errors\n",
    "    \n",
    "    if n_stimuli is None:\n",
    "            image_numbers = np.unique(mwk_data['stimulus_presented'])  # TODO: if not all images are shown (for eg, exp cut short), you'll have to manually type in total # images\n",
    "    else:\n",
    "        image_numbers = np.arange(1,n_stimuli+1) # all of my image starts with #1\n",
    "\n",
    "    timebase = np.arange(start_time, stop_time, timebin)\n",
    "    PSTH = np.full((len(image_numbers), max_number_of_reps, len(timebase),spikeTimes.shape[0]), np.nan)\n",
    "\n",
    "    for num in range(spikeTimes.shape[0]):\n",
    "        spikeTime = np.asanyarray(spikeTimes[num])\n",
    "        osamp = 10\n",
    "        psth_bin = np.zeros((len(samp_on_ms), osamp*(stop_time-start_time)))\n",
    "        psth_matrix = np.full((len(samp_on_ms), len(timebase)), np.nan)\n",
    "\n",
    "        for i in range(len(samp_on_ms)):\n",
    "\n",
    "            sidx = np.floor(osamp*(spikeTime[(spikeTime>=(samp_on_ms[i]+start_time))*(spikeTime<(samp_on_ms[i]+stop_time))]-(samp_on_ms[i]+start_time))).astype(int)\n",
    "            psth_bin[i, sidx] = 1\n",
    "            psth_matrix[i] = np.sum(np.reshape(psth_bin[i], [len(timebase), osamp*timebin]), axis=1)\n",
    "        \n",
    "        \n",
    "        psth = np.full((len(image_numbers), max_number_of_reps, len(timebase)), np.nan)  # Re-ordered PSTH\n",
    "\n",
    "        for i, image_num in enumerate(image_numbers):\n",
    "            index_in_table = np.where(mwk_data.stimulus_presented == image_num)[0]\n",
    "            selected_cells = psth_matrix[index_in_table, :]\n",
    "            psth[i, :selected_cells.shape[0], :] = selected_cells\n",
    "\n",
    "        logging.info(psth.shape)\n",
    "        # Save psth data\n",
    "        PSTH[:,:,:,num] = psth\n",
    "        \n",
    "    meta = {'start_time_ms': start_time, 'stop_time_ms': stop_time, 'tb_ms': timebin}\n",
    "    cmbined_psth = {'psth': PSTH, 'meta': meta}\n",
    "\n",
    "    return cmbined_psth\n",
    "    \n",
    "# psth = get_psth_from_nwb(nwbfile, path, start_time, stop_time, timebin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_nwb(nwbfile, path):\n",
    "    \n",
    "    SessionName = path.split('/')[-1]\n",
    "\n",
    "    # Crete Temporary path on BrainTree (Openmind Did not Allow Saving of H5 Files.)\n",
    "    temp_path = '/braintree/home/aliya277/dandi_brainscore/inventory'\n",
    "    try: os.mkdir(temp_path)\n",
    "    except: pass\n",
    "\n",
    "    # Save NWB File on Braintree\n",
    "    io = NWBHDF5IO(os.path.join(temp_path, f\"{SessionName}.nwb\"), \"w\") \n",
    "    io.write(nwbfile)\n",
    "    io.close()\n",
    "\n",
    "    # Copy NWB to Inventory Directory on Openmind\n",
    "    shutil.copy2(os.path.join(temp_path, f\"{SessionName}.nwb\"), os.path.join(path, f\"{SessionName}.nwb\"))   \n",
    "    \n",
    "    # Remove Temporary File and Folder on Braintree\n",
    "    os.remove(os.path.join(temp_path, f\"{SessionName}.nwb\"))\n",
    "    os.rmdir(os.path.join(temp_path))\n",
    "\n",
    "    return nwbfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'psth' already exists in NWBFile 'root'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/om/user/aliya277/dandi_brainscore/create_proc_nwb.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bopenmind.mit.edu/om/user/aliya277/dandi_brainscore/create_proc_nwb.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m data \u001b[39m=\u001b[39m psth[\u001b[39m'\u001b[39m\u001b[39mpsth\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bopenmind.mit.edu/om/user/aliya277/dandi_brainscore/create_proc_nwb.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m nwbfile\u001b[39m.\u001b[39;49madd_scratch(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bopenmind.mit.edu/om/user/aliya277/dandi_brainscore/create_proc_nwb.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     data,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bopenmind.mit.edu/om/user/aliya277/dandi_brainscore/create_proc_nwb.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpsth\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bopenmind.mit.edu/om/user/aliya277/dandi_brainscore/create_proc_nwb.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpsth, uncorrected [channels x stimuli x reps x timebins]\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bopenmind.mit.edu/om/user/aliya277/dandi_brainscore/create_proc_nwb.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.10/site-packages/hdmf/utils.py:644\u001b[0m, in \u001b[0;36mdocval.<locals>.dec.<locals>.func_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfunc_call\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    643\u001b[0m     pargs \u001b[39m=\u001b[39m _check_args(args, kwargs)\n\u001b[0;32m--> 644\u001b[0m     \u001b[39mreturn\u001b[39;00m func(args[\u001b[39m0\u001b[39;49m], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.10/site-packages/pynwb/file.py:1097\u001b[0m, in \u001b[0;36mNWBFile.add_scratch\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m     \u001b[39mif\u001b[39;00m description \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1095\u001b[0m         warn(\u001b[39m'\u001b[39m\u001b[39mThe description argument is ignored when adding an NWBContainer, ScratchData, or \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1096\u001b[0m              \u001b[39m'\u001b[39m\u001b[39mDynamicTable to scratch.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1097\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_scratch(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.10/site-packages/hdmf/utils.py:644\u001b[0m, in \u001b[0;36mdocval.<locals>.dec.<locals>.func_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfunc_call\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    643\u001b[0m     pargs \u001b[39m=\u001b[39m _check_args(args, kwargs)\n\u001b[0;32m--> 644\u001b[0m     \u001b[39mreturn\u001b[39;00m func(args[\u001b[39m0\u001b[39;49m], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.10/site-packages/hdmf/container.py:1031\u001b[0m, in \u001b[0;36mMultiContainerInterface.__make_add.<locals>._func\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1029\u001b[0m     \u001b[39mif\u001b[39;00m tmp\u001b[39m.\u001b[39mname \u001b[39min\u001b[39;00m d:\n\u001b[1;32m   1030\u001b[0m         msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m already exists in \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (tmp\u001b[39m.\u001b[39mname, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[0;32m-> 1031\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1032\u001b[0m     d[tmp\u001b[39m.\u001b[39mname] \u001b[39m=\u001b[39m tmp\n\u001b[1;32m   1033\u001b[0m \u001b[39mreturn\u001b[39;00m container\n",
      "\u001b[0;31mValueError\u001b[0m: 'psth' already exists in NWBFile 'root'"
     ]
    }
   ],
   "source": [
    "data = psth['psth']\n",
    "\n",
    "nwbfile.add_scratch(\n",
    "    data,\n",
    "    name=\"psth\",\n",
    "    description=\"psth, uncorrected [channels x stimuli x reps x timebins]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 86, 21, 30)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwbfile.scratch['psth'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5path = '/om/user/aliya277/inventory/norm_FOSS.sub_pico.20230823_124104.proc/h5Files/230823.pico.rsvp.normalizers.experiment_psth_raw.h5'\n",
    "\n",
    "def open_h5(path):\n",
    "    \n",
    "    filename = path.split('/')[-1]\n",
    "    # Crete Temporary path on BrainTree (Openmind Did not Allow Saving of H5 Files.)\n",
    "    temp_path = '/braintree/home/aliya277/dandi_brainscore/inventory'\n",
    "    try: os.mkdir(temp_path)\n",
    "    except: pass\n",
    "\n",
    "\n",
    "    # Copy File to Temporary Path\n",
    "    shutil.copy2(os.path.join(path), os.path.join(temp_path, filename))   \n",
    "\n",
    "    file = h5py.File(os.path.join(temp_path, filename),'r+')    \n",
    "\n",
    "    return file\n",
    "\n",
    "def close_h5(path):\n",
    "\n",
    "    filename = path.split('/')[-1]\n",
    "    temp_path = '/braintree/home/aliya277/dandi_brainscore/inventory'\n",
    "    os.remove(os.path.join(temp_path, filename))\n",
    "    os.rmdir(temp_path)\n",
    "\n",
    "file = open_h5(h5path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(psth['psth'], file['psth'][:], equal_nan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "0      [69.2, 135.55, 207.55, 242.9, 281.95, 331.5, 3...\n",
       "1      [22.1, 57.3, 63.9, 135.35, 167.1, 175.6, 225.8...\n",
       "2      [58.1, 69.15, 167.7, 171.1, 171.3, 286.9, 362....\n",
       "3      [16.9, 158.04999999999998, 237.9, 304.09999999...\n",
       "4      [49.75, 68.9, 87.65, 115.1, 342.95, 412.650000...\n",
       "                             ...                        \n",
       "187    [2.6, 5.3, 81.3, 155.35, 185.35, 251.649999999...\n",
       "188    [35.2, 164.14999999999998, 164.70000000000002,...\n",
       "189    [35.2, 155.35, 251.8, 286.95, 287.1, 287.4, 29...\n",
       "190    [2.6, 162.35, 198.8, 251.5, 251.7, 287.2, 294....\n",
       "191    [46.449999999999996, 73.85, 81.44999999999999,...\n",
       "Name: spike_times, Length: 192, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwbfile.units[:].spike_times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547d15e3cf1a43ceb91679bc5cf39827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='session_description:', layout=Layout(max_height='40px', max_width='…"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwb2widget(nwbfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dandibs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
