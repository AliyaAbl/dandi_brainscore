{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load modules and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, sys\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from brainio.assemblies import NeuronRecordingAssembly\n",
    "from pynwb import NWBHDF5IO, NWBFile\n",
    "import glob, os\n",
    "from datetime import datetime\n",
    "import pytz  # This is required to handle timezone conversions\n",
    "import sys\n",
    "import io as ioprint\n",
    "\n",
    "cwd = os.getcwd()\n",
    "sys.path.append(os.path.dirname(cwd))\n",
    "\n",
    "from ndashboard.nquality.raw_data_template import SessionNeuralData\n",
    "from ndashboard.nquality.quality_within_session import Session\n",
    "from ndashboard.nquality.quality_across_sessions import LongitudinalQuality\n",
    "\n",
    "nboot=10000\n",
    "\n",
    "def get_unix_timestamp(date_str, time_str, date_format='%Y%m%d', time_format='%H%M%S'):\n",
    "    # Combine date and time strings\n",
    "    datetime_str = f\"{date_str} {time_str}\"\n",
    "    \n",
    "    # Parse the datetime string into a datetime object\n",
    "    dt = datetime.strptime(datetime_str, f\"{date_format} {time_format}\")\n",
    "    \n",
    "    # Assuming the provided time is in UTC\n",
    "    # If it's in another timezone, you can adjust it accordingly using pytz\n",
    "    dt = pytz.utc.localize(dt)\n",
    "    \n",
    "    # Get the Unix timestamp\n",
    "    unix_timestamp = (dt.timestamp())\n",
    "    \n",
    "    return unix_timestamp\n",
    "\n",
    "def generate_timestamps(start_timestamp, interval_ms, length):\n",
    "    # Create an array of increments (100ms steps)\n",
    "    increments = np.arange(0, length * interval_ms / 1000, interval_ms / 1000)\n",
    "    \n",
    "    # Add the increments to the start timestamp\n",
    "    timestamps = start_timestamp + increments\n",
    "    \n",
    "    return timestamps\n",
    "\n",
    "def create_norm_assembly(psth, meta, start_timestemp = None):\n",
    "    timebase = np.arange(meta[0], meta[1], meta[2])\n",
    "    timebins = np.asarray([[int(x), int(x)+int(meta[2])] for x in timebase])\n",
    "    assert len(timebase) == psth.shape[2], f\"Number of bins is not correct. Expected {len(timebase)} got {psth.shape[2]}\"\n",
    "    \n",
    "    \n",
    "    assembly = xr.DataArray(psth,\n",
    "                    coords={'repetition': ('repetition', list(range(psth.shape[1]))),\n",
    "                            'stimulus_id': ('image', list(range(psth.shape[0]))),\n",
    "                            'time_bin_id': ('time_bin', list(range(psth.shape[2]))),\n",
    "                            'time_bin_start': ('time_bin', [x[0] for x in timebins]),\n",
    "                            'time_bin_stop': ('time_bin', [x[1] for x in timebins])},\n",
    "                    dims=['image', 'repetition', 'time_bin', 'neuroid'])\n",
    "\n",
    "    assembly = assembly.stack(presentation=('image', 'repetition')).reset_index('presentation')\n",
    "    assembly = assembly.drop('image')\n",
    "    assembly = assembly.isel(time_bin = slice(int(0-(meta[0]/meta[2])+(70/meta[2])), int(0-(meta[0]/meta[2])+(170/meta[2])))).sum('time_bin').transpose('presentation', 'neuroid')\n",
    "    \n",
    "    if start_timestemp == None: \n",
    "        assembly = assembly.assign_coords({'unix_timestamp': ('presentation', np.linspace(0, 100, assembly.shape[0]))})\n",
    "    else: \n",
    "        timestamps = generate_timestamps(start_timestemp, interval_ms=100, length=assembly.shape[0])\n",
    "        assembly   = assembly.assign_coords({'unix_timestamp': ('presentation', timestamps)})\n",
    "    numchannels = assembly.shape[1]\n",
    "\n",
    "    return assembly, numchannels\n",
    "\n",
    "global no_normalizer_days\n",
    "no_normalizer_days = []\n",
    "\n",
    "\n",
    "# def find_norm_with_date(realdate):\n",
    "    \n",
    "#     normalizer_file_paths = glob.glob(os.path.join(root_dir, '[norm]*', '*', '*', '[!h5]*'))\n",
    "#     for norm_file_path in normalizer_file_paths:\n",
    "#         date, time = os.path.basename(norm_file_path).split('.')[-2].split('_')\n",
    "#         if date == realdate: \n",
    "#             return norm_file_path\n",
    "    \n",
    "#     no_normalizer_days.append(realdate)\n",
    "#     return None\n",
    "\n",
    "def find_norm_with_date(realdate):\n",
    "    all_paths = []\n",
    "    normalizer_file_paths = glob.glob(os.path.join(root_dir, '[norm]*', '*', '*', '[!h5]*'))\n",
    "    for norm_file_path in normalizer_file_paths:\n",
    "        date, time = os.path.basename(norm_file_path).split('.')[-2].split('_')\n",
    "        if date == realdate: \n",
    "            all_paths.append(norm_file_path)\n",
    "\n",
    "    if len(all_paths) == 0: return [None]   \n",
    "    \n",
    "    return all_paths\n",
    "\n",
    "def get_list_of_days(days):\n",
    "    list_days = []\n",
    "    for day in days:\n",
    "        list_days.append(day.split('.')[-1])\n",
    "    return list_days\n",
    "\n",
    "def quality_within_session(day):\n",
    "    \n",
    "    norm_paths = find_norm_with_date(day)\n",
    "    for norm_path in norm_paths:\n",
    "        if norm_path != None:\n",
    "            norm_nwb_file_path = glob.glob(os.path.join(norm_path, '*[nwb]'))[0]\n",
    "            \n",
    "            print(f\"Within: using normalizer file {os.path.basename(norm_nwb_file_path)}\")\n",
    "            io = NWBHDF5IO(norm_nwb_file_path, \"r\") \n",
    "            norm_nwbfile = io.read()\n",
    "            try: \n",
    "                psth = norm_nwbfile.scratch['psth'][:]\n",
    "                meta = norm_nwbfile.scratch['psth meta'][:]\n",
    "                io.close()\n",
    "                da, nc  = create_norm_assembly(psth, meta)\n",
    "                session = SessionNeuralData(da)\n",
    "                session = Session(session, boot_seed=0, nboot=nboot)\n",
    "                ds_quality = session.ds_quality\n",
    "                pvalues = ds_quality['pvalue_signal_variance'].data\n",
    "                return pvalues\n",
    "            except: \n",
    "                print(f'No psth available for normalizer {day}.')\n",
    "                return [None]\n",
    "        else: \n",
    "            print(f'No normalizer found for day {day}.')\n",
    "            return [None]\n",
    "\n",
    "def quality_across_sessions(first_day, comparing_day):\n",
    "    \n",
    "    norm_paths_1 = find_norm_with_date(first_day)\n",
    "    norm_paths_2 = find_norm_with_date(comparing_day)\n",
    "\n",
    "    if len(norm_paths_2) == 2: norm_paths_1.append(norm_paths_1[0])\n",
    "\n",
    "    for norm_path_1, norm_path_2 in zip(norm_paths_1, norm_paths_2):\n",
    "        if norm_path_1 != None and norm_path_2 != None:\n",
    "\n",
    "            norm_nwb_file_path_1 = glob.glob(os.path.join(norm_path_1, '*[nwb]'))[0]\n",
    "            norm_nwb_file_path_2 = glob.glob(os.path.join(norm_path_2, '*[nwb]'))[0]\n",
    "\n",
    "            print(f\"Across: using normalizer file {os.path.basename(norm_nwb_file_path_1)} and {os.path.basename(norm_nwb_file_path_2)} \")\n",
    "            try: \n",
    "                io = NWBHDF5IO(norm_nwb_file_path_1, \"r\") \n",
    "                norm_nwbfile = io.read()\n",
    "                psth = norm_nwbfile.scratch['psth'][:]\n",
    "                meta = norm_nwbfile.scratch['psth meta'][:]\n",
    "                io.close()\n",
    "                n_channel_1 = psth.shape[-1]\n",
    "                da, nc  = create_norm_assembly(psth, meta)\n",
    "                session_1 = SessionNeuralData(da)\n",
    "\n",
    "                io = NWBHDF5IO(norm_nwb_file_path_2, \"r\") \n",
    "                norm_nwbfile = io.read()\n",
    "                psth = norm_nwbfile.scratch['psth'][:]\n",
    "                meta = norm_nwbfile.scratch['psth meta'][:]\n",
    "                io.close()\n",
    "                n_channel_2 = psth.shape[-1]\n",
    "                da, nc  = create_norm_assembly(psth, meta)\n",
    "                session_2 = SessionNeuralData(da)\n",
    "\n",
    "                if n_channel_1 == n_channel_2:\n",
    "\n",
    "                    session = LongitudinalQuality([session_1, session_2], boot_seed=0, nboot=nboot)\n",
    "                    ds_quality = session.ds_quality\n",
    "                    pvalues = ds_quality['pvalue_signal_variance'].data\n",
    "                    return pvalues\n",
    "                else: \n",
    "                    print(f\"Number of channels do not match. {n_channel_1} != {n_channel_2} \")\n",
    "                    #continue\n",
    "            except Exception as error:\n",
    "                print(error)\n",
    "                print(f'No psth available for normalizers {first_day, comparing_day}.')\n",
    "                return [None, None]\n",
    "            \n",
    "        else: \n",
    "            print(f'No normalizer found for day {first_day, comparing_day}.')\n",
    "            return [None, None]\n",
    "\n",
    "def update_sheet(df, exp_nwb_path, text):\n",
    "    imageset = os.path.basename(exp_nwb_path).split('.')[0].split('_')[1:]\n",
    "    if len(imageset) == 1: imageset = imageset[0]\n",
    "    elif len(imageset) > 1: imageset = '_'.join(imageset)\n",
    "    mask = df['ImageSet'] == imageset\n",
    "    index = df.index[mask].tolist()[0]\n",
    "    df.at[index, 'proc_nwb'] = text\n",
    "    # display(df.iloc[index])\n",
    "        \n",
    "        \n",
    "root_dir        = '/braintree/home/aliya277/inventory_new'\n",
    "\n",
    "df = pd.read_excel( os.path.dirname(cwd)+'/pico_inventory.xlsx' , sheet_name='Sheet2')\n",
    "df['proc_nwb'] = 'No inventory created yet.'\n",
    "# list of experiments sarah marked as 'not going on brainscore'\n",
    "list_exp_not_using = [row['ImageSet'] for index, row in df.iterrows() if row['BrainScore'] != 'Y']\n",
    "\n",
    "\n",
    "SubjectName = 'pico'\n",
    "storage_dir = '/braintree/home/aliya277/inventory_new'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: For each experiment, do a within-session and across-session quality check and save them in the normalizer nwb file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "exp_ko_context_size.sub_pico has 1 days and 1 sessions\n",
      "Checking if P-Values are added to experiment file for day 1\n",
      "________________________________________________________________________________\n",
      "exp_robustness_guy_d1_v40.sub_pico has 1 days and 3 sessions\n",
      "Checking if P-Values are added to experiment file for day 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/braintree/home/aliya277/dandi_brainscore/notebooks/2_quality_check.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbraintree-cpu-1.mit.edu/braintree/home/aliya277/dandi_brainscore/notebooks/2_quality_check.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=102'>103</a>\u001b[0m \u001b[39mfor\u001b[39;00m exp_nwb_path \u001b[39min\u001b[39;00m exp_nwb_paths:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbraintree-cpu-1.mit.edu/braintree/home/aliya277/dandi_brainscore/notebooks/2_quality_check.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=103'>104</a>\u001b[0m     \n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbraintree-cpu-1.mit.edu/braintree/home/aliya277/dandi_brainscore/notebooks/2_quality_check.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=104'>105</a>\u001b[0m     \u001b[39m# print(i_day, exp_nwb_path)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbraintree-cpu-1.mit.edu/braintree/home/aliya277/dandi_brainscore/notebooks/2_quality_check.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=105'>106</a>\u001b[0m     io \u001b[39m=\u001b[39m NWBHDF5IO(exp_nwb_path, \u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m) \n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bbraintree-cpu-1.mit.edu/braintree/home/aliya277/dandi_brainscore/notebooks/2_quality_check.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=106'>107</a>\u001b[0m     exp_nwbfile \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39;49mread()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbraintree-cpu-1.mit.edu/braintree/home/aliya277/dandi_brainscore/notebooks/2_quality_check.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=107'>108</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbraintree-cpu-1.mit.edu/braintree/home/aliya277/dandi_brainscore/notebooks/2_quality_check.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=108'>109</a>\u001b[0m         n_channel \u001b[39m=\u001b[39m exp_nwbfile\u001b[39m.\u001b[39mscratch[\u001b[39m'\u001b[39m\u001b[39mpsth\u001b[39m\u001b[39m'\u001b[39m][:]\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/hdmf/utils.py:644\u001b[0m, in \u001b[0;36mdocval.<locals>.dec.<locals>.func_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfunc_call\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    643\u001b[0m     pargs \u001b[39m=\u001b[39m _check_args(args, kwargs)\n\u001b[0;32m--> 644\u001b[0m     \u001b[39mreturn\u001b[39;00m func(args[\u001b[39m0\u001b[39;49m], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/pynwb/__init__.py:304\u001b[0m, in \u001b[0;36mNWBHDF5IO.read\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNWB version \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m not supported. PyNWB supports NWB files version 2 and above.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m    302\u001b[0m                         \u001b[39mstr\u001b[39m(file_version_str))\n\u001b[1;32m    303\u001b[0m \u001b[39m# read the file\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m file \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mread(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    305\u001b[0m \u001b[39mreturn\u001b[39;00m file\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/hdmf/backends/hdf5/h5tools.py:479\u001b[0m, in \u001b[0;36mHDF5IO.read\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[39mraise\u001b[39;00m UnsupportedOperation(\u001b[39m\"\u001b[39m\u001b[39mCannot read from file \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m in mode \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. Please use mode \u001b[39m\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, or \u001b[39m\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    477\u001b[0m                                \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msource, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__mode))\n\u001b[1;32m    478\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 479\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mread(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    480\u001b[0m \u001b[39mexcept\u001b[39;00m UnsupportedOperation \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    481\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mstr\u001b[39m(e) \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mCannot build data. There are no values.\u001b[39m\u001b[39m'\u001b[39m:  \u001b[39m# pragma: no cover\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/hdmf/utils.py:644\u001b[0m, in \u001b[0;36mdocval.<locals>.dec.<locals>.func_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfunc_call\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    643\u001b[0m     pargs \u001b[39m=\u001b[39m _check_args(args, kwargs)\n\u001b[0;32m--> 644\u001b[0m     \u001b[39mreturn\u001b[39;00m func(args[\u001b[39m0\u001b[39;49m], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/hdmf/backends/io.py:56\u001b[0m, in \u001b[0;36mHDMFIO.read\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39m@docval\u001b[39m(returns\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mthe Container object that was read in\u001b[39m\u001b[39m'\u001b[39m, rtype\u001b[39m=\u001b[39mContainer)\n\u001b[1;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     55\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Read a container from the IO source.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     f_builder \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_builder()\n\u001b[1;32m     57\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mlen\u001b[39m(v) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m f_builder\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m     58\u001b[0m         \u001b[39m# TODO also check that the keys are appropriate. print a better error message\u001b[39;00m\n\u001b[1;32m     59\u001b[0m         \u001b[39mraise\u001b[39;00m UnsupportedOperation(\u001b[39m'\u001b[39m\u001b[39mCannot build data. There are no values.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/hdmf/utils.py:644\u001b[0m, in \u001b[0;36mdocval.<locals>.dec.<locals>.func_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfunc_call\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    643\u001b[0m     pargs \u001b[39m=\u001b[39m _check_args(args, kwargs)\n\u001b[0;32m--> 644\u001b[0m     \u001b[39mreturn\u001b[39;00m func(args[\u001b[39m0\u001b[39;49m], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/hdmf/backends/hdf5/h5tools.py:503\u001b[0m, in \u001b[0;36mHDF5IO.read_builder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    501\u001b[0m     ignore\u001b[39m.\u001b[39madd(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__file[specloc]\u001b[39m.\u001b[39mname)\n\u001b[1;32m    502\u001b[0m \u001b[39mif\u001b[39;00m f_builder \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 503\u001b[0m     f_builder \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__read_group(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__file, ROOT_NAME, ignore\u001b[39m=\u001b[39;49mignore)\n\u001b[1;32m    504\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__read[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__file] \u001b[39m=\u001b[39m f_builder\n\u001b[1;32m    505\u001b[0m \u001b[39mreturn\u001b[39;00m f_builder\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/hdmf/backends/hdf5/h5tools.py:637\u001b[0m, in \u001b[0;36mHDF5IO.__read_group\u001b[0;34m(self, h5obj, name, ignore)\u001b[0m\n\u001b[1;32m    635\u001b[0m     obj_type \u001b[39m=\u001b[39m kwargs[\u001b[39m'\u001b[39m\u001b[39mgroups\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m builder \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 637\u001b[0m     builder \u001b[39m=\u001b[39m read_method(sub_h5obj)\n\u001b[1;32m    638\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_built(sub_h5obj\u001b[39m.\u001b[39mfile\u001b[39m.\u001b[39mfilename, sub_h5obj\u001b[39m.\u001b[39mid, builder)\n\u001b[1;32m    639\u001b[0m obj_type[builder\u001b[39m.\u001b[39mname] \u001b[39m=\u001b[39m builder\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/hdmf/backends/hdf5/h5tools.py:637\u001b[0m, in \u001b[0;36mHDF5IO.__read_group\u001b[0;34m(self, h5obj, name, ignore)\u001b[0m\n\u001b[1;32m    635\u001b[0m     obj_type \u001b[39m=\u001b[39m kwargs[\u001b[39m'\u001b[39m\u001b[39mgroups\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m builder \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 637\u001b[0m     builder \u001b[39m=\u001b[39m read_method(sub_h5obj)\n\u001b[1;32m    638\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_built(sub_h5obj\u001b[39m.\u001b[39mfile\u001b[39m.\u001b[39mfilename, sub_h5obj\u001b[39m.\u001b[39mid, builder)\n\u001b[1;32m    639\u001b[0m obj_type[builder\u001b[39m.\u001b[39mname] \u001b[39m=\u001b[39m builder\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/hdmf/backends/hdf5/h5tools.py:637\u001b[0m, in \u001b[0;36mHDF5IO.__read_group\u001b[0;34m(self, h5obj, name, ignore)\u001b[0m\n\u001b[1;32m    635\u001b[0m     obj_type \u001b[39m=\u001b[39m kwargs[\u001b[39m'\u001b[39m\u001b[39mgroups\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m builder \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 637\u001b[0m     builder \u001b[39m=\u001b[39m read_method(sub_h5obj)\n\u001b[1;32m    638\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_built(sub_h5obj\u001b[39m.\u001b[39mfile\u001b[39m.\u001b[39mfilename, sub_h5obj\u001b[39m.\u001b[39mid, builder)\n\u001b[1;32m    639\u001b[0m obj_type[builder\u001b[39m.\u001b[39mname] \u001b[39m=\u001b[39m builder\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/hdmf/backends/hdf5/h5tools.py:645\u001b[0m, in \u001b[0;36mHDF5IO.__read_group\u001b[0;34m(self, h5obj, name, ignore)\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    644\u001b[0m kwargs[\u001b[39m'\u001b[39m\u001b[39msource\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(h5obj\u001b[39m.\u001b[39mfile\u001b[39m.\u001b[39mfilename)\n\u001b[0;32m--> 645\u001b[0m ret \u001b[39m=\u001b[39m GroupBuilder(name, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    646\u001b[0m ret\u001b[39m.\u001b[39mlocation \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(h5obj\u001b[39m.\u001b[39mname)\n\u001b[1;32m    647\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_written(ret)\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/hdmf/utils.py:643\u001b[0m, in \u001b[0;36mdocval.<locals>.dec.<locals>.func_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfunc_call\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 643\u001b[0m     pargs \u001b[39m=\u001b[39m _check_args(args, kwargs)\n\u001b[1;32m    644\u001b[0m     \u001b[39mreturn\u001b[39;00m func(args[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/hdmf/utils.py:615\u001b[0m, in \u001b[0;36mdocval.<locals>.dec.<locals>._check_args\u001b[0;34m(args, kwargs)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Parse and check arguments to decorated function. Raise warnings and errors as appropriate.\"\"\"\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[39m# this function was separated from func_call() in order to make stepping through lines of code using pdb\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[39m# easier\u001b[39;00m\n\u001b[0;32m--> 615\u001b[0m parsed \u001b[39m=\u001b[39m __parse_args(\n\u001b[1;32m    616\u001b[0m     loc_val,\n\u001b[1;32m    617\u001b[0m     args[\u001b[39m1\u001b[39;49m:] \u001b[39mif\u001b[39;49;00m is_method \u001b[39melse\u001b[39;49;00m args,\n\u001b[1;32m    618\u001b[0m     kwargs,\n\u001b[1;32m    619\u001b[0m     enforce_type\u001b[39m=\u001b[39;49menforce_type,\n\u001b[1;32m    620\u001b[0m     enforce_shape\u001b[39m=\u001b[39;49menforce_shape,\n\u001b[1;32m    621\u001b[0m     allow_extra\u001b[39m=\u001b[39;49mallow_extra,\n\u001b[1;32m    622\u001b[0m     allow_positional\u001b[39m=\u001b[39;49mallow_positional\n\u001b[1;32m    623\u001b[0m )\n\u001b[1;32m    625\u001b[0m parse_warnings \u001b[39m=\u001b[39m parsed\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mfuture_warnings\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    626\u001b[0m \u001b[39mif\u001b[39;00m parse_warnings:\n",
      "File \u001b[0;32m~/anaconda3/envs/dandibs/lib/python3.11/site-packages/hdmf/utils.py:249\u001b[0m, in \u001b[0;36m__parse_args\u001b[0;34m(validator, args, kwargs, enforce_type, enforce_shape, allow_extra, allow_positional)\u001b[0m\n\u001b[1;32m    246\u001b[0m         syntax_errors\u001b[39m.\u001b[39mappend(msg)\n\u001b[1;32m    248\u001b[0m \u001b[39m# iterate through the docval specification and find a matching value in args / kwargs\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m it \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(validator)\n\u001b[1;32m    250\u001b[0m arg \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(it)\n\u001b[1;32m    252\u001b[0m \u001b[39m# process positional arguments of the docval specification (no default value)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "experiment_file_paths = glob.glob(os.path.join(root_dir, '[exp]*', '*'))\n",
    "experiment_file_paths = [d for d in experiment_file_paths if 'VideoStimulusSet' not in os.path.basename(d)]\n",
    "\n",
    "for experiment_path in experiment_file_paths: \n",
    "    # global text\n",
    "    # text = ''\n",
    "\n",
    "    days    = glob.glob(os.path.join(experiment_path, '*[!npy][!txt][!nwb]'))\n",
    "    n_days  = len(days)\n",
    "    n_sessions = 0\n",
    "    for day in days :\n",
    "        n_sessions += len(glob.glob(os.path.join(experiment_path, day, '*', '*[nwb]')))\n",
    "    first_day = days[0].split('.')[-1]\n",
    "    \n",
    "    # if not os.path.basename(experiment_path).split('_')[1]=='emogan.sub': continue \n",
    "\n",
    "    print('________________________________________________________________________________')\n",
    "    print(f'{os.path.basename(experiment_path)} has {n_days} days and {n_sessions} sessions')\n",
    "    \n",
    "    # ------------------------------------------------------------------------------ \n",
    "    # Skip code, if experiment is not wanted (see list above) or if nwb not exists.\n",
    "    # ------------------------------------------------------------------------------  \n",
    "    if os.path.basename(experiment_path).split(\"_\")[1].split('.')[0] in list_exp_not_using: \n",
    "        # print(os.path.basename(experiment_path).split(\"_\")[1].split('.')[0])\n",
    "        text = 'Experiment is not going on BrainScore'\n",
    "        update_sheet(df, experiment_path, text)\n",
    "        continue \n",
    "    if n_sessions == 0: \n",
    "        print(f'{os.path.basename(experiment_path)} has no nwb file.')\n",
    "        text = 'No nwb files in experiment.'\n",
    "        update_sheet(df, experiment_path, text)\n",
    "        continue     \n",
    "    \n",
    "    if os.path.isfile(os.path.join(experiment_path, 'pvalues_first_day.npy')):\n",
    "        pvalues_first_day = np.load(os.path.join(experiment_path, 'pvalues_first_day.npy'))\n",
    "\n",
    "    else: \n",
    "        # ------------------------------------------------------------------------------ \n",
    "        # Do within Session QC for the sessions of the first day.  \n",
    "        # ------------------------------------------------------------------------------  \n",
    "        original_stdout = sys.stdout\n",
    "        sys.stdout = ioprint.StringIO()\n",
    "\n",
    "        pvalues_first_day = quality_within_session(first_day)\n",
    "\n",
    "        output = sys.stdout.getvalue()\n",
    "        sys.stdout = original_stdout\n",
    "        \n",
    "        print(f\"{output}\")\n",
    "\n",
    "        if pvalues_first_day[0] == None: \n",
    "            update_sheet(df, experiment_path, output)\n",
    "            continue \n",
    "        np.save(os.path.join(experiment_path, 'pvalues_first_day.npy'), pvalues_first_day)\n",
    "    \n",
    "    if n_days > 1: \n",
    "        run_again = True\n",
    "        if os.path.isfile(os.path.join(experiment_path, 'corss_session_pvalues.npy')): \n",
    "            run_again = False\n",
    "            corss_session_pvalues = np.load(os.path.join(experiment_path, 'corss_session_pvalues.npy'))\n",
    "            if len(corss_session_pvalues) != n_days-1: run_again = True\n",
    "\n",
    "        if run_again==True:\n",
    "            # ------------------------------------------------------------------------------ \n",
    "            # Do across session QC for the other sessions with the first day. \n",
    "            # ------------------------------------------------------------------------------  \n",
    "            list_all_days = get_list_of_days(days[1:])\n",
    "            corss_session_pvalues = []\n",
    "            for comparing_day in list_all_days:\n",
    "                original_stdout = sys.stdout\n",
    "                sys.stdout = ioprint.StringIO()\n",
    "\n",
    "                p_values = quality_across_sessions(first_day, comparing_day)\n",
    "                        \n",
    "                output = sys.stdout.getvalue()\n",
    "                sys.stdout = original_stdout\n",
    "                \n",
    "                print(f\"{output}\")\n",
    "                update_sheet(df, experiment_path, output)\n",
    "                corss_session_pvalues.append(p_values)\n",
    "            if any(element is None for sublist in corss_session_pvalues for element in sublist): \n",
    "                print(\"Did not find all the normalizers.\")\n",
    "                continue\n",
    "            else:\n",
    "                if np.array(corss_session_pvalues == None).sum() == 0:\n",
    "                    for val in corss_session_pvalues: print(np.allclose(np.where(pvalues_first_day<0.05), np.where(val[0,:]<0.05)))\n",
    "\n",
    "                np.save(os.path.join(experiment_path, 'corss_session_pvalues.npy'), corss_session_pvalues)\n",
    "\n",
    "    # ------------------------------------------------------------------------------ \n",
    "    # Add p-values to respective experiment nwb files.\n",
    "    # ------------------------------------------------------------------------------ \n",
    "\n",
    "    for day, i_day in zip(days, range(n_days)) :\n",
    "        print(f'Checking if P-Values are added to experiment file for day {i_day+1}')\n",
    "        exp_nwb_paths = (glob.glob(os.path.join(experiment_path, day, '*',  '*[nwb]')))\n",
    "\n",
    "        if i_day ==0:   current_pvalues = pvalues_first_day\n",
    "        else:           current_pvalues = corss_session_pvalues[i_day-1][1,:]\n",
    "\n",
    "        if current_pvalues[0] == None: continue\n",
    "\n",
    "        for exp_nwb_path in exp_nwb_paths:\n",
    "            \n",
    "            # print(i_day, exp_nwb_path)\n",
    "            io = NWBHDF5IO(exp_nwb_path, \"a\") \n",
    "            exp_nwbfile = io.read()\n",
    "            try:\n",
    "                n_channel = exp_nwbfile.scratch['psth'][:].shape[-1]\n",
    "                n_channel_norm = pvalues_first_day.shape[0]\n",
    "                assert n_channel == n_channel_norm\n",
    "            except: print('Experiment File has no PSTH.')\n",
    "\n",
    "            try: exp_nwbfile.scratch['RecordingQualityArray'][:]\n",
    "            except:\n",
    "                print(\"Adding pvalues to nwb.\")\n",
    "                exp_nwbfile.add_scratch(\n",
    "                        current_pvalues,\n",
    "                        name=\"RecordingQualityArray\",\n",
    "                        description=\"An array of length equal to the number of electrodes, where each entry corresponds to \\\n",
    "                            the p-value for the electrode with the matching ID. For the initial recording, a within-session \\\n",
    "                            quality check is conducted; for subsequent recordings, a cross-session quality comparison is \\\n",
    "                            performed with the initial recording. Parameters used: boot_seed=0, nboot=10000.\",\n",
    "                        )\n",
    "                        \n",
    "                io.write(exp_nwbfile)\n",
    "\n",
    "            io.close()\n",
    "    \n",
    "    text = 'P-Values added.'\n",
    "    update_sheet(df, experiment_path, text)\n",
    "                \n",
    "\n",
    "# Update Sheet 2\n",
    "xls = pd.ExcelFile(f'{os.path.dirname(cwd)}/pico_inventory.xlsx')\n",
    "sheets = {sheet: xls.parse(sheet) for sheet in xls.sheet_names}\n",
    "\n",
    "sheets['Sheet2'] = df  \n",
    "\n",
    "with pd.ExcelWriter(f'{os.path.dirname(cwd)}/pico_inventory.xlsx', engine='openpyxl', mode='w') as writer:\n",
    "    for sheet_name, sheet_df in sheets.items():\n",
    "        sheet_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#corss_session_pvalues\n",
    "#None in corss_session_pvalues\n",
    "contains_none = any(element is None for sublist in corss_session_pvalues for element in sublist)\n",
    "contains_none"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dandibs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
